{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fa20_hw04_netid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKG2r_QRX5Px"
      },
      "source": [
        "# Overview \n",
        "\n",
        "Please see the [homework policy](https://fa20.fdl.thecoatlessprofessor.com/syllabus/#homework)\n",
        "for detailed instructions and some grading notes. Failure to follow instructions\n",
        "will result in point reductions. \n",
        "\n",
        "> \"Machine intelligence is the last invention that humanity will ever need to make.\"\n",
        ">\n",
        "> -- Nick Bostrom\n",
        "\n",
        "## Grading\n",
        "\n",
        "The rubric TAs will use to grade this assignment is:\n",
        "\n",
        "| Task                                                   | Pts |\n",
        "|:-------------------------------------------------------|----:|\n",
        "| Data Shapeshifter                                      | 10  |\n",
        "| Architecting Infrastructure                            | 15  |\n",
        "| Deriving Network Equations                             | 20  |\n",
        "| Implementing the Network                               | 45  |\n",
        "| It's Alive!                                            | 10  |\n",
        "| Cruisin' Keras                                         | 20  |\n",
        "| Total                                                  | 120 |\n",
        "\n",
        "## Objectives \n",
        "\n",
        "The objectives behind this homework assignment are as follows:\n",
        "\n",
        "- Implement functions in Python;\n",
        "- Constructing neural networks; and\n",
        "- Establishing sequential layers with Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NXeOMIS4Qot",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf038f19-7f36-4818-8ea7-cc59731a6dd7"
      },
      "source": [
        "#@title (Hidden) Diagnostic Check\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"python: {sys.version}\")\n",
        "print(f\"pandas: {np.__version__}\")\n",
        "print(f\"tensorflow: {tf.__version__}\")\n",
        "# Detect if a GPU is present\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "  print(f\"GPU Enabled: True\")\n",
        "else:\n",
        "  print(f\"GPU Enabled: False\")\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  from google.colab import files\n",
        "  is_google_colab = True\n",
        "\n",
        "  print(\"Notebook is on Google CoLab\")\n",
        "except:\n",
        "  is_google_colab = False\n",
        "  print(\"Notebook is being run locally or through another source.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python: 3.6.9 (default, Jul 17 2020, 12:50:27) \n",
            "[GCC 8.4.0]\n",
            "pandas: 1.18.5\n",
            "tensorflow: 2.3.0\n",
            "GPU Enabled: False\n",
            "Notebook is on Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbKkA-YM3Tgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c50898-eb39-4454-8d4f-627d5e0ce630"
      },
      "source": [
        "# Mounting the device will require following instructions to enter a\n",
        "if is_google_colab:\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print(\"Please make sure to upload images to Compass2g.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XEKpojMyMB1"
      },
      "source": [
        "# Assignment - Homework 4\n",
        "STAT 430 - FDL, Fall 2020\n",
        "\n",
        "Due: **Thursday, Oct 29th, 2020 at 6:00 PM**\n",
        "\n",
        "- **Author:** Jared(Mengchen) Qiu\n",
        "- **NetID:** mqiu3\n",
        "\n",
        "### Collaborators\n",
        "\n",
        "If you worked with any other student in preparing these answers, please\n",
        "make sure to list their full names and NetIDs (e.g. `FirstName LastName (NetID)` ).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIypRcljzQvR"
      },
      "source": [
        "## [10 Points] Exercise 1: Reshaping Data\n",
        "\n",
        "**Have questions on Exercise 2? Please ask at: <https://campuswire.com/c/G9F0E3E38/feed/68>**\n",
        "\n",
        "Reshaping data is a common requirement for working with neural networks. Previously, the assumption about data in neural networks\n",
        "was that there is exactly _one_ example or image. In practice, there needs to\n",
        "be a large amount of data for the neural network to function. Previously, $X$ was defined to be a single input vector: \n",
        "\n",
        "$$\n",
        "X_{m \\times 1} = \\begin{bmatrix}\n",
        "\\vert \\\\\n",
        "X^{(1)}  \\\\\n",
        "\\vert  \\\\\n",
        "\\end{bmatrix}_{m \\times 1}\n",
        "$$\n",
        "\n",
        "To enable multiple training examples, let's define $X$ to be:\n",
        "\n",
        "$$\n",
        "X_{m \\times n_e} = \\begin{bmatrix}\n",
        "\\vert & \\vert & \\vert \\\\\n",
        "X^{(1)} & X^{(2)} & X^{(3)} \\\\\n",
        "\\vert & \\vert & \\vert \\\\\n",
        "\\end{bmatrix}_{m \\times n_e}\n",
        "$$\n",
        "\n",
        "where $m$ is the number of inputs and $n_e$ is the number of observations. Therefore, the linear combinations in $Z^{[1]}$ would now look like:\n",
        "\n",
        "$$\n",
        "Z^{[1]} = \\begin{bmatrix}\n",
        "\\vert & \\vert & \\vert \\\\\n",
        "Z^{[1](1)} & Z^{[1](2)} & Z^{[1](3)} \\\\\n",
        "\\vert & \\vert & \\vert \\\\\n",
        "\\end{bmatrix}_{n_h \\times n_e}\n",
        "$$\n",
        "\n",
        "Let the parentheses -- $[]$ -- denote the layer number and the square brackets\n",
        "-- $()$ -- be the training data example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB3jHSalEfPl"
      },
      "source": [
        "### [5 points] (a) Reshaping Data Input\n",
        "\n",
        "Having said this, please reshape the MNIST training data from:\n",
        "\n",
        "$$\\text{Examples }\\times \\text{Height } \\times \\text{Width }$$\n",
        "\n",
        "To being shaped as:\n",
        "\n",
        "$$\\underbrace{\\left(\\text{Height } \\cdot \\text{Width }\\right)}_{=m} \\times \\underbrace{\\text{Examples }}_{=n_e}$$\n",
        "\n",
        "where $m$ is the number of features and $n_e$ represents the number of observations.\n",
        "\n",
        "\n",
        "Through this process, the data is being flattened:\n",
        "\n",
        "$$\\underbrace{\\left(\\text{Height } \\cdot \\text{Width }\\right)}_{=m}$$\n",
        "\n",
        "Therefore, we will have $m$ different pixels acting as features for the model.\n",
        "\n",
        "_Hints:_\n",
        "\n",
        "- Changing a data's shape is possible with [`np.reshape(data, (data.shape[0], -1))`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html#numpy.reshape) and [`np.reshape(data, (-1, data.shape[0]))`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html#numpy.reshape).\n",
        "  - **Note:** `-1` denotes an unknown dimension and requests that NumPy determine it during the reshape. \n",
        "- Arrays can be transposed with [`data.T`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.T.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfUQFLEhjvpO",
        "cellView": "form"
      },
      "source": [
        "#@title (Hidden) Initialization of Data\n",
        "%%capture\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "# Load the MNIST data via a helper\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "# Perform scaling\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 # [0, 255] -> [0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn476i5VwbFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9236b5a-6908-40ba-c5e2-ad34a8681382"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# code here \n",
        "x_train = x_train.reshape(x_train.shape[0], -1).T\n",
        "x_test = x_test.reshape(x_test.shape[0], -1).T\n",
        "# data should be reshaped\n",
        "\n",
        "# Grading helper:\n",
        "print(f'X has dimensions: {x_train.shape}')\n",
        "print(f'X has dimensions: {x_test.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X has dimensions: (784, 60000)\n",
            "X has dimensions: (784, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64s2pBTPwoiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51959556-3417-4cad-82b6-663c9d409080"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lPASIbhy237"
      },
      "source": [
        "#### Aside: Data reshaping \n",
        "\n",
        "Having trouble re-organizing the data? Try modifying a smaller example case first. **This is not graded but is intended to allow exploration of how data changes in a minimal example.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOuMoDie0qvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d80072-e955-4686-9954-140a7b0a367b"
      },
      "source": [
        "# Not graded code, acts a small test case\n",
        "import numpy as np\n",
        "a = np.arange(24).reshape((4, 3, 2))\n",
        "print(f\"Contents of `a` before transform:\\n {a}\\n\")\n",
        "print(f\"Dimensions of `a` before transform:\\n {a.shape}\\n\")\n",
        "\n",
        "# This is just is a direct implementation of the hint. \n",
        "a2 = np.reshape(a, (-1, a.shape[0]))\n",
        "print(f\"Contents of `a2` after transform:\\n {a2}\\n\")\n",
        "print(f\"Dimensions of `a2`:\\n {a2.shape}\\n\")\n",
        "\n",
        "# Use this line of code for reshaping x \n",
        "a3 = np.reshape(a, (a.shape[0], -1)).T\n",
        "print(f\"Contents of `a3` after transform:\\n {a3}\\n\")\n",
        "print(f\"Dimensions of `a3`:\\n {a3.shape}\\n\")\n",
        "\n",
        "b = np.arange(3)\n",
        "print(f\"Contents of `b`:\\n {b}\\n\")\n",
        "print(f\"Dimensions of `b`:\\n {b.shape}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Contents of `a` before transform:\n",
            " [[[ 0  1]\n",
            "  [ 2  3]\n",
            "  [ 4  5]]\n",
            "\n",
            " [[ 6  7]\n",
            "  [ 8  9]\n",
            "  [10 11]]\n",
            "\n",
            " [[12 13]\n",
            "  [14 15]\n",
            "  [16 17]]\n",
            "\n",
            " [[18 19]\n",
            "  [20 21]\n",
            "  [22 23]]]\n",
            "\n",
            "Dimensions of `a` before transform:\n",
            " (4, 3, 2)\n",
            "\n",
            "Contents of `a2` after transform:\n",
            " [[ 0  1  2  3]\n",
            " [ 4  5  6  7]\n",
            " [ 8  9 10 11]\n",
            " [12 13 14 15]\n",
            " [16 17 18 19]\n",
            " [20 21 22 23]]\n",
            "\n",
            "Dimensions of `a2`:\n",
            " (6, 4)\n",
            "\n",
            "Contents of `a3` after transform:\n",
            " [[ 0  6 12 18]\n",
            " [ 1  7 13 19]\n",
            " [ 2  8 14 20]\n",
            " [ 3  9 15 21]\n",
            " [ 4 10 16 22]\n",
            " [ 5 11 17 23]]\n",
            "\n",
            "Dimensions of `a3`:\n",
            " (6, 4)\n",
            "\n",
            "Contents of `b`:\n",
            " [0 1 2]\n",
            "\n",
            "Dimensions of `b`:\n",
            " (3,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jPbd7Mayonm"
      },
      "source": [
        "###  [5 Points] (b) Encoding Output\n",
        "\n",
        "Encode the label data ($Y$) using one-hot encoding. \n",
        "\n",
        "The data has the following labels:\n",
        "\n",
        "| Label|Class       |\n",
        "|-----:|:-----------|\n",
        "|     0|T-shirt/top |\n",
        "|     1|Trouser     |\n",
        "|     2|Pullover    |\n",
        "|     3|Dress       |\n",
        "|     4|Coat        |\n",
        "|     5|Sandal      |\n",
        "|     6|Shirt       |\n",
        "|     7|Sneaker     |\n",
        "|     8|Bag         |\n",
        "|     9|Ankle boot  |\n",
        "\n",
        "\n",
        "_Hint:_ \n",
        "\n",
        "- Have you read over the **Encoding** lecture?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZfyz4od8oVl"
      },
      "source": [
        "# Code here\n",
        "#y_train = (pd.get_dummies(y_train)).to_numpy()\n",
        "#y_test = (pd.get_dummies(y_test)).to_numpy()\n",
        "# Note: We want y_hat to be [n_c x n_e]\n",
        "# not [n_e x n_c]\n",
        "\n",
        "y_train_onehot = y_train[:,np.newaxis]\n",
        "y_test_onehot = y_test[:,np.newaxis]\n",
        "y_train_onehot = pd.get_dummies(y_train[:,np.newaxis].reshape(-1)).T.to_numpy()\n",
        "y_test_onehot = pd.get_dummies(y_test[:,np.newaxis].reshape(-1)).T.to_numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bF0X1015Ncd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81867e5f-a1c2-4000-f3f1-823228f4499c"
      },
      "source": [
        "# Grading helper\n",
        "print(f'Y has dimensions: {y_train.shape}')\n",
        "print(f'Y Train has dimensions: {y_train_onehot.shape}')\n",
        "print(f'Number of Images: {y_train_onehot.shape[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y has dimensions: (60000,)\n",
            "Y Train has dimensions: (10, 60000)\n",
            "Number of Images: 60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc2LZ5Q0xdbm"
      },
      "source": [
        "Test case notes:\n",
        "\n",
        "- `y_train` should have dimensions: (10, 60000)\n",
        "- `Number of Images` should be: 60000.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQXbtPp2zm5z"
      },
      "source": [
        "## [15 Points] Exericse 2: Architecting Infrastructure\n",
        "\n",
        "**Have questions on Exercise 2? Please ask at: <https://campuswire.com/c/G9F0E3E38/feed/69>**\n",
        "\n",
        "Within this part, we'll construct general functions that can be used across neural networks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tOycmv9RKeF"
      },
      "source": [
        "### [5 points] (a) He initialization\n",
        "\n",
        "Create a function to initialize weight parameters for a given layer in the neural network. The weight initialization should use [He's initialization](https://arxiv.org/abs/1502.01852) given by:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "W_{n^{[l]} \\times n^{[l-1]}} &\\sim N(0, 1) \\\\\n",
        "W &:= W\\sqrt{\\frac{2}{n^{[l-1]}}}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $N(0,1)$ is the standard normal distribution, $n^{[l-1]}$ denotes the number of input weights, and $n^{[l]}$ is the\n",
        "number of output weights.\n",
        "\n",
        "Implementation Guidelines:\n",
        "\n",
        "- **Arguments:**\n",
        "    - `n_in`: Number of input weights\n",
        "    - `n_out`: Number of output weights\n",
        "- **Return:**\n",
        "    - `W` a matrix of dimension $n^{[l]} \\times n^{[l-1]}$\n",
        "\n",
        "_Hints:_ NumPy has a built in way of generating random data with [`np.random.randn(n, p)`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOCkiOCVHyUk"
      },
      "source": [
        "# Code Here\n",
        "def ini(n_in, n_out):\n",
        "  to_ret = np.random.randn(n_out, n_in)\n",
        "  to_ret = to_ret*np.sqrt(2/n_in)\n",
        "  return to_ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRoSVmyqx3yr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942c8e7a-4e3b-4071-df81-096d8789acc0"
      },
      "source": [
        "# Run test case\n",
        "np.random.seed(112)\n",
        "# Should have initialization of:\n",
        "# he_initialization(4, 3)\n",
        "ini(4, 3)\n",
        "# array([[ 1.04491348, -0.93050742,  0.41260625,  0.34339403],\n",
        "#       [ 0.24151303,  0.41856918, -0.49456097, -0.19109271],\n",
        "#       [-0.35860733,  0.7416791 ,  0.90066355, -0.38414521]])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.04491348, -0.93050742,  0.41260625,  0.34339403],\n",
              "       [ 0.24151303,  0.41856918, -0.49456097, -0.19109271],\n",
              "       [-0.35860733,  0.7416791 ,  0.90066355, -0.38414521]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1ltf2EIHy17"
      },
      "source": [
        "### [10 points] (b) Activation Functions\n",
        "\n",
        "Implement the ReLU, Sigmoid, and Softmax activation functions alongside their derivatives.\n",
        "\n",
        "Implementation Guidelines for each function:\n",
        "\n",
        "- **Arguments:**\n",
        "    - `X` a matrix of dimension `a X b`.\n",
        "- **Return:**\n",
        "    - Appropriately \"activated\" or \"derivative\" matrix of dimension `a X b`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yniVGZrZFPUG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs-Pl34dIDYU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(z):\n",
        "  # Code Here\n",
        "  a = np.where(z > 0, z, 0)\n",
        "  return a\n",
        "  \n",
        "def relu_prime(z):\n",
        "  # Code here\n",
        "  prime = np.where(z >= 0, 1, 0)\n",
        "  return prime\n",
        "\n",
        "def sigmoid(z):\n",
        "  # Code here\n",
        "  a = 1/(1+np.exp(-z))\n",
        "  return a\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "  # Code here\n",
        "  prime = sigmoid(z) * (1 - sigmoid(z))\n",
        "  return prime\n",
        "\n",
        "def softmax(z):\n",
        "  # Code here\n",
        "  a = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
        "  return a\n",
        "\n",
        "def softmax_prime(z):\n",
        "  # Code here\n",
        "  s = softmax(z).reshape(-1,1)\n",
        "  prime = np.diagflat(s) - np.dot(s, s.T)\n",
        "  #prime = softmax(z)*(1-softmax(z))\n",
        "  return prime\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaHGju8mV4ER"
      },
      "source": [
        "## [20 points] Exercise 3 - Deriving a Neural Network!\n",
        "\n",
        "**Have questions on Exercise 3? Please ask at: <https://campuswire.com/c/G9F0E3E38/feed/70>**\n",
        "\n",
        "In this exercise block, we'll create a computational graph and compute the\n",
        "appropriate backpropagation derivatives.\n",
        "\n",
        "In the first and second hidden layers, there will be _unknown_ number of neurons. Let the neurons in the first hidden layer be denoted as $n_j^{(1)}$. Within the second hidden layer and output layer, let there be $n_c^{(2)}$ neurons. The input layer and first hidden layers should include a bias term to make the counts respectively $m + 1$ and $n_j^{(1)} + 1$.  For the non-linear activation functions, let the first hidden layer use \n",
        "$g^{(1)} (x) = \\mathrm{ReLU}(x)$, the second hidden layer be $g^{(2)} (x) = \\sigma(x)$. Finally, in the output layer, apply the $\\mathrm{softmax}(x)$ to obtain exactly $n_c^{(2)}$ neurons that correspond to $C$ classes. \n",
        "\n",
        "We'll use the categorical cross-entropy cost function of \n",
        "\n",
        "$$\n",
        "J\\left(\\boldsymbol{W}\\right) = -\\frac{1}{N} \\sum_{i=1}^{N} { \\sum_{c=1}^{C} { y_{ic} \\log\\left({ \\hat y_{ic} }\\right)  }}\n",
        "$$\n",
        "\n",
        "where,\n",
        "\n",
        "- $\\boldsymbol{W}$ represents the weights of the neural network -- including the bias term,\n",
        "- $N$ represents the number of examples the network is being trained with,\n",
        "- $y_{ic}$ is the true label in $c \\in C$, and;\n",
        "- $\\hat y_{ic}$ is the predicted label probability in $c \\in C$.\n",
        "\n",
        "\n",
        "_Hint:_ \n",
        "\n",
        "- Have you read over the **Backprop with Matrices** and the **Softmax Backprop** lectures?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAJAIGQUgtpr"
      },
      "source": [
        "### [10 points] (a) Sketching the computational graph\n",
        "\n",
        "Sketch the computational graph for the described neural network. For each layer on the graph, write out the forward propagation equations in matrix equation form. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-jwPNmRhKC6"
      },
      "source": [
        "... insert diagram here ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0LDicjAiJH7",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "996aaba0-6caf-482e-d4ff-b15c26db303b"
      },
      "source": [
        "# Upload an image file \n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-13ba3774-746e-45b4-8be4-9d5ec07147ef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-13ba3774-746e-45b4-8be4-9d5ec07147ef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c07239660e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Upload an image file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh_C45WuWQSR"
      },
      "source": [
        "# Display the embedded image in the notebook.\n",
        "Image('colab-logo.png', width=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T_HQIIwmuB2"
      },
      "source": [
        "### [10 points] (b) Deriving the backward propagation equations\n",
        "\n",
        "Within this step, derive the backward propagation equations for the network\n",
        "by obtaining the necessary partial derivatives.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU4h_HGrmr_Y"
      },
      "source": [
        "... insert formulas here ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhTHs18Tmqqy"
      },
      "source": [
        "# Upload an image file \n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUXxcT_7WNZ6"
      },
      "source": [
        "# Display the embedded image in the notebook.\n",
        "Image('colab-logo.png', width=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzSArV9G7lkJ"
      },
      "source": [
        "## [45 Points] Exercise 4 - Implementing the Network\n",
        "\n",
        "**Have questions on Exercise 4? Please ask at: <https://campuswire.com/c/G9F0E3E38/feed/71>**\n",
        "\n",
        "Within this part, we'll construct functions associated with _training_ a neural network as described in Exercise (3).\n",
        "\n",
        "In particular, we'll develop functions for: \n",
        "\n",
        "- Performing forward propagation\n",
        "- Perform backward propagation\n",
        "- Update network parameters\n",
        "- Computing the cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_ljdhD5RNyR"
      },
      "source": [
        "### [5 points] (a) Parameter Initialization\n",
        "\n",
        "Using the function in **Exercise 2 (a)** construct a function \n",
        "that initializes both the weights and the bias terms for each layer as specified in **Exercise 3**. The bias values should be initialized with 0 and the weights should be initialized with the function developed in **Exercise 2 (a)**.\n",
        "\n",
        "Implementation Guidelines:\n",
        "\n",
        "- **Arguments:**\n",
        "    - `m`: Number of input neurons\n",
        "    - `n_h`: Number of hidden neurons\n",
        "    - `n_y`: Number of output neurons\n",
        "- **Return:**\n",
        "    - `cache_parameters` with a dictionary data structure that contains:\n",
        "       - `W^(1)` a matrix of dimension `n_h X m`\n",
        "       - `b^(1)` a matrix of dimension `n_h X 1`\n",
        "       - `W^(2)` a matrix of dimension `n_y X n_h`\n",
        "       - `b^(2)` a matrix of dimension `n_y X 1`\n",
        "\n",
        "_Hint:_ \n",
        "\n",
        "- NumPy has a built in way of generating a zero vector with [`np.zeros((n, p))`](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ97Ta3ERNhZ"
      },
      "source": [
        "# code here\n",
        "def weight_bias(m, n_h, n_y):\n",
        "  cache_parameters = {}\n",
        "  cache_parameters['W^(1)'] = ini(m, n_h)\n",
        "  cache_parameters['b^(1)'] = np.zeros((n_h, 1))\n",
        "  cache_parameters['W^(2)'] = ini(n_h, n_y)\n",
        "  cache_parameters['b^(2)'] = np.zeros((n_y, 1))\n",
        "  return cache_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqyZe-QtIQUU"
      },
      "source": [
        "### [10 points] (b) Forward propagation\n",
        "\n",
        "Implement a function that performs the forward propagation\n",
        "and caches (saves) the value computed at each node on the computational graph. \n",
        "\n",
        "Implementation Guidelines:\n",
        "\n",
        "- **Arguments:**\n",
        "    - `X` a matrix of dimension `m X n_e`.\n",
        "    - `cache_parameters` with a dictionary data structure.\n",
        "- **Return:**\n",
        "    - `SM` a matrix of dimension `n_y X n_e`\n",
        "    - `cache_forward` with a dictionary data structure that contains:\n",
        "       - `Z^{(1)}`\n",
        "       - `A^{(1)}`\n",
        "       - `Z^{(2)}`\n",
        "       - `A^{(2)}`\n",
        "       - `SM`\n",
        "\n",
        "_Note:_ $A^{(2)} \\rightarrow SM$ to provide the softmax output that gives $\\hat y$. \n",
        "\n",
        "\n",
        "_Hint:_\n",
        "\n",
        "- Recall that items in a dictionary data structure can be accessed with `data[\"item\"]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DJCHduKIQsC"
      },
      "source": [
        "# code here\n",
        "def forward(X, cache_parameters):\n",
        "  cache_forward = {}\n",
        "  cache_forward['Z^{(1)}'] = cache_parameters['W^(1)'] @ X + cache_parameters['b^(1)']\n",
        "  cache_forward['A^{(1)}'] = relu(cache_forward['Z^{(1)}'])\n",
        "  cache_forward['Z^{(2)}'] = cache_parameters['W^(2)'] @ cache_forward['A^{(1)}'] + cache_parameters['b^(2)']\n",
        "  cache_forward['A^{(2)}'] = sigmoid(cache_forward['Z^{(2)}'])\n",
        "  cache_forward['SM'] = softmax(cache_forward['A^{(2)}'])\n",
        "  return cache_forward['SM'], cache_forward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBjuQYYQRnyK"
      },
      "source": [
        "**Why is it important to store the values computed in the prediction step?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1doOOF-JYoVg"
      },
      "source": [
        "... your answer here ... \n",
        "\n",
        "... did you write an answer here ??? \n",
        "\n",
        "Because we have to use them in backward propagation step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkApLxJLRptJ"
      },
      "source": [
        "### [10 Points] (c) Backward propagation\n",
        "\n",
        "Implement a function that performs the backward propagation\n",
        "at each node on the computational graph. Please use the back propagation equations derived in **Exericse 3**.\n",
        "\n",
        "- **Arguments:**\n",
        "    - `Y` a matrix of dimension `1 X n_e`.\n",
        "    - `X` a matrix of dimension `m X n_e`.\n",
        "    - `cache_parameters` a data dictionary.\n",
        "    - `cache_forward` a data dictionary.\n",
        "- **Return:**\n",
        "    - `cache_grad` with a dictionary data structure that contains:\n",
        "       - `dW2`\n",
        "       - `db2`\n",
        "       - `dW1`\n",
        "       - `db1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1fEnD_641e7"
      },
      "source": [
        "To ensure that the bias terms remain as 1D vectors, please use \n",
        "\n",
        "```python\n",
        "db = np.sum(\"gradient code here for bias\", axis = 1, keepdims = True)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3x6MU63SH82"
      },
      "source": [
        "# Code here\n",
        "def backward(Y, X, cache_parameters, cache_forward):\n",
        "  cache_grad = {}\n",
        "  inter = (cache_forward['SM'] -Y) / X.shape[1] * sigmoid_prime(cache_forward['Z^{(2)}'])\n",
        "  cache_grad['dW2'] = inter @ cache_forward['A^{(1)}'].T\n",
        "  cache_grad['db2'] = np.sum(inter*1, axis=1, keepdims=True)\n",
        "  cache_grad['dW1'] = ((cache_parameters['W^(2)'].T @ inter) * relu_prime(cache_forward['Z^{(1)}'])) @ X.T\n",
        "  cache_grad['db1'] = np.sum((cache_parameters['W^(2)'].T @ inter) * relu_prime(cache_forward['Z^{(1)}']), axis=1, keepdims=True)\n",
        "  return cache_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdEVy74LWuzZ"
      },
      "source": [
        "### [10 points] (d) Parameter Update\n",
        "\n",
        "With the backward propagation calculations in hand, the\n",
        "next step is to update the parameters in the neural network. The update\n",
        "step should be performed using **Batch Gradient Descent (BGD)**.\n",
        "\n",
        "- **Arguments:**\n",
        "    - `alpha` the learning rate parameter.\n",
        "    - `cache_parameters` a data dictionary containing parameters.\n",
        "    - `cache_grad` a data dictionary containing the gradients.\n",
        "- **Return:**\n",
        "    - `cache_parameters` an updated version of the dictionary data structure that contains:\n",
        "       - `W2`\n",
        "       - `b2`\n",
        "       - `W1`\n",
        "       - `b1`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4xL4yL4ZTJh"
      },
      "source": [
        "# Code here\n",
        "def iterator(alpha, cache_parameters, cache_grad):\n",
        "  W2 = cache_parameters['W^(2)'] - alpha * cache_grad['dW2']\n",
        "  b2 = cache_parameters['b^(2)'] - alpha * cache_grad['db2']\n",
        "  W1 = cache_parameters['W^(1)'] - alpha * cache_grad['dW1']\n",
        "  b1 = cache_parameters['b^(1)'] - alpha * cache_grad['db1']\n",
        "  cache_parameters['W^(2)'] = W2\n",
        "  cache_parameters['b^(2)'] = b2\n",
        "  cache_parameters['W^(1)'] = W1\n",
        "  cache_parameters['b^(1)'] = b1\n",
        "  return cache_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW1eYDOqZX7X"
      },
      "source": [
        "### [10 Points] (e) Model Training Wrapper\n",
        "\n",
        "Create a function that encases all of the prior functions. This function should\n",
        "perform the training and provide side-effects of the training process. \n",
        "\n",
        "- **Arguments:**\n",
        "    - `Y` a matrix of dimension `n_c X n_e`.\n",
        "    - `X` a matrix of dimension `m X n_e`.\n",
        "    - `n_h` number of neurons in the hidden layer.\n",
        "    - `alpha` the learning rate parameter. Default `1e-3`. \n",
        "    - `epochs` the total number of times to train. Default `50`.\n",
        "- **Side-effect**\n",
        "    - Every 10 iterations output the iteration number and the present cost.\n",
        "- **Return:**\n",
        "    - `cost_history` the cost values from each iteration of the training step.\n",
        "    - `cache_parameters` with a dictionary data structure that contains:\n",
        "       - `W2`\n",
        "       - `b2`\n",
        "       - `W1`\n",
        "       - `b1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVRkcbiLp7w0"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F27T8jPZYVT"
      },
      "source": [
        "# Code Here\n",
        "def Wrapper(Y, X, n_h, alpha=1e-3, epochs=50):\n",
        "  cost_history = []\n",
        "  parameters_ = weight_bias(X.shape[0], n_h, Y.shape[0])\n",
        "  for i in range(epochs):\n",
        "    sm, forward_ = forward(X, parameters_)\n",
        "    #cost = (np.sum(np.sum(Y * np.log(forward_['SM']), axis=0)))/Y.shape[1]*(-1)\n",
        "    backward_ = backward(Y, X, parameters_, forward_)\n",
        "    parameters_ = iterator(alpha, parameters_, backward_)\n",
        "    cache_parameters = parameters_\n",
        "    cost = np.sum(Y * np.log(sm)) / Y.shape[1]*(-1)\n",
        "    cost_history.append(cost)\n",
        "    if i % 10 == 0:\n",
        "      print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "  return cost_history, cache_parameters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TApF9Mfpp5ZZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63kDR2rt-7XA"
      },
      "source": [
        "## [10 Points] Exericse 5: It's Alive!\n",
        "\n",
        "**Have questions on Exercise 5? Please ask at: <https://campuswire.com/c/G9F0E3E38/feed/72>**\n",
        "\n",
        "In this exercise, we put everything together from the previous four exercises and train our network from scratch!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur07yy0ZvXKN"
      },
      "source": [
        "### [5 points] (a) Training the Model\n",
        "\n",
        "In this exercise, perform the full network training on the MNIST Fashion data shaped in **Exercise 1 (a)**. \n",
        "\n",
        "Retrieve and visualize the cost function history across **40 epochs** with a learning rate of **0.001**. \n",
        "\n",
        "**Note:** Make sure to run the hidden code chunk and the reshaping code you wrote prior to training the code within this cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on-b7AATvYH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336c2858-a51f-48ce-fded-fa753e031b25"
      },
      "source": [
        "# Code Here\n",
        "\n",
        "history, para = Wrapper(y_train_onehot, x_train, 64, 0.0001, 40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.339418\n",
            "Cost after iteration 10: 2.339203\n",
            "Cost after iteration 20: 2.338988\n",
            "Cost after iteration 30: 2.338772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKX9shfCM_vj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "17190fc9-8773-4e50-f5f8-34727fb10c4f"
      },
      "source": [
        "%matplotlib inline\n",
        "# Visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "# Must have variable named as `cost_history` with 1 dimension.\n",
        "plt.plot(history)\n",
        "plt.ylabel('Cost Function')\n",
        "plt.xlabel('Number of Iterations')\n",
        "plt.title('Network Training Overview')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tImazUr9YJ-T"
      },
      "source": [
        "### [5 points] (b) Predicting the Model\n",
        "\n",
        "Use the model parameters obtained from training the network in **(i)** to make predictions on the data. Consider creating a prediction function to aide in this\n",
        "task.\n",
        "\n",
        "- **Arguments:**\n",
        "    - `X` a matrix of dimension `m X n_e`.\n",
        "    - `cache_parameters` a dictionary data structure that contains estimated parameters.\n",
        "- **Return:**\n",
        "    - `prediction` predicted values\n",
        "\n",
        "_Hint:_ Logic for this was rewritten during the forward propagation step. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP26WRk3a5mR"
      },
      "source": [
        "# Code Here\n",
        "def predd(X, cache_parameters):\n",
        "  a = cache_parameters['W^(1)'] @ X + cache_parameters['b^(1)']\n",
        "  b = relu(a)\n",
        "  c = cache_parameters['W^(2)'] @ b + cache_parameters['b^(2)']\n",
        "  d = sigmoid(c)\n",
        "  predictn = softmax(d)\n",
        "  return predictn\n",
        "\n",
        "prediction = predd(x_train, para)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlJNt2w7a7GX"
      },
      "source": [
        "The following should be used to show the image alongside of its predicted\n",
        "class.\n",
        "\n",
        "Note: \n",
        "\n",
        "- [`np.argmax(data)`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html) returns the index of the highest value.\n",
        "- [`np.max(data)`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.max.html) returns the highest value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1mGiMjfYXfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "81446862-8403-4695-e306-8cbd907c1ac6"
      },
      "source": [
        "%matplotlib inline\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fashion_mnist_viz = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "# Load the MNIST data via a helper\n",
        "(x_train_viz, y_train_viz), (x_test_viz, y_test_viz) = fashion_mnist_viz.load_data()\n",
        "# Perform scaling\n",
        "x_train_viz, x_test_viz = x_train_viz / 255.0, x_test_viz / 255.0\n",
        "\n",
        "# Set seed for reproducibility\n",
        "################################################\n",
        "# Change to the last four digits of your UIN\n",
        "#\n",
        "################################################\n",
        "np.random.seed(2707)\n",
        "\n",
        "# Obtain a set of indices to obtain predictions\n",
        "idx = list(np.random.randint(x_test_viz.shape[1], size = 10))\n",
        "\n",
        "# Increase the figure size\n",
        "plt.subplots(figsize=(18, 10))\n",
        "\n",
        "# Iterate through 10 training examples\n",
        "for i in range(10):\n",
        "  obs = idx[i]                           # Retrieve the index\n",
        "  plt.subplot(2, 5, i+1)                 # Create a subplot\n",
        "  img = x_test_viz[obs].reshape((28,28)) # Examples x Width x Height\n",
        "  pred = np.argmax(prediction[:, obs])   # Obtain the highest index\n",
        "  plt.imshow(img, cmap='gray')           # Plot gray scale\n",
        "  plt.title(f'Image ID: {obs} \\n Class: {y_test_viz[obs]}, Predicted: {pred} ')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x720 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bxhSMXWfD5O"
      },
      "source": [
        "## [20 points] Exercise 6 - Cruisin' Keras!\n",
        "\n",
        "**Have questions on Exercise 6? Please ask at: <https://campuswire.com/c/G9F0E3E38/feed/73>**\n",
        "\n",
        "In this exercise block, we'll implement the same neural network from **Exercise 3 - 5** but using Keras. Hopefully, the results are similar to what was computed in **Exercise 5**! (Fingers crossed )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llUnqwrSfLyA"
      },
      "source": [
        "### (a) (10 points) Structuring a model in keras\n",
        "\n",
        "Structure a two-layer sequential model in Keras that mirrors the guidelines in **Exercise 3**. \n",
        "\n",
        "_Hint:_ \n",
        "\n",
        "- Have you read over the **Keras in Practice** lecture?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrGut3Q8gsrA"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, layers\n",
        "\n",
        "# code Here\n",
        "model1 = Sequential([\n",
        "      layers.Input(shape=(784,)),\n",
        "      layers.Dense(64, activation='relu'), # or embed\n",
        "      layers.Dense(64, activation='sigmoid'),\n",
        "      layers.Dense(10, activation='softmax')\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF3RFlvovrre"
      },
      "source": [
        "Show the model layer information by providing a model summary with Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCaAefjJvtsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3c2e16-759a-4338-ddcf-6a4f1c36b969"
      },
      "source": [
        "# code here\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 55,050\n",
            "Trainable params: 55,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdgvsQGpfdZn"
      },
      "source": [
        "### (b) (10 points) Model Training\n",
        "\n",
        "Train the neural network within Keras. Training should be done in a similar \n",
        "manner to **Exercise 5 (a)** using a `batch_size` equivalent to the size of the data set under `categorical_crossentropy` loss. Unlike before, please construct a graph that shows training and test behavior. This information is available on\n",
        "the `model.fit` call. \n",
        "\n",
        "Make sure the call to `.fit()` includes `validation_data = (x_test, y_test)` as specified in order to access `val_loss`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgc-msBhgtRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f435e6a6-d851-4343-9937-889e5d50674d"
      },
      "source": [
        "# code here\n",
        "model1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#\n",
        "#from keras import backend as K\n",
        "#K.set_value(model.optimizer.learning_rate, 0.5)\n",
        "model1_ = model1.fit(x_train.T, y_train_onehot.T, batch_size = 60000, epochs = 5000, validation_data = (x_test.T, y_test_onehot.T))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 2.2931 - accuracy: 0.1517 - val_loss: 2.2893 - val_accuracy: 0.1529\n",
            "Epoch 2/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.2888 - accuracy: 0.1554 - val_loss: 2.2851 - val_accuracy: 0.1562\n",
            "Epoch 3/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.2846 - accuracy: 0.1596 - val_loss: 2.2810 - val_accuracy: 0.1608\n",
            "Epoch 4/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.2805 - accuracy: 0.1635 - val_loss: 2.2770 - val_accuracy: 0.1650\n",
            "Epoch 5/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.2765 - accuracy: 0.1675 - val_loss: 2.2731 - val_accuracy: 0.1694\n",
            "Epoch 6/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.2726 - accuracy: 0.1718 - val_loss: 2.2692 - val_accuracy: 0.1732\n",
            "Epoch 7/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.2688 - accuracy: 0.1755 - val_loss: 2.2655 - val_accuracy: 0.1778\n",
            "Epoch 8/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.2650 - accuracy: 0.1790 - val_loss: 2.2618 - val_accuracy: 0.1820\n",
            "Epoch 9/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.2613 - accuracy: 0.1824 - val_loss: 2.2582 - val_accuracy: 0.1857\n",
            "Epoch 10/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.2577 - accuracy: 0.1858 - val_loss: 2.2547 - val_accuracy: 0.1907\n",
            "Epoch 11/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.2541 - accuracy: 0.1890 - val_loss: 2.2512 - val_accuracy: 0.1942\n",
            "Epoch 12/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.2507 - accuracy: 0.1927 - val_loss: 2.2478 - val_accuracy: 0.1973\n",
            "Epoch 13/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.2472 - accuracy: 0.1962 - val_loss: 2.2445 - val_accuracy: 0.2009\n",
            "Epoch 14/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.2439 - accuracy: 0.1993 - val_loss: 2.2412 - val_accuracy: 0.2044\n",
            "Epoch 15/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.2406 - accuracy: 0.2023 - val_loss: 2.2380 - val_accuracy: 0.2068\n",
            "Epoch 16/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.2374 - accuracy: 0.2049 - val_loss: 2.2348 - val_accuracy: 0.2096\n",
            "Epoch 17/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.2342 - accuracy: 0.2074 - val_loss: 2.2317 - val_accuracy: 0.2119\n",
            "Epoch 18/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 2.2311 - accuracy: 0.2098 - val_loss: 2.2286 - val_accuracy: 0.2134\n",
            "Epoch 19/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.2280 - accuracy: 0.2127 - val_loss: 2.2256 - val_accuracy: 0.2158\n",
            "Epoch 20/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.2250 - accuracy: 0.2150 - val_loss: 2.2227 - val_accuracy: 0.2187\n",
            "Epoch 21/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.2220 - accuracy: 0.2177 - val_loss: 2.2198 - val_accuracy: 0.2216\n",
            "Epoch 22/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.2191 - accuracy: 0.2203 - val_loss: 2.2169 - val_accuracy: 0.2240\n",
            "Epoch 23/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.2162 - accuracy: 0.2228 - val_loss: 2.2141 - val_accuracy: 0.2263\n",
            "Epoch 24/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.2134 - accuracy: 0.2254 - val_loss: 2.2113 - val_accuracy: 0.2291\n",
            "Epoch 25/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.2106 - accuracy: 0.2277 - val_loss: 2.2086 - val_accuracy: 0.2323\n",
            "Epoch 26/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.2079 - accuracy: 0.2300 - val_loss: 2.2059 - val_accuracy: 0.2352\n",
            "Epoch 27/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.2052 - accuracy: 0.2326 - val_loss: 2.2032 - val_accuracy: 0.2374\n",
            "Epoch 28/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.2025 - accuracy: 0.2350 - val_loss: 2.2006 - val_accuracy: 0.2404\n",
            "Epoch 29/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 2.1999 - accuracy: 0.2376 - val_loss: 2.1980 - val_accuracy: 0.2433\n",
            "Epoch 30/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.1973 - accuracy: 0.2399 - val_loss: 2.1955 - val_accuracy: 0.2470\n",
            "Epoch 31/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.1947 - accuracy: 0.2422 - val_loss: 2.1930 - val_accuracy: 0.2499\n",
            "Epoch 32/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.1922 - accuracy: 0.2449 - val_loss: 2.1905 - val_accuracy: 0.2538\n",
            "Epoch 33/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1897 - accuracy: 0.2479 - val_loss: 2.1881 - val_accuracy: 0.2571\n",
            "Epoch 34/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1872 - accuracy: 0.2509 - val_loss: 2.1856 - val_accuracy: 0.2606\n",
            "Epoch 35/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.1848 - accuracy: 0.2542 - val_loss: 2.1833 - val_accuracy: 0.2631\n",
            "Epoch 36/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.1824 - accuracy: 0.2576 - val_loss: 2.1809 - val_accuracy: 0.2654\n",
            "Epoch 37/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.1801 - accuracy: 0.2610 - val_loss: 2.1786 - val_accuracy: 0.2680\n",
            "Epoch 38/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1777 - accuracy: 0.2641 - val_loss: 2.1763 - val_accuracy: 0.2718\n",
            "Epoch 39/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.1754 - accuracy: 0.2677 - val_loss: 2.1740 - val_accuracy: 0.2750\n",
            "Epoch 40/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.1732 - accuracy: 0.2713 - val_loss: 2.1718 - val_accuracy: 0.2803\n",
            "Epoch 41/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.1709 - accuracy: 0.2751 - val_loss: 2.1696 - val_accuracy: 0.2830\n",
            "Epoch 42/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.1687 - accuracy: 0.2792 - val_loss: 2.1674 - val_accuracy: 0.2869\n",
            "Epoch 43/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.1665 - accuracy: 0.2834 - val_loss: 2.1652 - val_accuracy: 0.2912\n",
            "Epoch 44/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.1643 - accuracy: 0.2871 - val_loss: 2.1630 - val_accuracy: 0.2944\n",
            "Epoch 45/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 2.1621 - accuracy: 0.2912 - val_loss: 2.1609 - val_accuracy: 0.2998\n",
            "Epoch 46/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.1600 - accuracy: 0.2959 - val_loss: 2.1588 - val_accuracy: 0.3039\n",
            "Epoch 47/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1579 - accuracy: 0.3007 - val_loss: 2.1567 - val_accuracy: 0.3103\n",
            "Epoch 48/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.1558 - accuracy: 0.3052 - val_loss: 2.1546 - val_accuracy: 0.3148\n",
            "Epoch 49/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.1537 - accuracy: 0.3103 - val_loss: 2.1526 - val_accuracy: 0.3199\n",
            "Epoch 50/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.1516 - accuracy: 0.3160 - val_loss: 2.1505 - val_accuracy: 0.3252\n",
            "Epoch 51/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1496 - accuracy: 0.3211 - val_loss: 2.1485 - val_accuracy: 0.3297\n",
            "Epoch 52/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1475 - accuracy: 0.3258 - val_loss: 2.1465 - val_accuracy: 0.3348\n",
            "Epoch 53/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1455 - accuracy: 0.3301 - val_loss: 2.1445 - val_accuracy: 0.3401\n",
            "Epoch 54/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.1435 - accuracy: 0.3346 - val_loss: 2.1426 - val_accuracy: 0.3437\n",
            "Epoch 55/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1416 - accuracy: 0.3393 - val_loss: 2.1406 - val_accuracy: 0.3479\n",
            "Epoch 56/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.1396 - accuracy: 0.3429 - val_loss: 2.1387 - val_accuracy: 0.3512\n",
            "Epoch 57/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.1376 - accuracy: 0.3467 - val_loss: 2.1367 - val_accuracy: 0.3560\n",
            "Epoch 58/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.1357 - accuracy: 0.3510 - val_loss: 2.1348 - val_accuracy: 0.3603\n",
            "Epoch 59/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.1338 - accuracy: 0.3546 - val_loss: 2.1329 - val_accuracy: 0.3640\n",
            "Epoch 60/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1319 - accuracy: 0.3586 - val_loss: 2.1310 - val_accuracy: 0.3683\n",
            "Epoch 61/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 2.1300 - accuracy: 0.3623 - val_loss: 2.1291 - val_accuracy: 0.3714\n",
            "Epoch 62/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.1281 - accuracy: 0.3663 - val_loss: 2.1273 - val_accuracy: 0.3743\n",
            "Epoch 63/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1262 - accuracy: 0.3695 - val_loss: 2.1254 - val_accuracy: 0.3781\n",
            "Epoch 64/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1243 - accuracy: 0.3728 - val_loss: 2.1235 - val_accuracy: 0.3816\n",
            "Epoch 65/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.1225 - accuracy: 0.3757 - val_loss: 2.1217 - val_accuracy: 0.3844\n",
            "Epoch 66/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.1206 - accuracy: 0.3787 - val_loss: 2.1199 - val_accuracy: 0.3882\n",
            "Epoch 67/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.1188 - accuracy: 0.3812 - val_loss: 2.1181 - val_accuracy: 0.3914\n",
            "Epoch 68/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.1170 - accuracy: 0.3839 - val_loss: 2.1162 - val_accuracy: 0.3944\n",
            "Epoch 69/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1152 - accuracy: 0.3860 - val_loss: 2.1144 - val_accuracy: 0.3962\n",
            "Epoch 70/5000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 2.1133 - accuracy: 0.3887 - val_loss: 2.1126 - val_accuracy: 0.3983\n",
            "Epoch 71/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.1115 - accuracy: 0.3911 - val_loss: 2.1109 - val_accuracy: 0.4002\n",
            "Epoch 72/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.1097 - accuracy: 0.3937 - val_loss: 2.1091 - val_accuracy: 0.4019\n",
            "Epoch 73/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.1080 - accuracy: 0.3959 - val_loss: 2.1073 - val_accuracy: 0.4034\n",
            "Epoch 74/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 2.1062 - accuracy: 0.3985 - val_loss: 2.1055 - val_accuracy: 0.4051\n",
            "Epoch 75/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.1044 - accuracy: 0.4009 - val_loss: 2.1038 - val_accuracy: 0.4075\n",
            "Epoch 76/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.1026 - accuracy: 0.4033 - val_loss: 2.1020 - val_accuracy: 0.4088\n",
            "Epoch 77/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 2.1009 - accuracy: 0.4058 - val_loss: 2.1003 - val_accuracy: 0.4100\n",
            "Epoch 78/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0991 - accuracy: 0.4079 - val_loss: 2.0985 - val_accuracy: 0.4125\n",
            "Epoch 79/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0974 - accuracy: 0.4101 - val_loss: 2.0968 - val_accuracy: 0.4137\n",
            "Epoch 80/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0956 - accuracy: 0.4121 - val_loss: 2.0951 - val_accuracy: 0.4151\n",
            "Epoch 81/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.0939 - accuracy: 0.4138 - val_loss: 2.0933 - val_accuracy: 0.4156\n",
            "Epoch 82/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0922 - accuracy: 0.4160 - val_loss: 2.0916 - val_accuracy: 0.4175\n",
            "Epoch 83/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.0904 - accuracy: 0.4180 - val_loss: 2.0899 - val_accuracy: 0.4186\n",
            "Epoch 84/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.0887 - accuracy: 0.4200 - val_loss: 2.0882 - val_accuracy: 0.4204\n",
            "Epoch 85/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.0870 - accuracy: 0.4218 - val_loss: 2.0865 - val_accuracy: 0.4216\n",
            "Epoch 86/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.0853 - accuracy: 0.4237 - val_loss: 2.0848 - val_accuracy: 0.4231\n",
            "Epoch 87/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.0836 - accuracy: 0.4253 - val_loss: 2.0831 - val_accuracy: 0.4245\n",
            "Epoch 88/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.0819 - accuracy: 0.4269 - val_loss: 2.0814 - val_accuracy: 0.4264\n",
            "Epoch 89/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.0802 - accuracy: 0.4283 - val_loss: 2.0797 - val_accuracy: 0.4290\n",
            "Epoch 90/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.0785 - accuracy: 0.4298 - val_loss: 2.0780 - val_accuracy: 0.4308\n",
            "Epoch 91/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0768 - accuracy: 0.4317 - val_loss: 2.0763 - val_accuracy: 0.4327\n",
            "Epoch 92/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.0751 - accuracy: 0.4331 - val_loss: 2.0747 - val_accuracy: 0.4344\n",
            "Epoch 93/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.0734 - accuracy: 0.4348 - val_loss: 2.0730 - val_accuracy: 0.4362\n",
            "Epoch 94/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0717 - accuracy: 0.4364 - val_loss: 2.0713 - val_accuracy: 0.4379\n",
            "Epoch 95/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.0700 - accuracy: 0.4376 - val_loss: 2.0696 - val_accuracy: 0.4394\n",
            "Epoch 96/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0684 - accuracy: 0.4390 - val_loss: 2.0680 - val_accuracy: 0.4403\n",
            "Epoch 97/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.0667 - accuracy: 0.4409 - val_loss: 2.0663 - val_accuracy: 0.4422\n",
            "Epoch 98/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.0650 - accuracy: 0.4426 - val_loss: 2.0646 - val_accuracy: 0.4443\n",
            "Epoch 99/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0633 - accuracy: 0.4439 - val_loss: 2.0630 - val_accuracy: 0.4454\n",
            "Epoch 100/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.0617 - accuracy: 0.4453 - val_loss: 2.0613 - val_accuracy: 0.4466\n",
            "Epoch 101/5000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 2.0600 - accuracy: 0.4464 - val_loss: 2.0597 - val_accuracy: 0.4480\n",
            "Epoch 102/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0584 - accuracy: 0.4479 - val_loss: 2.0580 - val_accuracy: 0.4483\n",
            "Epoch 103/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.0567 - accuracy: 0.4491 - val_loss: 2.0563 - val_accuracy: 0.4492\n",
            "Epoch 104/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0550 - accuracy: 0.4505 - val_loss: 2.0547 - val_accuracy: 0.4507\n",
            "Epoch 105/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0534 - accuracy: 0.4515 - val_loss: 2.0530 - val_accuracy: 0.4521\n",
            "Epoch 106/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0517 - accuracy: 0.4527 - val_loss: 2.0514 - val_accuracy: 0.4523\n",
            "Epoch 107/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.0501 - accuracy: 0.4542 - val_loss: 2.0498 - val_accuracy: 0.4536\n",
            "Epoch 108/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.0484 - accuracy: 0.4554 - val_loss: 2.0481 - val_accuracy: 0.4538\n",
            "Epoch 109/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0468 - accuracy: 0.4568 - val_loss: 2.0465 - val_accuracy: 0.4547\n",
            "Epoch 110/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0451 - accuracy: 0.4579 - val_loss: 2.0448 - val_accuracy: 0.4558\n",
            "Epoch 111/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.0435 - accuracy: 0.4590 - val_loss: 2.0432 - val_accuracy: 0.4569\n",
            "Epoch 112/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0418 - accuracy: 0.4603 - val_loss: 2.0416 - val_accuracy: 0.4584\n",
            "Epoch 113/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0402 - accuracy: 0.4615 - val_loss: 2.0399 - val_accuracy: 0.4597\n",
            "Epoch 114/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 2.0386 - accuracy: 0.4627 - val_loss: 2.0383 - val_accuracy: 0.4600\n",
            "Epoch 115/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.0369 - accuracy: 0.4643 - val_loss: 2.0367 - val_accuracy: 0.4609\n",
            "Epoch 116/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0353 - accuracy: 0.4652 - val_loss: 2.0350 - val_accuracy: 0.4617\n",
            "Epoch 117/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.0336 - accuracy: 0.4666 - val_loss: 2.0334 - val_accuracy: 0.4629\n",
            "Epoch 118/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.0320 - accuracy: 0.4675 - val_loss: 2.0318 - val_accuracy: 0.4649\n",
            "Epoch 119/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0304 - accuracy: 0.4687 - val_loss: 2.0301 - val_accuracy: 0.4657\n",
            "Epoch 120/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0287 - accuracy: 0.4699 - val_loss: 2.0285 - val_accuracy: 0.4670\n",
            "Epoch 121/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0271 - accuracy: 0.4710 - val_loss: 2.0269 - val_accuracy: 0.4679\n",
            "Epoch 122/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.0255 - accuracy: 0.4719 - val_loss: 2.0253 - val_accuracy: 0.4692\n",
            "Epoch 123/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0239 - accuracy: 0.4726 - val_loss: 2.0236 - val_accuracy: 0.4697\n",
            "Epoch 124/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 2.0222 - accuracy: 0.4733 - val_loss: 2.0220 - val_accuracy: 0.4711\n",
            "Epoch 125/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 2.0206 - accuracy: 0.4742 - val_loss: 2.0204 - val_accuracy: 0.4714\n",
            "Epoch 126/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.0190 - accuracy: 0.4750 - val_loss: 2.0188 - val_accuracy: 0.4728\n",
            "Epoch 127/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.0173 - accuracy: 0.4760 - val_loss: 2.0171 - val_accuracy: 0.4736\n",
            "Epoch 128/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.0157 - accuracy: 0.4773 - val_loss: 2.0155 - val_accuracy: 0.4745\n",
            "Epoch 129/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 2.0141 - accuracy: 0.4783 - val_loss: 2.0139 - val_accuracy: 0.4751\n",
            "Epoch 130/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.0125 - accuracy: 0.4791 - val_loss: 2.0123 - val_accuracy: 0.4762\n",
            "Epoch 131/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0108 - accuracy: 0.4797 - val_loss: 2.0107 - val_accuracy: 0.4765\n",
            "Epoch 132/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 2.0092 - accuracy: 0.4806 - val_loss: 2.0091 - val_accuracy: 0.4769\n",
            "Epoch 133/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 2.0076 - accuracy: 0.4818 - val_loss: 2.0074 - val_accuracy: 0.4785\n",
            "Epoch 134/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 2.0060 - accuracy: 0.4827 - val_loss: 2.0058 - val_accuracy: 0.4798\n",
            "Epoch 135/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.0043 - accuracy: 0.4834 - val_loss: 2.0042 - val_accuracy: 0.4805\n",
            "Epoch 136/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 2.0027 - accuracy: 0.4846 - val_loss: 2.0026 - val_accuracy: 0.4816\n",
            "Epoch 137/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 2.0011 - accuracy: 0.4854 - val_loss: 2.0010 - val_accuracy: 0.4827\n",
            "Epoch 138/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.9995 - accuracy: 0.4860 - val_loss: 1.9994 - val_accuracy: 0.4829\n",
            "Epoch 139/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.9979 - accuracy: 0.4871 - val_loss: 1.9977 - val_accuracy: 0.4844\n",
            "Epoch 140/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.9962 - accuracy: 0.4881 - val_loss: 1.9961 - val_accuracy: 0.4852\n",
            "Epoch 141/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.9946 - accuracy: 0.4886 - val_loss: 1.9945 - val_accuracy: 0.4858\n",
            "Epoch 142/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.9930 - accuracy: 0.4893 - val_loss: 1.9929 - val_accuracy: 0.4867\n",
            "Epoch 143/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.9914 - accuracy: 0.4902 - val_loss: 1.9913 - val_accuracy: 0.4868\n",
            "Epoch 144/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.9898 - accuracy: 0.4911 - val_loss: 1.9897 - val_accuracy: 0.4877\n",
            "Epoch 145/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9881 - accuracy: 0.4920 - val_loss: 1.9881 - val_accuracy: 0.4890\n",
            "Epoch 146/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.9865 - accuracy: 0.4929 - val_loss: 1.9864 - val_accuracy: 0.4891\n",
            "Epoch 147/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.9849 - accuracy: 0.4937 - val_loss: 1.9848 - val_accuracy: 0.4898\n",
            "Epoch 148/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.9833 - accuracy: 0.4943 - val_loss: 1.9832 - val_accuracy: 0.4905\n",
            "Epoch 149/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.9817 - accuracy: 0.4951 - val_loss: 1.9816 - val_accuracy: 0.4917\n",
            "Epoch 150/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.9801 - accuracy: 0.4958 - val_loss: 1.9800 - val_accuracy: 0.4926\n",
            "Epoch 151/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9784 - accuracy: 0.4966 - val_loss: 1.9784 - val_accuracy: 0.4933\n",
            "Epoch 152/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.9768 - accuracy: 0.4976 - val_loss: 1.9768 - val_accuracy: 0.4938\n",
            "Epoch 153/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.9752 - accuracy: 0.4983 - val_loss: 1.9752 - val_accuracy: 0.4947\n",
            "Epoch 154/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.9736 - accuracy: 0.4991 - val_loss: 1.9736 - val_accuracy: 0.4955\n",
            "Epoch 155/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9720 - accuracy: 0.5000 - val_loss: 1.9719 - val_accuracy: 0.4960\n",
            "Epoch 156/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9704 - accuracy: 0.5006 - val_loss: 1.9703 - val_accuracy: 0.4965\n",
            "Epoch 157/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.9687 - accuracy: 0.5014 - val_loss: 1.9687 - val_accuracy: 0.4964\n",
            "Epoch 158/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.9671 - accuracy: 0.5024 - val_loss: 1.9671 - val_accuracy: 0.4968\n",
            "Epoch 159/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.9655 - accuracy: 0.5033 - val_loss: 1.9655 - val_accuracy: 0.4975\n",
            "Epoch 160/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.9639 - accuracy: 0.5042 - val_loss: 1.9639 - val_accuracy: 0.4986\n",
            "Epoch 161/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9623 - accuracy: 0.5051 - val_loss: 1.9623 - val_accuracy: 0.4991\n",
            "Epoch 162/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.9607 - accuracy: 0.5057 - val_loss: 1.9607 - val_accuracy: 0.4991\n",
            "Epoch 163/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.9591 - accuracy: 0.5063 - val_loss: 1.9591 - val_accuracy: 0.5003\n",
            "Epoch 164/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.9575 - accuracy: 0.5073 - val_loss: 1.9575 - val_accuracy: 0.5018\n",
            "Epoch 165/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9558 - accuracy: 0.5079 - val_loss: 1.9559 - val_accuracy: 0.5024\n",
            "Epoch 166/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.9542 - accuracy: 0.5087 - val_loss: 1.9542 - val_accuracy: 0.5036\n",
            "Epoch 167/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.9526 - accuracy: 0.5098 - val_loss: 1.9526 - val_accuracy: 0.5041\n",
            "Epoch 168/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.9510 - accuracy: 0.5106 - val_loss: 1.9510 - val_accuracy: 0.5044\n",
            "Epoch 169/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9494 - accuracy: 0.5113 - val_loss: 1.9494 - val_accuracy: 0.5051\n",
            "Epoch 170/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.9478 - accuracy: 0.5123 - val_loss: 1.9478 - val_accuracy: 0.5062\n",
            "Epoch 171/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.9462 - accuracy: 0.5130 - val_loss: 1.9462 - val_accuracy: 0.5076\n",
            "Epoch 172/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9446 - accuracy: 0.5137 - val_loss: 1.9446 - val_accuracy: 0.5085\n",
            "Epoch 173/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.9429 - accuracy: 0.5141 - val_loss: 1.9430 - val_accuracy: 0.5095\n",
            "Epoch 174/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.9413 - accuracy: 0.5148 - val_loss: 1.9414 - val_accuracy: 0.5103\n",
            "Epoch 175/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9397 - accuracy: 0.5156 - val_loss: 1.9398 - val_accuracy: 0.5110\n",
            "Epoch 176/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9381 - accuracy: 0.5164 - val_loss: 1.9382 - val_accuracy: 0.5114\n",
            "Epoch 177/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.9365 - accuracy: 0.5171 - val_loss: 1.9366 - val_accuracy: 0.5123\n",
            "Epoch 178/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.9349 - accuracy: 0.5180 - val_loss: 1.9350 - val_accuracy: 0.5132\n",
            "Epoch 179/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9333 - accuracy: 0.5188 - val_loss: 1.9334 - val_accuracy: 0.5137\n",
            "Epoch 180/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.9317 - accuracy: 0.5195 - val_loss: 1.9318 - val_accuracy: 0.5142\n",
            "Epoch 181/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.9301 - accuracy: 0.5203 - val_loss: 1.9302 - val_accuracy: 0.5144\n",
            "Epoch 182/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.9285 - accuracy: 0.5209 - val_loss: 1.9286 - val_accuracy: 0.5149\n",
            "Epoch 183/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.9269 - accuracy: 0.5216 - val_loss: 1.9270 - val_accuracy: 0.5157\n",
            "Epoch 184/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.9252 - accuracy: 0.5225 - val_loss: 1.9254 - val_accuracy: 0.5163\n",
            "Epoch 185/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.9236 - accuracy: 0.5234 - val_loss: 1.9238 - val_accuracy: 0.5171\n",
            "Epoch 186/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.9220 - accuracy: 0.5243 - val_loss: 1.9222 - val_accuracy: 0.5180\n",
            "Epoch 187/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9204 - accuracy: 0.5249 - val_loss: 1.9206 - val_accuracy: 0.5185\n",
            "Epoch 188/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.9188 - accuracy: 0.5254 - val_loss: 1.9190 - val_accuracy: 0.5192\n",
            "Epoch 189/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.9172 - accuracy: 0.5261 - val_loss: 1.9174 - val_accuracy: 0.5198\n",
            "Epoch 190/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.9156 - accuracy: 0.5267 - val_loss: 1.9158 - val_accuracy: 0.5208\n",
            "Epoch 191/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.9140 - accuracy: 0.5273 - val_loss: 1.9142 - val_accuracy: 0.5213\n",
            "Epoch 192/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.9124 - accuracy: 0.5282 - val_loss: 1.9126 - val_accuracy: 0.5220\n",
            "Epoch 193/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.9108 - accuracy: 0.5289 - val_loss: 1.9110 - val_accuracy: 0.5222\n",
            "Epoch 194/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.9092 - accuracy: 0.5296 - val_loss: 1.9094 - val_accuracy: 0.5231\n",
            "Epoch 195/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.9076 - accuracy: 0.5306 - val_loss: 1.9078 - val_accuracy: 0.5238\n",
            "Epoch 196/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.9060 - accuracy: 0.5311 - val_loss: 1.9062 - val_accuracy: 0.5246\n",
            "Epoch 197/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.9044 - accuracy: 0.5318 - val_loss: 1.9046 - val_accuracy: 0.5253\n",
            "Epoch 198/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.9028 - accuracy: 0.5326 - val_loss: 1.9030 - val_accuracy: 0.5265\n",
            "Epoch 199/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.9012 - accuracy: 0.5332 - val_loss: 1.9014 - val_accuracy: 0.5275\n",
            "Epoch 200/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.8996 - accuracy: 0.5338 - val_loss: 1.8998 - val_accuracy: 0.5278\n",
            "Epoch 201/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.8980 - accuracy: 0.5343 - val_loss: 1.8982 - val_accuracy: 0.5288\n",
            "Epoch 202/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.8964 - accuracy: 0.5349 - val_loss: 1.8966 - val_accuracy: 0.5292\n",
            "Epoch 203/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8948 - accuracy: 0.5356 - val_loss: 1.8950 - val_accuracy: 0.5300\n",
            "Epoch 204/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.8932 - accuracy: 0.5365 - val_loss: 1.8934 - val_accuracy: 0.5309\n",
            "Epoch 205/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.8916 - accuracy: 0.5371 - val_loss: 1.8918 - val_accuracy: 0.5317\n",
            "Epoch 206/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.8900 - accuracy: 0.5378 - val_loss: 1.8903 - val_accuracy: 0.5321\n",
            "Epoch 207/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.8884 - accuracy: 0.5385 - val_loss: 1.8887 - val_accuracy: 0.5330\n",
            "Epoch 208/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.8868 - accuracy: 0.5394 - val_loss: 1.8871 - val_accuracy: 0.5335\n",
            "Epoch 209/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.8852 - accuracy: 0.5399 - val_loss: 1.8855 - val_accuracy: 0.5341\n",
            "Epoch 210/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.8836 - accuracy: 0.5409 - val_loss: 1.8839 - val_accuracy: 0.5345\n",
            "Epoch 211/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.8820 - accuracy: 0.5414 - val_loss: 1.8823 - val_accuracy: 0.5355\n",
            "Epoch 212/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.8805 - accuracy: 0.5421 - val_loss: 1.8807 - val_accuracy: 0.5360\n",
            "Epoch 213/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8789 - accuracy: 0.5426 - val_loss: 1.8792 - val_accuracy: 0.5370\n",
            "Epoch 214/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.8773 - accuracy: 0.5434 - val_loss: 1.8776 - val_accuracy: 0.5374\n",
            "Epoch 215/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.8757 - accuracy: 0.5439 - val_loss: 1.8760 - val_accuracy: 0.5381\n",
            "Epoch 216/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8741 - accuracy: 0.5444 - val_loss: 1.8744 - val_accuracy: 0.5391\n",
            "Epoch 217/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.8725 - accuracy: 0.5447 - val_loss: 1.8728 - val_accuracy: 0.5395\n",
            "Epoch 218/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.8709 - accuracy: 0.5454 - val_loss: 1.8712 - val_accuracy: 0.5402\n",
            "Epoch 219/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8693 - accuracy: 0.5461 - val_loss: 1.8697 - val_accuracy: 0.5404\n",
            "Epoch 220/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.8678 - accuracy: 0.5468 - val_loss: 1.8681 - val_accuracy: 0.5410\n",
            "Epoch 221/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.8662 - accuracy: 0.5473 - val_loss: 1.8665 - val_accuracy: 0.5408\n",
            "Epoch 222/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.8646 - accuracy: 0.5479 - val_loss: 1.8649 - val_accuracy: 0.5415\n",
            "Epoch 223/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.8630 - accuracy: 0.5485 - val_loss: 1.8634 - val_accuracy: 0.5428\n",
            "Epoch 224/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.8614 - accuracy: 0.5493 - val_loss: 1.8618 - val_accuracy: 0.5431\n",
            "Epoch 225/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.8598 - accuracy: 0.5499 - val_loss: 1.8602 - val_accuracy: 0.5440\n",
            "Epoch 226/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.8583 - accuracy: 0.5505 - val_loss: 1.8586 - val_accuracy: 0.5442\n",
            "Epoch 227/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8567 - accuracy: 0.5511 - val_loss: 1.8571 - val_accuracy: 0.5446\n",
            "Epoch 228/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.8551 - accuracy: 0.5516 - val_loss: 1.8555 - val_accuracy: 0.5452\n",
            "Epoch 229/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8535 - accuracy: 0.5523 - val_loss: 1.8539 - val_accuracy: 0.5460\n",
            "Epoch 230/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.8520 - accuracy: 0.5528 - val_loss: 1.8524 - val_accuracy: 0.5469\n",
            "Epoch 231/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.8504 - accuracy: 0.5531 - val_loss: 1.8508 - val_accuracy: 0.5469\n",
            "Epoch 232/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.8488 - accuracy: 0.5537 - val_loss: 1.8492 - val_accuracy: 0.5473\n",
            "Epoch 233/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8472 - accuracy: 0.5544 - val_loss: 1.8477 - val_accuracy: 0.5485\n",
            "Epoch 234/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.8457 - accuracy: 0.5550 - val_loss: 1.8461 - val_accuracy: 0.5491\n",
            "Epoch 235/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.8441 - accuracy: 0.5554 - val_loss: 1.8445 - val_accuracy: 0.5496\n",
            "Epoch 236/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.8425 - accuracy: 0.5560 - val_loss: 1.8430 - val_accuracy: 0.5502\n",
            "Epoch 237/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.8410 - accuracy: 0.5566 - val_loss: 1.8414 - val_accuracy: 0.5505\n",
            "Epoch 238/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8394 - accuracy: 0.5571 - val_loss: 1.8398 - val_accuracy: 0.5512\n",
            "Epoch 239/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8378 - accuracy: 0.5577 - val_loss: 1.8383 - val_accuracy: 0.5519\n",
            "Epoch 240/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.8363 - accuracy: 0.5581 - val_loss: 1.8367 - val_accuracy: 0.5523\n",
            "Epoch 241/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.8347 - accuracy: 0.5586 - val_loss: 1.8352 - val_accuracy: 0.5524\n",
            "Epoch 242/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.8331 - accuracy: 0.5591 - val_loss: 1.8336 - val_accuracy: 0.5531\n",
            "Epoch 243/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.8316 - accuracy: 0.5597 - val_loss: 1.8321 - val_accuracy: 0.5538\n",
            "Epoch 244/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8300 - accuracy: 0.5604 - val_loss: 1.8305 - val_accuracy: 0.5543\n",
            "Epoch 245/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.8285 - accuracy: 0.5608 - val_loss: 1.8290 - val_accuracy: 0.5552\n",
            "Epoch 246/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.8269 - accuracy: 0.5613 - val_loss: 1.8274 - val_accuracy: 0.5557\n",
            "Epoch 247/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.8253 - accuracy: 0.5619 - val_loss: 1.8259 - val_accuracy: 0.5563\n",
            "Epoch 248/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.8238 - accuracy: 0.5625 - val_loss: 1.8243 - val_accuracy: 0.5569\n",
            "Epoch 249/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.8222 - accuracy: 0.5631 - val_loss: 1.8228 - val_accuracy: 0.5574\n",
            "Epoch 250/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.8207 - accuracy: 0.5637 - val_loss: 1.8212 - val_accuracy: 0.5579\n",
            "Epoch 251/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8191 - accuracy: 0.5642 - val_loss: 1.8197 - val_accuracy: 0.5584\n",
            "Epoch 252/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.8176 - accuracy: 0.5647 - val_loss: 1.8181 - val_accuracy: 0.5590\n",
            "Epoch 253/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.8160 - accuracy: 0.5650 - val_loss: 1.8166 - val_accuracy: 0.5595\n",
            "Epoch 254/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.8145 - accuracy: 0.5653 - val_loss: 1.8151 - val_accuracy: 0.5599\n",
            "Epoch 255/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.8129 - accuracy: 0.5659 - val_loss: 1.8135 - val_accuracy: 0.5603\n",
            "Epoch 256/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.8114 - accuracy: 0.5663 - val_loss: 1.8120 - val_accuracy: 0.5608\n",
            "Epoch 257/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.8098 - accuracy: 0.5667 - val_loss: 1.8104 - val_accuracy: 0.5612\n",
            "Epoch 258/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.8083 - accuracy: 0.5671 - val_loss: 1.8089 - val_accuracy: 0.5621\n",
            "Epoch 259/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.8068 - accuracy: 0.5677 - val_loss: 1.8074 - val_accuracy: 0.5627\n",
            "Epoch 260/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.8052 - accuracy: 0.5683 - val_loss: 1.8058 - val_accuracy: 0.5633\n",
            "Epoch 261/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.8037 - accuracy: 0.5688 - val_loss: 1.8043 - val_accuracy: 0.5646\n",
            "Epoch 262/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.8021 - accuracy: 0.5692 - val_loss: 1.8028 - val_accuracy: 0.5646\n",
            "Epoch 263/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.8006 - accuracy: 0.5696 - val_loss: 1.8012 - val_accuracy: 0.5654\n",
            "Epoch 264/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7991 - accuracy: 0.5703 - val_loss: 1.7997 - val_accuracy: 0.5657\n",
            "Epoch 265/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7975 - accuracy: 0.5707 - val_loss: 1.7982 - val_accuracy: 0.5659\n",
            "Epoch 266/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7960 - accuracy: 0.5713 - val_loss: 1.7967 - val_accuracy: 0.5661\n",
            "Epoch 267/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7945 - accuracy: 0.5718 - val_loss: 1.7951 - val_accuracy: 0.5664\n",
            "Epoch 268/5000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7930 - accuracy: 0.5724 - val_loss: 1.7936 - val_accuracy: 0.5666\n",
            "Epoch 269/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7914 - accuracy: 0.5730 - val_loss: 1.7921 - val_accuracy: 0.5669\n",
            "Epoch 270/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7899 - accuracy: 0.5738 - val_loss: 1.7906 - val_accuracy: 0.5675\n",
            "Epoch 271/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7884 - accuracy: 0.5743 - val_loss: 1.7891 - val_accuracy: 0.5677\n",
            "Epoch 272/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7869 - accuracy: 0.5750 - val_loss: 1.7876 - val_accuracy: 0.5679\n",
            "Epoch 273/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7853 - accuracy: 0.5754 - val_loss: 1.7860 - val_accuracy: 0.5680\n",
            "Epoch 274/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7838 - accuracy: 0.5760 - val_loss: 1.7845 - val_accuracy: 0.5686\n",
            "Epoch 275/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7823 - accuracy: 0.5763 - val_loss: 1.7830 - val_accuracy: 0.5691\n",
            "Epoch 276/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7808 - accuracy: 0.5768 - val_loss: 1.7815 - val_accuracy: 0.5697\n",
            "Epoch 277/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.7793 - accuracy: 0.5774 - val_loss: 1.7800 - val_accuracy: 0.5701\n",
            "Epoch 278/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7777 - accuracy: 0.5777 - val_loss: 1.7785 - val_accuracy: 0.5703\n",
            "Epoch 279/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7762 - accuracy: 0.5780 - val_loss: 1.7770 - val_accuracy: 0.5707\n",
            "Epoch 280/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7747 - accuracy: 0.5786 - val_loss: 1.7755 - val_accuracy: 0.5711\n",
            "Epoch 281/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7732 - accuracy: 0.5792 - val_loss: 1.7740 - val_accuracy: 0.5719\n",
            "Epoch 282/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7717 - accuracy: 0.5795 - val_loss: 1.7725 - val_accuracy: 0.5722\n",
            "Epoch 283/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7702 - accuracy: 0.5801 - val_loss: 1.7710 - val_accuracy: 0.5730\n",
            "Epoch 284/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.7687 - accuracy: 0.5804 - val_loss: 1.7695 - val_accuracy: 0.5738\n",
            "Epoch 285/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7672 - accuracy: 0.5809 - val_loss: 1.7680 - val_accuracy: 0.5743\n",
            "Epoch 286/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.7657 - accuracy: 0.5813 - val_loss: 1.7665 - val_accuracy: 0.5747\n",
            "Epoch 287/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7642 - accuracy: 0.5817 - val_loss: 1.7650 - val_accuracy: 0.5751\n",
            "Epoch 288/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7627 - accuracy: 0.5821 - val_loss: 1.7635 - val_accuracy: 0.5757\n",
            "Epoch 289/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7612 - accuracy: 0.5827 - val_loss: 1.7620 - val_accuracy: 0.5762\n",
            "Epoch 290/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7597 - accuracy: 0.5832 - val_loss: 1.7605 - val_accuracy: 0.5764\n",
            "Epoch 291/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7582 - accuracy: 0.5838 - val_loss: 1.7590 - val_accuracy: 0.5765\n",
            "Epoch 292/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7567 - accuracy: 0.5841 - val_loss: 1.7576 - val_accuracy: 0.5769\n",
            "Epoch 293/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7552 - accuracy: 0.5844 - val_loss: 1.7561 - val_accuracy: 0.5772\n",
            "Epoch 294/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.7537 - accuracy: 0.5850 - val_loss: 1.7546 - val_accuracy: 0.5779\n",
            "Epoch 295/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7522 - accuracy: 0.5857 - val_loss: 1.7531 - val_accuracy: 0.5788\n",
            "Epoch 296/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7508 - accuracy: 0.5861 - val_loss: 1.7516 - val_accuracy: 0.5792\n",
            "Epoch 297/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7493 - accuracy: 0.5864 - val_loss: 1.7502 - val_accuracy: 0.5794\n",
            "Epoch 298/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7478 - accuracy: 0.5867 - val_loss: 1.7487 - val_accuracy: 0.5798\n",
            "Epoch 299/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.7463 - accuracy: 0.5871 - val_loss: 1.7472 - val_accuracy: 0.5801\n",
            "Epoch 300/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7448 - accuracy: 0.5874 - val_loss: 1.7457 - val_accuracy: 0.5802\n",
            "Epoch 301/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7434 - accuracy: 0.5878 - val_loss: 1.7443 - val_accuracy: 0.5808\n",
            "Epoch 302/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7419 - accuracy: 0.5883 - val_loss: 1.7428 - val_accuracy: 0.5811\n",
            "Epoch 303/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7404 - accuracy: 0.5887 - val_loss: 1.7413 - val_accuracy: 0.5817\n",
            "Epoch 304/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7389 - accuracy: 0.5892 - val_loss: 1.7399 - val_accuracy: 0.5822\n",
            "Epoch 305/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7375 - accuracy: 0.5897 - val_loss: 1.7384 - val_accuracy: 0.5826\n",
            "Epoch 306/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7360 - accuracy: 0.5899 - val_loss: 1.7369 - val_accuracy: 0.5827\n",
            "Epoch 307/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7345 - accuracy: 0.5902 - val_loss: 1.7355 - val_accuracy: 0.5832\n",
            "Epoch 308/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.7331 - accuracy: 0.5905 - val_loss: 1.7340 - val_accuracy: 0.5836\n",
            "Epoch 309/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7316 - accuracy: 0.5910 - val_loss: 1.7326 - val_accuracy: 0.5840\n",
            "Epoch 310/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.7301 - accuracy: 0.5914 - val_loss: 1.7311 - val_accuracy: 0.5845\n",
            "Epoch 311/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.7287 - accuracy: 0.5918 - val_loss: 1.7297 - val_accuracy: 0.5847\n",
            "Epoch 312/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7272 - accuracy: 0.5922 - val_loss: 1.7282 - val_accuracy: 0.5850\n",
            "Epoch 313/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7258 - accuracy: 0.5926 - val_loss: 1.7268 - val_accuracy: 0.5856\n",
            "Epoch 314/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.7243 - accuracy: 0.5929 - val_loss: 1.7253 - val_accuracy: 0.5861\n",
            "Epoch 315/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7228 - accuracy: 0.5934 - val_loss: 1.7239 - val_accuracy: 0.5865\n",
            "Epoch 316/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.7214 - accuracy: 0.5937 - val_loss: 1.7224 - val_accuracy: 0.5874\n",
            "Epoch 317/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.7199 - accuracy: 0.5941 - val_loss: 1.7210 - val_accuracy: 0.5879\n",
            "Epoch 318/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7185 - accuracy: 0.5945 - val_loss: 1.7195 - val_accuracy: 0.5879\n",
            "Epoch 319/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7171 - accuracy: 0.5950 - val_loss: 1.7181 - val_accuracy: 0.5886\n",
            "Epoch 320/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.7156 - accuracy: 0.5954 - val_loss: 1.7167 - val_accuracy: 0.5889\n",
            "Epoch 321/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7142 - accuracy: 0.5957 - val_loss: 1.7152 - val_accuracy: 0.5893\n",
            "Epoch 322/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.7127 - accuracy: 0.5960 - val_loss: 1.7138 - val_accuracy: 0.5894\n",
            "Epoch 323/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.7113 - accuracy: 0.5964 - val_loss: 1.7124 - val_accuracy: 0.5897\n",
            "Epoch 324/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7098 - accuracy: 0.5968 - val_loss: 1.7109 - val_accuracy: 0.5901\n",
            "Epoch 325/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.7084 - accuracy: 0.5971 - val_loss: 1.7095 - val_accuracy: 0.5908\n",
            "Epoch 326/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.7070 - accuracy: 0.5975 - val_loss: 1.7081 - val_accuracy: 0.5908\n",
            "Epoch 327/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.7055 - accuracy: 0.5977 - val_loss: 1.7067 - val_accuracy: 0.5912\n",
            "Epoch 328/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7041 - accuracy: 0.5981 - val_loss: 1.7052 - val_accuracy: 0.5914\n",
            "Epoch 329/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.7027 - accuracy: 0.5987 - val_loss: 1.7038 - val_accuracy: 0.5916\n",
            "Epoch 330/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.7013 - accuracy: 0.5990 - val_loss: 1.7024 - val_accuracy: 0.5917\n",
            "Epoch 331/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6998 - accuracy: 0.5993 - val_loss: 1.7010 - val_accuracy: 0.5927\n",
            "Epoch 332/5000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 1.6984 - accuracy: 0.5998 - val_loss: 1.6996 - val_accuracy: 0.5935\n",
            "Epoch 333/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6970 - accuracy: 0.6002 - val_loss: 1.6982 - val_accuracy: 0.5937\n",
            "Epoch 334/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6956 - accuracy: 0.6005 - val_loss: 1.6967 - val_accuracy: 0.5941\n",
            "Epoch 335/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6942 - accuracy: 0.6007 - val_loss: 1.6953 - val_accuracy: 0.5946\n",
            "Epoch 336/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6927 - accuracy: 0.6013 - val_loss: 1.6939 - val_accuracy: 0.5948\n",
            "Epoch 337/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6913 - accuracy: 0.6017 - val_loss: 1.6925 - val_accuracy: 0.5955\n",
            "Epoch 338/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6899 - accuracy: 0.6020 - val_loss: 1.6911 - val_accuracy: 0.5961\n",
            "Epoch 339/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6885 - accuracy: 0.6023 - val_loss: 1.6897 - val_accuracy: 0.5964\n",
            "Epoch 340/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.6871 - accuracy: 0.6026 - val_loss: 1.6883 - val_accuracy: 0.5969\n",
            "Epoch 341/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6857 - accuracy: 0.6029 - val_loss: 1.6869 - val_accuracy: 0.5969\n",
            "Epoch 342/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6843 - accuracy: 0.6033 - val_loss: 1.6855 - val_accuracy: 0.5971\n",
            "Epoch 343/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6829 - accuracy: 0.6039 - val_loss: 1.6841 - val_accuracy: 0.5975\n",
            "Epoch 344/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6815 - accuracy: 0.6042 - val_loss: 1.6827 - val_accuracy: 0.5980\n",
            "Epoch 345/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6801 - accuracy: 0.6046 - val_loss: 1.6813 - val_accuracy: 0.5984\n",
            "Epoch 346/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6787 - accuracy: 0.6048 - val_loss: 1.6800 - val_accuracy: 0.5989\n",
            "Epoch 347/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6773 - accuracy: 0.6052 - val_loss: 1.6786 - val_accuracy: 0.5996\n",
            "Epoch 348/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6759 - accuracy: 0.6054 - val_loss: 1.6772 - val_accuracy: 0.5999\n",
            "Epoch 349/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.6745 - accuracy: 0.6057 - val_loss: 1.6758 - val_accuracy: 0.6002\n",
            "Epoch 350/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6731 - accuracy: 0.6062 - val_loss: 1.6744 - val_accuracy: 0.6007\n",
            "Epoch 351/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6717 - accuracy: 0.6065 - val_loss: 1.6730 - val_accuracy: 0.6010\n",
            "Epoch 352/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6703 - accuracy: 0.6069 - val_loss: 1.6717 - val_accuracy: 0.6011\n",
            "Epoch 353/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6690 - accuracy: 0.6073 - val_loss: 1.6703 - val_accuracy: 0.6014\n",
            "Epoch 354/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6676 - accuracy: 0.6076 - val_loss: 1.6689 - val_accuracy: 0.6014\n",
            "Epoch 355/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6662 - accuracy: 0.6079 - val_loss: 1.6675 - val_accuracy: 0.6019\n",
            "Epoch 356/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6648 - accuracy: 0.6080 - val_loss: 1.6662 - val_accuracy: 0.6025\n",
            "Epoch 357/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6634 - accuracy: 0.6082 - val_loss: 1.6648 - val_accuracy: 0.6027\n",
            "Epoch 358/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6621 - accuracy: 0.6086 - val_loss: 1.6634 - val_accuracy: 0.6032\n",
            "Epoch 359/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6607 - accuracy: 0.6089 - val_loss: 1.6621 - val_accuracy: 0.6037\n",
            "Epoch 360/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6593 - accuracy: 0.6092 - val_loss: 1.6607 - val_accuracy: 0.6040\n",
            "Epoch 361/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6580 - accuracy: 0.6097 - val_loss: 1.6594 - val_accuracy: 0.6046\n",
            "Epoch 362/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6566 - accuracy: 0.6101 - val_loss: 1.6580 - val_accuracy: 0.6055\n",
            "Epoch 363/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6552 - accuracy: 0.6102 - val_loss: 1.6566 - val_accuracy: 0.6056\n",
            "Epoch 364/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.6539 - accuracy: 0.6105 - val_loss: 1.6553 - val_accuracy: 0.6058\n",
            "Epoch 365/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6525 - accuracy: 0.6108 - val_loss: 1.6539 - val_accuracy: 0.6062\n",
            "Epoch 366/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6512 - accuracy: 0.6113 - val_loss: 1.6526 - val_accuracy: 0.6063\n",
            "Epoch 367/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6498 - accuracy: 0.6115 - val_loss: 1.6512 - val_accuracy: 0.6066\n",
            "Epoch 368/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6484 - accuracy: 0.6118 - val_loss: 1.6499 - val_accuracy: 0.6066\n",
            "Epoch 369/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6471 - accuracy: 0.6120 - val_loss: 1.6486 - val_accuracy: 0.6066\n",
            "Epoch 370/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.6457 - accuracy: 0.6122 - val_loss: 1.6472 - val_accuracy: 0.6066\n",
            "Epoch 371/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6444 - accuracy: 0.6125 - val_loss: 1.6459 - val_accuracy: 0.6067\n",
            "Epoch 372/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6430 - accuracy: 0.6127 - val_loss: 1.6445 - val_accuracy: 0.6072\n",
            "Epoch 373/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6417 - accuracy: 0.6132 - val_loss: 1.6432 - val_accuracy: 0.6076\n",
            "Epoch 374/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6404 - accuracy: 0.6133 - val_loss: 1.6419 - val_accuracy: 0.6076\n",
            "Epoch 375/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6390 - accuracy: 0.6136 - val_loss: 1.6405 - val_accuracy: 0.6079\n",
            "Epoch 376/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.6377 - accuracy: 0.6140 - val_loss: 1.6392 - val_accuracy: 0.6089\n",
            "Epoch 377/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6363 - accuracy: 0.6142 - val_loss: 1.6379 - val_accuracy: 0.6094\n",
            "Epoch 378/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6350 - accuracy: 0.6146 - val_loss: 1.6365 - val_accuracy: 0.6094\n",
            "Epoch 379/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.6337 - accuracy: 0.6149 - val_loss: 1.6352 - val_accuracy: 0.6100\n",
            "Epoch 380/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6323 - accuracy: 0.6151 - val_loss: 1.6339 - val_accuracy: 0.6105\n",
            "Epoch 381/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6310 - accuracy: 0.6152 - val_loss: 1.6326 - val_accuracy: 0.6105\n",
            "Epoch 382/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6297 - accuracy: 0.6154 - val_loss: 1.6313 - val_accuracy: 0.6106\n",
            "Epoch 383/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6284 - accuracy: 0.6157 - val_loss: 1.6299 - val_accuracy: 0.6107\n",
            "Epoch 384/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.6270 - accuracy: 0.6160 - val_loss: 1.6286 - val_accuracy: 0.6110\n",
            "Epoch 385/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6257 - accuracy: 0.6162 - val_loss: 1.6273 - val_accuracy: 0.6115\n",
            "Epoch 386/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.6244 - accuracy: 0.6166 - val_loss: 1.6260 - val_accuracy: 0.6116\n",
            "Epoch 387/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6231 - accuracy: 0.6167 - val_loss: 1.6247 - val_accuracy: 0.6118\n",
            "Epoch 388/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6218 - accuracy: 0.6172 - val_loss: 1.6234 - val_accuracy: 0.6121\n",
            "Epoch 389/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6205 - accuracy: 0.6174 - val_loss: 1.6221 - val_accuracy: 0.6123\n",
            "Epoch 390/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.6192 - accuracy: 0.6178 - val_loss: 1.6208 - val_accuracy: 0.6123\n",
            "Epoch 391/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6178 - accuracy: 0.6181 - val_loss: 1.6195 - val_accuracy: 0.6126\n",
            "Epoch 392/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6165 - accuracy: 0.6183 - val_loss: 1.6182 - val_accuracy: 0.6129\n",
            "Epoch 393/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.6152 - accuracy: 0.6184 - val_loss: 1.6169 - val_accuracy: 0.6130\n",
            "Epoch 394/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6139 - accuracy: 0.6187 - val_loss: 1.6156 - val_accuracy: 0.6135\n",
            "Epoch 395/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6126 - accuracy: 0.6189 - val_loss: 1.6143 - val_accuracy: 0.6139\n",
            "Epoch 396/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.6113 - accuracy: 0.6191 - val_loss: 1.6130 - val_accuracy: 0.6141\n",
            "Epoch 397/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.6100 - accuracy: 0.6192 - val_loss: 1.6117 - val_accuracy: 0.6140\n",
            "Epoch 398/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6087 - accuracy: 0.6194 - val_loss: 1.6104 - val_accuracy: 0.6142\n",
            "Epoch 399/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.6075 - accuracy: 0.6198 - val_loss: 1.6092 - val_accuracy: 0.6143\n",
            "Epoch 400/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6062 - accuracy: 0.6199 - val_loss: 1.6079 - val_accuracy: 0.6142\n",
            "Epoch 401/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6049 - accuracy: 0.6202 - val_loss: 1.6066 - val_accuracy: 0.6147\n",
            "Epoch 402/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.6036 - accuracy: 0.6204 - val_loss: 1.6053 - val_accuracy: 0.6150\n",
            "Epoch 403/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.6023 - accuracy: 0.6207 - val_loss: 1.6040 - val_accuracy: 0.6152\n",
            "Epoch 404/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.6010 - accuracy: 0.6210 - val_loss: 1.6028 - val_accuracy: 0.6151\n",
            "Epoch 405/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.5997 - accuracy: 0.6214 - val_loss: 1.6015 - val_accuracy: 0.6155\n",
            "Epoch 406/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.5985 - accuracy: 0.6216 - val_loss: 1.6002 - val_accuracy: 0.6156\n",
            "Epoch 407/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.5972 - accuracy: 0.6217 - val_loss: 1.5990 - val_accuracy: 0.6159\n",
            "Epoch 408/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.5959 - accuracy: 0.6219 - val_loss: 1.5977 - val_accuracy: 0.6160\n",
            "Epoch 409/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.5946 - accuracy: 0.6221 - val_loss: 1.5964 - val_accuracy: 0.6163\n",
            "Epoch 410/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.5934 - accuracy: 0.6223 - val_loss: 1.5952 - val_accuracy: 0.6168\n",
            "Epoch 411/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.5921 - accuracy: 0.6226 - val_loss: 1.5939 - val_accuracy: 0.6169\n",
            "Epoch 412/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5908 - accuracy: 0.6228 - val_loss: 1.5927 - val_accuracy: 0.6172\n",
            "Epoch 413/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.5896 - accuracy: 0.6228 - val_loss: 1.5914 - val_accuracy: 0.6174\n",
            "Epoch 414/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.5883 - accuracy: 0.6231 - val_loss: 1.5901 - val_accuracy: 0.6175\n",
            "Epoch 415/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.5871 - accuracy: 0.6234 - val_loss: 1.5889 - val_accuracy: 0.6180\n",
            "Epoch 416/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.5858 - accuracy: 0.6236 - val_loss: 1.5876 - val_accuracy: 0.6182\n",
            "Epoch 417/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.5845 - accuracy: 0.6238 - val_loss: 1.5864 - val_accuracy: 0.6185\n",
            "Epoch 418/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.5833 - accuracy: 0.6240 - val_loss: 1.5852 - val_accuracy: 0.6188\n",
            "Epoch 419/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.5820 - accuracy: 0.6243 - val_loss: 1.5839 - val_accuracy: 0.6191\n",
            "Epoch 420/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5808 - accuracy: 0.6246 - val_loss: 1.5827 - val_accuracy: 0.6192\n",
            "Epoch 421/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5795 - accuracy: 0.6246 - val_loss: 1.5814 - val_accuracy: 0.6192\n",
            "Epoch 422/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.5783 - accuracy: 0.6247 - val_loss: 1.5802 - val_accuracy: 0.6195\n",
            "Epoch 423/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5771 - accuracy: 0.6250 - val_loss: 1.5790 - val_accuracy: 0.6197\n",
            "Epoch 424/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.5758 - accuracy: 0.6253 - val_loss: 1.5777 - val_accuracy: 0.6197\n",
            "Epoch 425/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.5746 - accuracy: 0.6256 - val_loss: 1.5765 - val_accuracy: 0.6198\n",
            "Epoch 426/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.5733 - accuracy: 0.6259 - val_loss: 1.5753 - val_accuracy: 0.6196\n",
            "Epoch 427/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5721 - accuracy: 0.6263 - val_loss: 1.5740 - val_accuracy: 0.6196\n",
            "Epoch 428/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.5709 - accuracy: 0.6265 - val_loss: 1.5728 - val_accuracy: 0.6198\n",
            "Epoch 429/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.5696 - accuracy: 0.6266 - val_loss: 1.5716 - val_accuracy: 0.6200\n",
            "Epoch 430/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.5684 - accuracy: 0.6270 - val_loss: 1.5704 - val_accuracy: 0.6203\n",
            "Epoch 431/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5672 - accuracy: 0.6271 - val_loss: 1.5691 - val_accuracy: 0.6203\n",
            "Epoch 432/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.5660 - accuracy: 0.6273 - val_loss: 1.5679 - val_accuracy: 0.6205\n",
            "Epoch 433/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5647 - accuracy: 0.6275 - val_loss: 1.5667 - val_accuracy: 0.6206\n",
            "Epoch 434/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.5635 - accuracy: 0.6276 - val_loss: 1.5655 - val_accuracy: 0.6206\n",
            "Epoch 435/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.5623 - accuracy: 0.6278 - val_loss: 1.5643 - val_accuracy: 0.6209\n",
            "Epoch 436/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.5611 - accuracy: 0.6279 - val_loss: 1.5631 - val_accuracy: 0.6210\n",
            "Epoch 437/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.5599 - accuracy: 0.6280 - val_loss: 1.5619 - val_accuracy: 0.6211\n",
            "Epoch 438/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.5587 - accuracy: 0.6282 - val_loss: 1.5607 - val_accuracy: 0.6214\n",
            "Epoch 439/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.5574 - accuracy: 0.6283 - val_loss: 1.5595 - val_accuracy: 0.6217\n",
            "Epoch 440/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.5562 - accuracy: 0.6284 - val_loss: 1.5583 - val_accuracy: 0.6221\n",
            "Epoch 441/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.5550 - accuracy: 0.6285 - val_loss: 1.5571 - val_accuracy: 0.6228\n",
            "Epoch 442/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.5538 - accuracy: 0.6287 - val_loss: 1.5559 - val_accuracy: 0.6230\n",
            "Epoch 443/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5526 - accuracy: 0.6290 - val_loss: 1.5547 - val_accuracy: 0.6232\n",
            "Epoch 444/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.5514 - accuracy: 0.6292 - val_loss: 1.5535 - val_accuracy: 0.6232\n",
            "Epoch 445/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.5502 - accuracy: 0.6293 - val_loss: 1.5523 - val_accuracy: 0.6236\n",
            "Epoch 446/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5490 - accuracy: 0.6295 - val_loss: 1.5511 - val_accuracy: 0.6236\n",
            "Epoch 447/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.5478 - accuracy: 0.6296 - val_loss: 1.5499 - val_accuracy: 0.6234\n",
            "Epoch 448/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5466 - accuracy: 0.6299 - val_loss: 1.5487 - val_accuracy: 0.6237\n",
            "Epoch 449/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.5454 - accuracy: 0.6300 - val_loss: 1.5475 - val_accuracy: 0.6238\n",
            "Epoch 450/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.5443 - accuracy: 0.6302 - val_loss: 1.5464 - val_accuracy: 0.6240\n",
            "Epoch 451/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.5431 - accuracy: 0.6304 - val_loss: 1.5452 - val_accuracy: 0.6243\n",
            "Epoch 452/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.5419 - accuracy: 0.6305 - val_loss: 1.5440 - val_accuracy: 0.6245\n",
            "Epoch 453/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.5407 - accuracy: 0.6307 - val_loss: 1.5428 - val_accuracy: 0.6245\n",
            "Epoch 454/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5395 - accuracy: 0.6309 - val_loss: 1.5417 - val_accuracy: 0.6247\n",
            "Epoch 455/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.5383 - accuracy: 0.6310 - val_loss: 1.5405 - val_accuracy: 0.6250\n",
            "Epoch 456/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.5372 - accuracy: 0.6312 - val_loss: 1.5393 - val_accuracy: 0.6254\n",
            "Epoch 457/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.5360 - accuracy: 0.6313 - val_loss: 1.5381 - val_accuracy: 0.6259\n",
            "Epoch 458/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5348 - accuracy: 0.6316 - val_loss: 1.5370 - val_accuracy: 0.6267\n",
            "Epoch 459/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.5336 - accuracy: 0.6317 - val_loss: 1.5358 - val_accuracy: 0.6269\n",
            "Epoch 460/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5325 - accuracy: 0.6320 - val_loss: 1.5347 - val_accuracy: 0.6272\n",
            "Epoch 461/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.5313 - accuracy: 0.6320 - val_loss: 1.5335 - val_accuracy: 0.6274\n",
            "Epoch 462/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.5301 - accuracy: 0.6321 - val_loss: 1.5323 - val_accuracy: 0.6277\n",
            "Epoch 463/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.5290 - accuracy: 0.6320 - val_loss: 1.5312 - val_accuracy: 0.6277\n",
            "Epoch 464/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.5278 - accuracy: 0.6322 - val_loss: 1.5300 - val_accuracy: 0.6275\n",
            "Epoch 465/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.5267 - accuracy: 0.6323 - val_loss: 1.5289 - val_accuracy: 0.6276\n",
            "Epoch 466/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.5255 - accuracy: 0.6323 - val_loss: 1.5277 - val_accuracy: 0.6277\n",
            "Epoch 467/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5243 - accuracy: 0.6324 - val_loss: 1.5266 - val_accuracy: 0.6277\n",
            "Epoch 468/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.5232 - accuracy: 0.6326 - val_loss: 1.5254 - val_accuracy: 0.6279\n",
            "Epoch 469/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.5220 - accuracy: 0.6328 - val_loss: 1.5243 - val_accuracy: 0.6277\n",
            "Epoch 470/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.5209 - accuracy: 0.6330 - val_loss: 1.5232 - val_accuracy: 0.6276\n",
            "Epoch 471/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.5197 - accuracy: 0.6331 - val_loss: 1.5220 - val_accuracy: 0.6277\n",
            "Epoch 472/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.5186 - accuracy: 0.6332 - val_loss: 1.5209 - val_accuracy: 0.6276\n",
            "Epoch 473/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.5175 - accuracy: 0.6334 - val_loss: 1.5197 - val_accuracy: 0.6276\n",
            "Epoch 474/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.5163 - accuracy: 0.6336 - val_loss: 1.5186 - val_accuracy: 0.6276\n",
            "Epoch 475/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5152 - accuracy: 0.6337 - val_loss: 1.5175 - val_accuracy: 0.6278\n",
            "Epoch 476/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5140 - accuracy: 0.6338 - val_loss: 1.5163 - val_accuracy: 0.6282\n",
            "Epoch 477/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.5129 - accuracy: 0.6340 - val_loss: 1.5152 - val_accuracy: 0.6281\n",
            "Epoch 478/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.5118 - accuracy: 0.6341 - val_loss: 1.5141 - val_accuracy: 0.6281\n",
            "Epoch 479/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.5106 - accuracy: 0.6342 - val_loss: 1.5130 - val_accuracy: 0.6280\n",
            "Epoch 480/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.5095 - accuracy: 0.6344 - val_loss: 1.5119 - val_accuracy: 0.6278\n",
            "Epoch 481/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.5084 - accuracy: 0.6345 - val_loss: 1.5107 - val_accuracy: 0.6279\n",
            "Epoch 482/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.5073 - accuracy: 0.6347 - val_loss: 1.5096 - val_accuracy: 0.6278\n",
            "Epoch 483/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.5061 - accuracy: 0.6348 - val_loss: 1.5085 - val_accuracy: 0.6280\n",
            "Epoch 484/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.5050 - accuracy: 0.6351 - val_loss: 1.5074 - val_accuracy: 0.6283\n",
            "Epoch 485/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.5039 - accuracy: 0.6352 - val_loss: 1.5063 - val_accuracy: 0.6284\n",
            "Epoch 486/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.5028 - accuracy: 0.6352 - val_loss: 1.5052 - val_accuracy: 0.6285\n",
            "Epoch 487/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.5017 - accuracy: 0.6354 - val_loss: 1.5041 - val_accuracy: 0.6289\n",
            "Epoch 488/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.5005 - accuracy: 0.6357 - val_loss: 1.5029 - val_accuracy: 0.6292\n",
            "Epoch 489/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.4994 - accuracy: 0.6357 - val_loss: 1.5018 - val_accuracy: 0.6294\n",
            "Epoch 490/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4983 - accuracy: 0.6359 - val_loss: 1.5007 - val_accuracy: 0.6293\n",
            "Epoch 491/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.4972 - accuracy: 0.6360 - val_loss: 1.4996 - val_accuracy: 0.6293\n",
            "Epoch 492/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4961 - accuracy: 0.6361 - val_loss: 1.4985 - val_accuracy: 0.6294\n",
            "Epoch 493/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.4950 - accuracy: 0.6362 - val_loss: 1.4974 - val_accuracy: 0.6295\n",
            "Epoch 494/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4939 - accuracy: 0.6362 - val_loss: 1.4963 - val_accuracy: 0.6299\n",
            "Epoch 495/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.4928 - accuracy: 0.6363 - val_loss: 1.4953 - val_accuracy: 0.6300\n",
            "Epoch 496/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.4917 - accuracy: 0.6364 - val_loss: 1.4942 - val_accuracy: 0.6303\n",
            "Epoch 497/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.4906 - accuracy: 0.6365 - val_loss: 1.4931 - val_accuracy: 0.6307\n",
            "Epoch 498/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.4895 - accuracy: 0.6366 - val_loss: 1.4920 - val_accuracy: 0.6309\n",
            "Epoch 499/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.4884 - accuracy: 0.6367 - val_loss: 1.4909 - val_accuracy: 0.6310\n",
            "Epoch 500/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4873 - accuracy: 0.6368 - val_loss: 1.4898 - val_accuracy: 0.6309\n",
            "Epoch 501/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4862 - accuracy: 0.6369 - val_loss: 1.4887 - val_accuracy: 0.6308\n",
            "Epoch 502/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4851 - accuracy: 0.6371 - val_loss: 1.4877 - val_accuracy: 0.6314\n",
            "Epoch 503/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4841 - accuracy: 0.6374 - val_loss: 1.4866 - val_accuracy: 0.6314\n",
            "Epoch 504/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4830 - accuracy: 0.6376 - val_loss: 1.4855 - val_accuracy: 0.6315\n",
            "Epoch 505/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.4819 - accuracy: 0.6379 - val_loss: 1.4844 - val_accuracy: 0.6314\n",
            "Epoch 506/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.4808 - accuracy: 0.6381 - val_loss: 1.4834 - val_accuracy: 0.6317\n",
            "Epoch 507/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.4797 - accuracy: 0.6382 - val_loss: 1.4823 - val_accuracy: 0.6321\n",
            "Epoch 508/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.4787 - accuracy: 0.6383 - val_loss: 1.4812 - val_accuracy: 0.6324\n",
            "Epoch 509/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4776 - accuracy: 0.6384 - val_loss: 1.4802 - val_accuracy: 0.6326\n",
            "Epoch 510/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.4765 - accuracy: 0.6385 - val_loss: 1.4791 - val_accuracy: 0.6329\n",
            "Epoch 511/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.4754 - accuracy: 0.6386 - val_loss: 1.4780 - val_accuracy: 0.6330\n",
            "Epoch 512/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4744 - accuracy: 0.6387 - val_loss: 1.4770 - val_accuracy: 0.6331\n",
            "Epoch 513/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4733 - accuracy: 0.6388 - val_loss: 1.4759 - val_accuracy: 0.6331\n",
            "Epoch 514/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.4722 - accuracy: 0.6391 - val_loss: 1.4749 - val_accuracy: 0.6334\n",
            "Epoch 515/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.4712 - accuracy: 0.6392 - val_loss: 1.4738 - val_accuracy: 0.6335\n",
            "Epoch 516/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4701 - accuracy: 0.6393 - val_loss: 1.4727 - val_accuracy: 0.6336\n",
            "Epoch 517/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.4691 - accuracy: 0.6394 - val_loss: 1.4717 - val_accuracy: 0.6335\n",
            "Epoch 518/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.4680 - accuracy: 0.6395 - val_loss: 1.4706 - val_accuracy: 0.6338\n",
            "Epoch 519/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.4669 - accuracy: 0.6396 - val_loss: 1.4696 - val_accuracy: 0.6342\n",
            "Epoch 520/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.4659 - accuracy: 0.6398 - val_loss: 1.4685 - val_accuracy: 0.6343\n",
            "Epoch 521/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4648 - accuracy: 0.6399 - val_loss: 1.4675 - val_accuracy: 0.6345\n",
            "Epoch 522/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4638 - accuracy: 0.6400 - val_loss: 1.4665 - val_accuracy: 0.6348\n",
            "Epoch 523/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.4627 - accuracy: 0.6400 - val_loss: 1.4654 - val_accuracy: 0.6350\n",
            "Epoch 524/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.4617 - accuracy: 0.6403 - val_loss: 1.4644 - val_accuracy: 0.6353\n",
            "Epoch 525/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4606 - accuracy: 0.6405 - val_loss: 1.4633 - val_accuracy: 0.6352\n",
            "Epoch 526/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.4596 - accuracy: 0.6407 - val_loss: 1.4623 - val_accuracy: 0.6350\n",
            "Epoch 527/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.4586 - accuracy: 0.6407 - val_loss: 1.4613 - val_accuracy: 0.6349\n",
            "Epoch 528/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.4575 - accuracy: 0.6409 - val_loss: 1.4602 - val_accuracy: 0.6350\n",
            "Epoch 529/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.4565 - accuracy: 0.6410 - val_loss: 1.4592 - val_accuracy: 0.6348\n",
            "Epoch 530/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4554 - accuracy: 0.6410 - val_loss: 1.4582 - val_accuracy: 0.6349\n",
            "Epoch 531/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4544 - accuracy: 0.6410 - val_loss: 1.4572 - val_accuracy: 0.6350\n",
            "Epoch 532/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.4534 - accuracy: 0.6411 - val_loss: 1.4561 - val_accuracy: 0.6351\n",
            "Epoch 533/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.4524 - accuracy: 0.6413 - val_loss: 1.4551 - val_accuracy: 0.6354\n",
            "Epoch 534/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4513 - accuracy: 0.6413 - val_loss: 1.4541 - val_accuracy: 0.6355\n",
            "Epoch 535/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.4503 - accuracy: 0.6415 - val_loss: 1.4531 - val_accuracy: 0.6356\n",
            "Epoch 536/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.4493 - accuracy: 0.6415 - val_loss: 1.4521 - val_accuracy: 0.6357\n",
            "Epoch 537/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4483 - accuracy: 0.6416 - val_loss: 1.4510 - val_accuracy: 0.6357\n",
            "Epoch 538/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.4472 - accuracy: 0.6418 - val_loss: 1.4500 - val_accuracy: 0.6359\n",
            "Epoch 539/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.4462 - accuracy: 0.6420 - val_loss: 1.4490 - val_accuracy: 0.6360\n",
            "Epoch 540/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4452 - accuracy: 0.6421 - val_loss: 1.4480 - val_accuracy: 0.6360\n",
            "Epoch 541/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.4442 - accuracy: 0.6422 - val_loss: 1.4470 - val_accuracy: 0.6359\n",
            "Epoch 542/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.4432 - accuracy: 0.6424 - val_loss: 1.4460 - val_accuracy: 0.6363\n",
            "Epoch 543/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4421 - accuracy: 0.6424 - val_loss: 1.4450 - val_accuracy: 0.6360\n",
            "Epoch 544/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.4411 - accuracy: 0.6426 - val_loss: 1.4440 - val_accuracy: 0.6361\n",
            "Epoch 545/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4401 - accuracy: 0.6427 - val_loss: 1.4430 - val_accuracy: 0.6362\n",
            "Epoch 546/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4391 - accuracy: 0.6429 - val_loss: 1.4420 - val_accuracy: 0.6366\n",
            "Epoch 547/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.4381 - accuracy: 0.6429 - val_loss: 1.4410 - val_accuracy: 0.6367\n",
            "Epoch 548/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.4371 - accuracy: 0.6429 - val_loss: 1.4400 - val_accuracy: 0.6369\n",
            "Epoch 549/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4361 - accuracy: 0.6429 - val_loss: 1.4390 - val_accuracy: 0.6368\n",
            "Epoch 550/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4351 - accuracy: 0.6430 - val_loss: 1.4380 - val_accuracy: 0.6370\n",
            "Epoch 551/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.4341 - accuracy: 0.6431 - val_loss: 1.4370 - val_accuracy: 0.6369\n",
            "Epoch 552/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.4331 - accuracy: 0.6433 - val_loss: 1.4360 - val_accuracy: 0.6368\n",
            "Epoch 553/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4321 - accuracy: 0.6434 - val_loss: 1.4350 - val_accuracy: 0.6371\n",
            "Epoch 554/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.4311 - accuracy: 0.6435 - val_loss: 1.4341 - val_accuracy: 0.6369\n",
            "Epoch 555/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.4301 - accuracy: 0.6435 - val_loss: 1.4331 - val_accuracy: 0.6371\n",
            "Epoch 556/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.4291 - accuracy: 0.6437 - val_loss: 1.4321 - val_accuracy: 0.6374\n",
            "Epoch 557/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.4282 - accuracy: 0.6437 - val_loss: 1.4311 - val_accuracy: 0.6376\n",
            "Epoch 558/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.4272 - accuracy: 0.6438 - val_loss: 1.4301 - val_accuracy: 0.6376\n",
            "Epoch 559/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.4262 - accuracy: 0.6439 - val_loss: 1.4291 - val_accuracy: 0.6377\n",
            "Epoch 560/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4252 - accuracy: 0.6440 - val_loss: 1.4282 - val_accuracy: 0.6379\n",
            "Epoch 561/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4242 - accuracy: 0.6441 - val_loss: 1.4272 - val_accuracy: 0.6380\n",
            "Epoch 562/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.4232 - accuracy: 0.6442 - val_loss: 1.4262 - val_accuracy: 0.6381\n",
            "Epoch 563/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4223 - accuracy: 0.6443 - val_loss: 1.4253 - val_accuracy: 0.6380\n",
            "Epoch 564/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4213 - accuracy: 0.6445 - val_loss: 1.4243 - val_accuracy: 0.6383\n",
            "Epoch 565/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.4203 - accuracy: 0.6446 - val_loss: 1.4233 - val_accuracy: 0.6385\n",
            "Epoch 566/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.4193 - accuracy: 0.6446 - val_loss: 1.4224 - val_accuracy: 0.6387\n",
            "Epoch 567/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.4184 - accuracy: 0.6447 - val_loss: 1.4214 - val_accuracy: 0.6388\n",
            "Epoch 568/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.4174 - accuracy: 0.6448 - val_loss: 1.4204 - val_accuracy: 0.6390\n",
            "Epoch 569/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.4164 - accuracy: 0.6449 - val_loss: 1.4195 - val_accuracy: 0.6390\n",
            "Epoch 570/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.4155 - accuracy: 0.6450 - val_loss: 1.4185 - val_accuracy: 0.6391\n",
            "Epoch 571/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4145 - accuracy: 0.6450 - val_loss: 1.4176 - val_accuracy: 0.6390\n",
            "Epoch 572/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4135 - accuracy: 0.6451 - val_loss: 1.4166 - val_accuracy: 0.6393\n",
            "Epoch 573/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.4126 - accuracy: 0.6453 - val_loss: 1.4156 - val_accuracy: 0.6395\n",
            "Epoch 574/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.4116 - accuracy: 0.6454 - val_loss: 1.4147 - val_accuracy: 0.6397\n",
            "Epoch 575/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4107 - accuracy: 0.6455 - val_loss: 1.4137 - val_accuracy: 0.6396\n",
            "Epoch 576/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.4097 - accuracy: 0.6456 - val_loss: 1.4128 - val_accuracy: 0.6396\n",
            "Epoch 577/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.4088 - accuracy: 0.6457 - val_loss: 1.4118 - val_accuracy: 0.6397\n",
            "Epoch 578/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.4078 - accuracy: 0.6457 - val_loss: 1.4109 - val_accuracy: 0.6399\n",
            "Epoch 579/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.4068 - accuracy: 0.6459 - val_loss: 1.4100 - val_accuracy: 0.6401\n",
            "Epoch 580/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.4059 - accuracy: 0.6459 - val_loss: 1.4090 - val_accuracy: 0.6402\n",
            "Epoch 581/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.4050 - accuracy: 0.6460 - val_loss: 1.4081 - val_accuracy: 0.6403\n",
            "Epoch 582/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.4040 - accuracy: 0.6462 - val_loss: 1.4071 - val_accuracy: 0.6403\n",
            "Epoch 583/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4031 - accuracy: 0.6463 - val_loss: 1.4062 - val_accuracy: 0.6408\n",
            "Epoch 584/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.4021 - accuracy: 0.6463 - val_loss: 1.4053 - val_accuracy: 0.6409\n",
            "Epoch 585/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.4012 - accuracy: 0.6464 - val_loss: 1.4043 - val_accuracy: 0.6409\n",
            "Epoch 586/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.4002 - accuracy: 0.6466 - val_loss: 1.4034 - val_accuracy: 0.6410\n",
            "Epoch 587/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3993 - accuracy: 0.6467 - val_loss: 1.4025 - val_accuracy: 0.6407\n",
            "Epoch 588/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3984 - accuracy: 0.6468 - val_loss: 1.4015 - val_accuracy: 0.6406\n",
            "Epoch 589/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3974 - accuracy: 0.6469 - val_loss: 1.4006 - val_accuracy: 0.6407\n",
            "Epoch 590/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.3965 - accuracy: 0.6471 - val_loss: 1.3997 - val_accuracy: 0.6406\n",
            "Epoch 591/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.3956 - accuracy: 0.6472 - val_loss: 1.3988 - val_accuracy: 0.6407\n",
            "Epoch 592/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3946 - accuracy: 0.6473 - val_loss: 1.3978 - val_accuracy: 0.6408\n",
            "Epoch 593/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3937 - accuracy: 0.6474 - val_loss: 1.3969 - val_accuracy: 0.6409\n",
            "Epoch 594/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.3928 - accuracy: 0.6475 - val_loss: 1.3960 - val_accuracy: 0.6410\n",
            "Epoch 595/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.3919 - accuracy: 0.6476 - val_loss: 1.3951 - val_accuracy: 0.6408\n",
            "Epoch 596/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.3909 - accuracy: 0.6477 - val_loss: 1.3942 - val_accuracy: 0.6408\n",
            "Epoch 597/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.3900 - accuracy: 0.6477 - val_loss: 1.3932 - val_accuracy: 0.6408\n",
            "Epoch 598/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3891 - accuracy: 0.6478 - val_loss: 1.3923 - val_accuracy: 0.6408\n",
            "Epoch 599/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3882 - accuracy: 0.6480 - val_loss: 1.3914 - val_accuracy: 0.6409\n",
            "Epoch 600/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.3872 - accuracy: 0.6480 - val_loss: 1.3905 - val_accuracy: 0.6409\n",
            "Epoch 601/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.3863 - accuracy: 0.6482 - val_loss: 1.3896 - val_accuracy: 0.6407\n",
            "Epoch 602/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3854 - accuracy: 0.6482 - val_loss: 1.3887 - val_accuracy: 0.6407\n",
            "Epoch 603/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3845 - accuracy: 0.6484 - val_loss: 1.3878 - val_accuracy: 0.6407\n",
            "Epoch 604/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.3836 - accuracy: 0.6484 - val_loss: 1.3869 - val_accuracy: 0.6410\n",
            "Epoch 605/5000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.3827 - accuracy: 0.6485 - val_loss: 1.3860 - val_accuracy: 0.6412\n",
            "Epoch 606/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.3818 - accuracy: 0.6485 - val_loss: 1.3851 - val_accuracy: 0.6412\n",
            "Epoch 607/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.3809 - accuracy: 0.6486 - val_loss: 1.3842 - val_accuracy: 0.6412\n",
            "Epoch 608/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.3800 - accuracy: 0.6487 - val_loss: 1.3833 - val_accuracy: 0.6414\n",
            "Epoch 609/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.3791 - accuracy: 0.6488 - val_loss: 1.3824 - val_accuracy: 0.6415\n",
            "Epoch 610/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.3782 - accuracy: 0.6488 - val_loss: 1.3815 - val_accuracy: 0.6415\n",
            "Epoch 611/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3773 - accuracy: 0.6489 - val_loss: 1.3806 - val_accuracy: 0.6418\n",
            "Epoch 612/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3764 - accuracy: 0.6490 - val_loss: 1.3797 - val_accuracy: 0.6420\n",
            "Epoch 613/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.3755 - accuracy: 0.6490 - val_loss: 1.3788 - val_accuracy: 0.6420\n",
            "Epoch 614/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.3746 - accuracy: 0.6490 - val_loss: 1.3779 - val_accuracy: 0.6422\n",
            "Epoch 615/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.3737 - accuracy: 0.6491 - val_loss: 1.3770 - val_accuracy: 0.6422\n",
            "Epoch 616/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3728 - accuracy: 0.6492 - val_loss: 1.3762 - val_accuracy: 0.6422\n",
            "Epoch 617/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3719 - accuracy: 0.6494 - val_loss: 1.3753 - val_accuracy: 0.6425\n",
            "Epoch 618/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3710 - accuracy: 0.6496 - val_loss: 1.3744 - val_accuracy: 0.6427\n",
            "Epoch 619/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3701 - accuracy: 0.6496 - val_loss: 1.3735 - val_accuracy: 0.6429\n",
            "Epoch 620/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.3692 - accuracy: 0.6497 - val_loss: 1.3726 - val_accuracy: 0.6431\n",
            "Epoch 621/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.3683 - accuracy: 0.6497 - val_loss: 1.3718 - val_accuracy: 0.6433\n",
            "Epoch 622/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3675 - accuracy: 0.6499 - val_loss: 1.3709 - val_accuracy: 0.6432\n",
            "Epoch 623/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.3666 - accuracy: 0.6500 - val_loss: 1.3700 - val_accuracy: 0.6431\n",
            "Epoch 624/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3657 - accuracy: 0.6500 - val_loss: 1.3691 - val_accuracy: 0.6433\n",
            "Epoch 625/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3648 - accuracy: 0.6501 - val_loss: 1.3683 - val_accuracy: 0.6434\n",
            "Epoch 626/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3639 - accuracy: 0.6503 - val_loss: 1.3674 - val_accuracy: 0.6433\n",
            "Epoch 627/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3631 - accuracy: 0.6503 - val_loss: 1.3665 - val_accuracy: 0.6436\n",
            "Epoch 628/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3622 - accuracy: 0.6505 - val_loss: 1.3656 - val_accuracy: 0.6437\n",
            "Epoch 629/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3613 - accuracy: 0.6506 - val_loss: 1.3648 - val_accuracy: 0.6437\n",
            "Epoch 630/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.3604 - accuracy: 0.6506 - val_loss: 1.3639 - val_accuracy: 0.6438\n",
            "Epoch 631/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.3596 - accuracy: 0.6507 - val_loss: 1.3631 - val_accuracy: 0.6438\n",
            "Epoch 632/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.3587 - accuracy: 0.6509 - val_loss: 1.3622 - val_accuracy: 0.6439\n",
            "Epoch 633/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3578 - accuracy: 0.6510 - val_loss: 1.3613 - val_accuracy: 0.6438\n",
            "Epoch 634/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3570 - accuracy: 0.6511 - val_loss: 1.3605 - val_accuracy: 0.6439\n",
            "Epoch 635/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.3561 - accuracy: 0.6512 - val_loss: 1.3596 - val_accuracy: 0.6441\n",
            "Epoch 636/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3552 - accuracy: 0.6513 - val_loss: 1.3588 - val_accuracy: 0.6444\n",
            "Epoch 637/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.3544 - accuracy: 0.6514 - val_loss: 1.3579 - val_accuracy: 0.6448\n",
            "Epoch 638/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3535 - accuracy: 0.6516 - val_loss: 1.3571 - val_accuracy: 0.6449\n",
            "Epoch 639/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.3527 - accuracy: 0.6518 - val_loss: 1.3562 - val_accuracy: 0.6451\n",
            "Epoch 640/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.3518 - accuracy: 0.6518 - val_loss: 1.3554 - val_accuracy: 0.6452\n",
            "Epoch 641/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3509 - accuracy: 0.6519 - val_loss: 1.3545 - val_accuracy: 0.6454\n",
            "Epoch 642/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3501 - accuracy: 0.6520 - val_loss: 1.3537 - val_accuracy: 0.6454\n",
            "Epoch 643/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.3492 - accuracy: 0.6521 - val_loss: 1.3528 - val_accuracy: 0.6454\n",
            "Epoch 644/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.3484 - accuracy: 0.6522 - val_loss: 1.3520 - val_accuracy: 0.6457\n",
            "Epoch 645/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.3475 - accuracy: 0.6523 - val_loss: 1.3511 - val_accuracy: 0.6456\n",
            "Epoch 646/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.3467 - accuracy: 0.6525 - val_loss: 1.3503 - val_accuracy: 0.6459\n",
            "Epoch 647/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3458 - accuracy: 0.6526 - val_loss: 1.3494 - val_accuracy: 0.6458\n",
            "Epoch 648/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.3450 - accuracy: 0.6527 - val_loss: 1.3486 - val_accuracy: 0.6459\n",
            "Epoch 649/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.3442 - accuracy: 0.6528 - val_loss: 1.3478 - val_accuracy: 0.6460\n",
            "Epoch 650/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3433 - accuracy: 0.6529 - val_loss: 1.3469 - val_accuracy: 0.6459\n",
            "Epoch 651/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.3425 - accuracy: 0.6530 - val_loss: 1.3461 - val_accuracy: 0.6462\n",
            "Epoch 652/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3416 - accuracy: 0.6532 - val_loss: 1.3453 - val_accuracy: 0.6463\n",
            "Epoch 653/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3408 - accuracy: 0.6533 - val_loss: 1.3444 - val_accuracy: 0.6463\n",
            "Epoch 654/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3399 - accuracy: 0.6533 - val_loss: 1.3436 - val_accuracy: 0.6463\n",
            "Epoch 655/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.3391 - accuracy: 0.6533 - val_loss: 1.3428 - val_accuracy: 0.6463\n",
            "Epoch 656/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3383 - accuracy: 0.6535 - val_loss: 1.3419 - val_accuracy: 0.6468\n",
            "Epoch 657/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3374 - accuracy: 0.6536 - val_loss: 1.3411 - val_accuracy: 0.6471\n",
            "Epoch 658/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.3366 - accuracy: 0.6537 - val_loss: 1.3403 - val_accuracy: 0.6472\n",
            "Epoch 659/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.3358 - accuracy: 0.6539 - val_loss: 1.3395 - val_accuracy: 0.6473\n",
            "Epoch 660/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.3350 - accuracy: 0.6541 - val_loss: 1.3386 - val_accuracy: 0.6473\n",
            "Epoch 661/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.3341 - accuracy: 0.6542 - val_loss: 1.3378 - val_accuracy: 0.6472\n",
            "Epoch 662/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3333 - accuracy: 0.6543 - val_loss: 1.3370 - val_accuracy: 0.6472\n",
            "Epoch 663/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3325 - accuracy: 0.6543 - val_loss: 1.3362 - val_accuracy: 0.6472\n",
            "Epoch 664/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.3316 - accuracy: 0.6544 - val_loss: 1.3354 - val_accuracy: 0.6473\n",
            "Epoch 665/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3308 - accuracy: 0.6545 - val_loss: 1.3346 - val_accuracy: 0.6474\n",
            "Epoch 666/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3300 - accuracy: 0.6546 - val_loss: 1.3337 - val_accuracy: 0.6474\n",
            "Epoch 667/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3292 - accuracy: 0.6546 - val_loss: 1.3329 - val_accuracy: 0.6476\n",
            "Epoch 668/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3284 - accuracy: 0.6548 - val_loss: 1.3321 - val_accuracy: 0.6475\n",
            "Epoch 669/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.3276 - accuracy: 0.6549 - val_loss: 1.3313 - val_accuracy: 0.6478\n",
            "Epoch 670/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3267 - accuracy: 0.6549 - val_loss: 1.3305 - val_accuracy: 0.6478\n",
            "Epoch 671/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3259 - accuracy: 0.6550 - val_loss: 1.3297 - val_accuracy: 0.6478\n",
            "Epoch 672/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3251 - accuracy: 0.6550 - val_loss: 1.3289 - val_accuracy: 0.6480\n",
            "Epoch 673/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3243 - accuracy: 0.6551 - val_loss: 1.3281 - val_accuracy: 0.6481\n",
            "Epoch 674/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3235 - accuracy: 0.6552 - val_loss: 1.3273 - val_accuracy: 0.6481\n",
            "Epoch 675/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.3227 - accuracy: 0.6554 - val_loss: 1.3265 - val_accuracy: 0.6483\n",
            "Epoch 676/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3219 - accuracy: 0.6555 - val_loss: 1.3257 - val_accuracy: 0.6481\n",
            "Epoch 677/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.3211 - accuracy: 0.6556 - val_loss: 1.3249 - val_accuracy: 0.6484\n",
            "Epoch 678/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.3203 - accuracy: 0.6557 - val_loss: 1.3241 - val_accuracy: 0.6488\n",
            "Epoch 679/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.3195 - accuracy: 0.6558 - val_loss: 1.3233 - val_accuracy: 0.6488\n",
            "Epoch 680/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.3186 - accuracy: 0.6558 - val_loss: 1.3225 - val_accuracy: 0.6490\n",
            "Epoch 681/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3178 - accuracy: 0.6560 - val_loss: 1.3217 - val_accuracy: 0.6491\n",
            "Epoch 682/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3170 - accuracy: 0.6561 - val_loss: 1.3209 - val_accuracy: 0.6491\n",
            "Epoch 683/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.3162 - accuracy: 0.6563 - val_loss: 1.3201 - val_accuracy: 0.6492\n",
            "Epoch 684/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.3155 - accuracy: 0.6564 - val_loss: 1.3193 - val_accuracy: 0.6492\n",
            "Epoch 685/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3147 - accuracy: 0.6565 - val_loss: 1.3185 - val_accuracy: 0.6493\n",
            "Epoch 686/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3139 - accuracy: 0.6567 - val_loss: 1.3177 - val_accuracy: 0.6494\n",
            "Epoch 687/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3131 - accuracy: 0.6568 - val_loss: 1.3169 - val_accuracy: 0.6495\n",
            "Epoch 688/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.3123 - accuracy: 0.6569 - val_loss: 1.3162 - val_accuracy: 0.6496\n",
            "Epoch 689/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.3115 - accuracy: 0.6570 - val_loss: 1.3154 - val_accuracy: 0.6499\n",
            "Epoch 690/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.3107 - accuracy: 0.6571 - val_loss: 1.3146 - val_accuracy: 0.6501\n",
            "Epoch 691/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.3099 - accuracy: 0.6572 - val_loss: 1.3138 - val_accuracy: 0.6504\n",
            "Epoch 692/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3091 - accuracy: 0.6572 - val_loss: 1.3130 - val_accuracy: 0.6505\n",
            "Epoch 693/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3083 - accuracy: 0.6574 - val_loss: 1.3123 - val_accuracy: 0.6506\n",
            "Epoch 694/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.3075 - accuracy: 0.6575 - val_loss: 1.3115 - val_accuracy: 0.6509\n",
            "Epoch 695/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.3068 - accuracy: 0.6575 - val_loss: 1.3107 - val_accuracy: 0.6509\n",
            "Epoch 696/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.3060 - accuracy: 0.6575 - val_loss: 1.3099 - val_accuracy: 0.6511\n",
            "Epoch 697/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.3052 - accuracy: 0.6577 - val_loss: 1.3091 - val_accuracy: 0.6513\n",
            "Epoch 698/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.3044 - accuracy: 0.6578 - val_loss: 1.3084 - val_accuracy: 0.6515\n",
            "Epoch 699/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.3036 - accuracy: 0.6580 - val_loss: 1.3076 - val_accuracy: 0.6519\n",
            "Epoch 700/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.3029 - accuracy: 0.6581 - val_loss: 1.3068 - val_accuracy: 0.6520\n",
            "Epoch 701/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.3021 - accuracy: 0.6581 - val_loss: 1.3061 - val_accuracy: 0.6521\n",
            "Epoch 702/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.3013 - accuracy: 0.6582 - val_loss: 1.3053 - val_accuracy: 0.6522\n",
            "Epoch 703/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.3005 - accuracy: 0.6582 - val_loss: 1.3045 - val_accuracy: 0.6522\n",
            "Epoch 704/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2998 - accuracy: 0.6583 - val_loss: 1.3038 - val_accuracy: 0.6522\n",
            "Epoch 705/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2990 - accuracy: 0.6585 - val_loss: 1.3030 - val_accuracy: 0.6524\n",
            "Epoch 706/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2982 - accuracy: 0.6586 - val_loss: 1.3022 - val_accuracy: 0.6523\n",
            "Epoch 707/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2975 - accuracy: 0.6588 - val_loss: 1.3015 - val_accuracy: 0.6522\n",
            "Epoch 708/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.2967 - accuracy: 0.6589 - val_loss: 1.3007 - val_accuracy: 0.6523\n",
            "Epoch 709/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2959 - accuracy: 0.6589 - val_loss: 1.3000 - val_accuracy: 0.6524\n",
            "Epoch 710/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.2952 - accuracy: 0.6591 - val_loss: 1.2992 - val_accuracy: 0.6525\n",
            "Epoch 711/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.2944 - accuracy: 0.6593 - val_loss: 1.2984 - val_accuracy: 0.6526\n",
            "Epoch 712/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.2936 - accuracy: 0.6593 - val_loss: 1.2977 - val_accuracy: 0.6526\n",
            "Epoch 713/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2929 - accuracy: 0.6595 - val_loss: 1.2969 - val_accuracy: 0.6526\n",
            "Epoch 714/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2921 - accuracy: 0.6596 - val_loss: 1.2962 - val_accuracy: 0.6526\n",
            "Epoch 715/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2914 - accuracy: 0.6597 - val_loss: 1.2954 - val_accuracy: 0.6527\n",
            "Epoch 716/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2906 - accuracy: 0.6599 - val_loss: 1.2947 - val_accuracy: 0.6528\n",
            "Epoch 717/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2898 - accuracy: 0.6600 - val_loss: 1.2939 - val_accuracy: 0.6525\n",
            "Epoch 718/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2891 - accuracy: 0.6602 - val_loss: 1.2932 - val_accuracy: 0.6526\n",
            "Epoch 719/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2883 - accuracy: 0.6603 - val_loss: 1.2924 - val_accuracy: 0.6526\n",
            "Epoch 720/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.2876 - accuracy: 0.6604 - val_loss: 1.2917 - val_accuracy: 0.6528\n",
            "Epoch 721/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.2868 - accuracy: 0.6605 - val_loss: 1.2909 - val_accuracy: 0.6528\n",
            "Epoch 722/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.2861 - accuracy: 0.6605 - val_loss: 1.2902 - val_accuracy: 0.6528\n",
            "Epoch 723/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2853 - accuracy: 0.6605 - val_loss: 1.2895 - val_accuracy: 0.6530\n",
            "Epoch 724/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2846 - accuracy: 0.6606 - val_loss: 1.2887 - val_accuracy: 0.6531\n",
            "Epoch 725/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2838 - accuracy: 0.6607 - val_loss: 1.2880 - val_accuracy: 0.6533\n",
            "Epoch 726/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.2831 - accuracy: 0.6607 - val_loss: 1.2872 - val_accuracy: 0.6533\n",
            "Epoch 727/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2823 - accuracy: 0.6609 - val_loss: 1.2865 - val_accuracy: 0.6532\n",
            "Epoch 728/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.2816 - accuracy: 0.6610 - val_loss: 1.2858 - val_accuracy: 0.6533\n",
            "Epoch 729/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.2809 - accuracy: 0.6611 - val_loss: 1.2850 - val_accuracy: 0.6535\n",
            "Epoch 730/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.2801 - accuracy: 0.6612 - val_loss: 1.2843 - val_accuracy: 0.6538\n",
            "Epoch 731/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.2794 - accuracy: 0.6614 - val_loss: 1.2836 - val_accuracy: 0.6540\n",
            "Epoch 732/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.2786 - accuracy: 0.6615 - val_loss: 1.2828 - val_accuracy: 0.6541\n",
            "Epoch 733/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2779 - accuracy: 0.6616 - val_loss: 1.2821 - val_accuracy: 0.6541\n",
            "Epoch 734/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2772 - accuracy: 0.6617 - val_loss: 1.2814 - val_accuracy: 0.6542\n",
            "Epoch 735/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.2764 - accuracy: 0.6618 - val_loss: 1.2806 - val_accuracy: 0.6542\n",
            "Epoch 736/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.2757 - accuracy: 0.6619 - val_loss: 1.2799 - val_accuracy: 0.6546\n",
            "Epoch 737/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2750 - accuracy: 0.6619 - val_loss: 1.2792 - val_accuracy: 0.6545\n",
            "Epoch 738/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.2742 - accuracy: 0.6621 - val_loss: 1.2785 - val_accuracy: 0.6549\n",
            "Epoch 739/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.2735 - accuracy: 0.6622 - val_loss: 1.2777 - val_accuracy: 0.6549\n",
            "Epoch 740/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.2728 - accuracy: 0.6625 - val_loss: 1.2770 - val_accuracy: 0.6550\n",
            "Epoch 741/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.2720 - accuracy: 0.6625 - val_loss: 1.2763 - val_accuracy: 0.6549\n",
            "Epoch 742/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2713 - accuracy: 0.6625 - val_loss: 1.2756 - val_accuracy: 0.6549\n",
            "Epoch 743/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2706 - accuracy: 0.6626 - val_loss: 1.2749 - val_accuracy: 0.6549\n",
            "Epoch 744/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.2699 - accuracy: 0.6627 - val_loss: 1.2741 - val_accuracy: 0.6548\n",
            "Epoch 745/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2691 - accuracy: 0.6628 - val_loss: 1.2734 - val_accuracy: 0.6547\n",
            "Epoch 746/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2684 - accuracy: 0.6629 - val_loss: 1.2727 - val_accuracy: 0.6548\n",
            "Epoch 747/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.2677 - accuracy: 0.6630 - val_loss: 1.2720 - val_accuracy: 0.6549\n",
            "Epoch 748/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2670 - accuracy: 0.6630 - val_loss: 1.2713 - val_accuracy: 0.6551\n",
            "Epoch 749/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2663 - accuracy: 0.6631 - val_loss: 1.2706 - val_accuracy: 0.6553\n",
            "Epoch 750/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2655 - accuracy: 0.6632 - val_loss: 1.2699 - val_accuracy: 0.6553\n",
            "Epoch 751/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2648 - accuracy: 0.6633 - val_loss: 1.2691 - val_accuracy: 0.6555\n",
            "Epoch 752/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.2641 - accuracy: 0.6634 - val_loss: 1.2684 - val_accuracy: 0.6555\n",
            "Epoch 753/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2634 - accuracy: 0.6634 - val_loss: 1.2677 - val_accuracy: 0.6558\n",
            "Epoch 754/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2627 - accuracy: 0.6634 - val_loss: 1.2670 - val_accuracy: 0.6559\n",
            "Epoch 755/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2620 - accuracy: 0.6636 - val_loss: 1.2663 - val_accuracy: 0.6559\n",
            "Epoch 756/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2613 - accuracy: 0.6637 - val_loss: 1.2656 - val_accuracy: 0.6562\n",
            "Epoch 757/5000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.2605 - accuracy: 0.6638 - val_loss: 1.2649 - val_accuracy: 0.6563\n",
            "Epoch 758/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2598 - accuracy: 0.6639 - val_loss: 1.2642 - val_accuracy: 0.6564\n",
            "Epoch 759/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2591 - accuracy: 0.6640 - val_loss: 1.2635 - val_accuracy: 0.6566\n",
            "Epoch 760/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2584 - accuracy: 0.6642 - val_loss: 1.2628 - val_accuracy: 0.6565\n",
            "Epoch 761/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2577 - accuracy: 0.6643 - val_loss: 1.2621 - val_accuracy: 0.6565\n",
            "Epoch 762/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2570 - accuracy: 0.6645 - val_loss: 1.2614 - val_accuracy: 0.6565\n",
            "Epoch 763/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2563 - accuracy: 0.6647 - val_loss: 1.2607 - val_accuracy: 0.6568\n",
            "Epoch 764/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2556 - accuracy: 0.6648 - val_loss: 1.2600 - val_accuracy: 0.6567\n",
            "Epoch 765/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2549 - accuracy: 0.6649 - val_loss: 1.2593 - val_accuracy: 0.6567\n",
            "Epoch 766/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2542 - accuracy: 0.6650 - val_loss: 1.2586 - val_accuracy: 0.6569\n",
            "Epoch 767/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2535 - accuracy: 0.6651 - val_loss: 1.2579 - val_accuracy: 0.6568\n",
            "Epoch 768/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.2528 - accuracy: 0.6652 - val_loss: 1.2572 - val_accuracy: 0.6568\n",
            "Epoch 769/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2521 - accuracy: 0.6654 - val_loss: 1.2566 - val_accuracy: 0.6572\n",
            "Epoch 770/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.2514 - accuracy: 0.6655 - val_loss: 1.2559 - val_accuracy: 0.6573\n",
            "Epoch 771/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.2507 - accuracy: 0.6655 - val_loss: 1.2552 - val_accuracy: 0.6571\n",
            "Epoch 772/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.2500 - accuracy: 0.6655 - val_loss: 1.2545 - val_accuracy: 0.6572\n",
            "Epoch 773/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2493 - accuracy: 0.6656 - val_loss: 1.2538 - val_accuracy: 0.6573\n",
            "Epoch 774/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2486 - accuracy: 0.6657 - val_loss: 1.2531 - val_accuracy: 0.6572\n",
            "Epoch 775/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.2479 - accuracy: 0.6657 - val_loss: 1.2524 - val_accuracy: 0.6576\n",
            "Epoch 776/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2473 - accuracy: 0.6659 - val_loss: 1.2517 - val_accuracy: 0.6577\n",
            "Epoch 777/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.2466 - accuracy: 0.6659 - val_loss: 1.2511 - val_accuracy: 0.6577\n",
            "Epoch 778/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.2459 - accuracy: 0.6661 - val_loss: 1.2504 - val_accuracy: 0.6578\n",
            "Epoch 779/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2452 - accuracy: 0.6661 - val_loss: 1.2497 - val_accuracy: 0.6578\n",
            "Epoch 780/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.2445 - accuracy: 0.6662 - val_loss: 1.2490 - val_accuracy: 0.6579\n",
            "Epoch 781/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.2438 - accuracy: 0.6663 - val_loss: 1.2483 - val_accuracy: 0.6579\n",
            "Epoch 782/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.2431 - accuracy: 0.6664 - val_loss: 1.2477 - val_accuracy: 0.6582\n",
            "Epoch 783/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.2425 - accuracy: 0.6665 - val_loss: 1.2470 - val_accuracy: 0.6581\n",
            "Epoch 784/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2418 - accuracy: 0.6666 - val_loss: 1.2463 - val_accuracy: 0.6582\n",
            "Epoch 785/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2411 - accuracy: 0.6666 - val_loss: 1.2456 - val_accuracy: 0.6584\n",
            "Epoch 786/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2404 - accuracy: 0.6666 - val_loss: 1.2450 - val_accuracy: 0.6588\n",
            "Epoch 787/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2397 - accuracy: 0.6667 - val_loss: 1.2443 - val_accuracy: 0.6588\n",
            "Epoch 788/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.2391 - accuracy: 0.6668 - val_loss: 1.2436 - val_accuracy: 0.6587\n",
            "Epoch 789/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2384 - accuracy: 0.6669 - val_loss: 1.2430 - val_accuracy: 0.6588\n",
            "Epoch 790/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2377 - accuracy: 0.6670 - val_loss: 1.2423 - val_accuracy: 0.6588\n",
            "Epoch 791/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.2370 - accuracy: 0.6671 - val_loss: 1.2416 - val_accuracy: 0.6591\n",
            "Epoch 792/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2364 - accuracy: 0.6672 - val_loss: 1.2410 - val_accuracy: 0.6590\n",
            "Epoch 793/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.2357 - accuracy: 0.6673 - val_loss: 1.2403 - val_accuracy: 0.6591\n",
            "Epoch 794/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.2350 - accuracy: 0.6674 - val_loss: 1.2396 - val_accuracy: 0.6593\n",
            "Epoch 795/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2343 - accuracy: 0.6676 - val_loss: 1.2390 - val_accuracy: 0.6594\n",
            "Epoch 796/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2337 - accuracy: 0.6677 - val_loss: 1.2383 - val_accuracy: 0.6596\n",
            "Epoch 797/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.2330 - accuracy: 0.6678 - val_loss: 1.2376 - val_accuracy: 0.6597\n",
            "Epoch 798/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.2323 - accuracy: 0.6679 - val_loss: 1.2370 - val_accuracy: 0.6598\n",
            "Epoch 799/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2317 - accuracy: 0.6680 - val_loss: 1.2363 - val_accuracy: 0.6601\n",
            "Epoch 800/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2310 - accuracy: 0.6682 - val_loss: 1.2357 - val_accuracy: 0.6602\n",
            "Epoch 801/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2303 - accuracy: 0.6683 - val_loss: 1.2350 - val_accuracy: 0.6602\n",
            "Epoch 802/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2297 - accuracy: 0.6684 - val_loss: 1.2343 - val_accuracy: 0.6603\n",
            "Epoch 803/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.2290 - accuracy: 0.6685 - val_loss: 1.2337 - val_accuracy: 0.6605\n",
            "Epoch 804/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.2283 - accuracy: 0.6686 - val_loss: 1.2330 - val_accuracy: 0.6605\n",
            "Epoch 805/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.2277 - accuracy: 0.6687 - val_loss: 1.2324 - val_accuracy: 0.6606\n",
            "Epoch 806/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2270 - accuracy: 0.6687 - val_loss: 1.2317 - val_accuracy: 0.6610\n",
            "Epoch 807/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2264 - accuracy: 0.6688 - val_loss: 1.2311 - val_accuracy: 0.6611\n",
            "Epoch 808/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.2257 - accuracy: 0.6689 - val_loss: 1.2304 - val_accuracy: 0.6611\n",
            "Epoch 809/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2250 - accuracy: 0.6691 - val_loss: 1.2298 - val_accuracy: 0.6612\n",
            "Epoch 810/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2244 - accuracy: 0.6693 - val_loss: 1.2291 - val_accuracy: 0.6613\n",
            "Epoch 811/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.2237 - accuracy: 0.6693 - val_loss: 1.2285 - val_accuracy: 0.6614\n",
            "Epoch 812/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2231 - accuracy: 0.6695 - val_loss: 1.2278 - val_accuracy: 0.6613\n",
            "Epoch 813/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2224 - accuracy: 0.6695 - val_loss: 1.2272 - val_accuracy: 0.6613\n",
            "Epoch 814/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.2218 - accuracy: 0.6696 - val_loss: 1.2265 - val_accuracy: 0.6615\n",
            "Epoch 815/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2211 - accuracy: 0.6698 - val_loss: 1.2259 - val_accuracy: 0.6617\n",
            "Epoch 816/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2205 - accuracy: 0.6700 - val_loss: 1.2252 - val_accuracy: 0.6619\n",
            "Epoch 817/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.2198 - accuracy: 0.6701 - val_loss: 1.2246 - val_accuracy: 0.6620\n",
            "Epoch 818/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2192 - accuracy: 0.6703 - val_loss: 1.2240 - val_accuracy: 0.6621\n",
            "Epoch 819/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2185 - accuracy: 0.6704 - val_loss: 1.2233 - val_accuracy: 0.6622\n",
            "Epoch 820/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2179 - accuracy: 0.6704 - val_loss: 1.2227 - val_accuracy: 0.6623\n",
            "Epoch 821/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2172 - accuracy: 0.6705 - val_loss: 1.2220 - val_accuracy: 0.6623\n",
            "Epoch 822/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.2166 - accuracy: 0.6705 - val_loss: 1.2214 - val_accuracy: 0.6622\n",
            "Epoch 823/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.2160 - accuracy: 0.6705 - val_loss: 1.2208 - val_accuracy: 0.6625\n",
            "Epoch 824/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2153 - accuracy: 0.6707 - val_loss: 1.2201 - val_accuracy: 0.6626\n",
            "Epoch 825/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.2147 - accuracy: 0.6707 - val_loss: 1.2195 - val_accuracy: 0.6625\n",
            "Epoch 826/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2140 - accuracy: 0.6708 - val_loss: 1.2189 - val_accuracy: 0.6627\n",
            "Epoch 827/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.2134 - accuracy: 0.6709 - val_loss: 1.2182 - val_accuracy: 0.6629\n",
            "Epoch 828/5000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 1.2127 - accuracy: 0.6710 - val_loss: 1.2176 - val_accuracy: 0.6629\n",
            "Epoch 829/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2121 - accuracy: 0.6712 - val_loss: 1.2170 - val_accuracy: 0.6630\n",
            "Epoch 830/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.2115 - accuracy: 0.6712 - val_loss: 1.2163 - val_accuracy: 0.6630\n",
            "Epoch 831/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.2108 - accuracy: 0.6713 - val_loss: 1.2157 - val_accuracy: 0.6634\n",
            "Epoch 832/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2102 - accuracy: 0.6714 - val_loss: 1.2151 - val_accuracy: 0.6636\n",
            "Epoch 833/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.2096 - accuracy: 0.6715 - val_loss: 1.2144 - val_accuracy: 0.6637\n",
            "Epoch 834/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.2089 - accuracy: 0.6716 - val_loss: 1.2138 - val_accuracy: 0.6638\n",
            "Epoch 835/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.2083 - accuracy: 0.6717 - val_loss: 1.2132 - val_accuracy: 0.6639\n",
            "Epoch 836/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.2077 - accuracy: 0.6717 - val_loss: 1.2126 - val_accuracy: 0.6641\n",
            "Epoch 837/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.2070 - accuracy: 0.6719 - val_loss: 1.2119 - val_accuracy: 0.6643\n",
            "Epoch 838/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2064 - accuracy: 0.6720 - val_loss: 1.2113 - val_accuracy: 0.6643\n",
            "Epoch 839/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.2058 - accuracy: 0.6722 - val_loss: 1.2107 - val_accuracy: 0.6645\n",
            "Epoch 840/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.2052 - accuracy: 0.6722 - val_loss: 1.2101 - val_accuracy: 0.6645\n",
            "Epoch 841/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.2045 - accuracy: 0.6723 - val_loss: 1.2095 - val_accuracy: 0.6644\n",
            "Epoch 842/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.2039 - accuracy: 0.6726 - val_loss: 1.2088 - val_accuracy: 0.6644\n",
            "Epoch 843/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.2033 - accuracy: 0.6726 - val_loss: 1.2082 - val_accuracy: 0.6644\n",
            "Epoch 844/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.2026 - accuracy: 0.6727 - val_loss: 1.2076 - val_accuracy: 0.6644\n",
            "Epoch 845/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.2020 - accuracy: 0.6729 - val_loss: 1.2070 - val_accuracy: 0.6645\n",
            "Epoch 846/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.2014 - accuracy: 0.6730 - val_loss: 1.2064 - val_accuracy: 0.6647\n",
            "Epoch 847/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.2008 - accuracy: 0.6731 - val_loss: 1.2058 - val_accuracy: 0.6649\n",
            "Epoch 848/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.2002 - accuracy: 0.6732 - val_loss: 1.2051 - val_accuracy: 0.6648\n",
            "Epoch 849/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1995 - accuracy: 0.6733 - val_loss: 1.2045 - val_accuracy: 0.6651\n",
            "Epoch 850/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1989 - accuracy: 0.6734 - val_loss: 1.2039 - val_accuracy: 0.6651\n",
            "Epoch 851/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1983 - accuracy: 0.6737 - val_loss: 1.2033 - val_accuracy: 0.6653\n",
            "Epoch 852/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1977 - accuracy: 0.6737 - val_loss: 1.2027 - val_accuracy: 0.6655\n",
            "Epoch 853/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1971 - accuracy: 0.6738 - val_loss: 1.2021 - val_accuracy: 0.6655\n",
            "Epoch 854/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1965 - accuracy: 0.6739 - val_loss: 1.2015 - val_accuracy: 0.6656\n",
            "Epoch 855/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1958 - accuracy: 0.6740 - val_loss: 1.2009 - val_accuracy: 0.6656\n",
            "Epoch 856/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.1952 - accuracy: 0.6742 - val_loss: 1.2003 - val_accuracy: 0.6656\n",
            "Epoch 857/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1946 - accuracy: 0.6743 - val_loss: 1.1996 - val_accuracy: 0.6658\n",
            "Epoch 858/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1940 - accuracy: 0.6744 - val_loss: 1.1990 - val_accuracy: 0.6660\n",
            "Epoch 859/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1934 - accuracy: 0.6745 - val_loss: 1.1984 - val_accuracy: 0.6661\n",
            "Epoch 860/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.1928 - accuracy: 0.6746 - val_loss: 1.1978 - val_accuracy: 0.6662\n",
            "Epoch 861/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1922 - accuracy: 0.6746 - val_loss: 1.1972 - val_accuracy: 0.6662\n",
            "Epoch 862/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1916 - accuracy: 0.6747 - val_loss: 1.1966 - val_accuracy: 0.6667\n",
            "Epoch 863/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.1910 - accuracy: 0.6748 - val_loss: 1.1960 - val_accuracy: 0.6667\n",
            "Epoch 864/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1903 - accuracy: 0.6749 - val_loss: 1.1954 - val_accuracy: 0.6667\n",
            "Epoch 865/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1897 - accuracy: 0.6751 - val_loss: 1.1948 - val_accuracy: 0.6669\n",
            "Epoch 866/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1891 - accuracy: 0.6751 - val_loss: 1.1942 - val_accuracy: 0.6667\n",
            "Epoch 867/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1885 - accuracy: 0.6752 - val_loss: 1.1936 - val_accuracy: 0.6667\n",
            "Epoch 868/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1879 - accuracy: 0.6752 - val_loss: 1.1930 - val_accuracy: 0.6667\n",
            "Epoch 869/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1873 - accuracy: 0.6752 - val_loss: 1.1924 - val_accuracy: 0.6665\n",
            "Epoch 870/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1867 - accuracy: 0.6753 - val_loss: 1.1918 - val_accuracy: 0.6664\n",
            "Epoch 871/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.1861 - accuracy: 0.6754 - val_loss: 1.1913 - val_accuracy: 0.6665\n",
            "Epoch 872/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.1855 - accuracy: 0.6754 - val_loss: 1.1907 - val_accuracy: 0.6665\n",
            "Epoch 873/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1849 - accuracy: 0.6755 - val_loss: 1.1901 - val_accuracy: 0.6665\n",
            "Epoch 874/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1843 - accuracy: 0.6756 - val_loss: 1.1895 - val_accuracy: 0.6667\n",
            "Epoch 875/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1837 - accuracy: 0.6757 - val_loss: 1.1889 - val_accuracy: 0.6667\n",
            "Epoch 876/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1831 - accuracy: 0.6758 - val_loss: 1.1883 - val_accuracy: 0.6667\n",
            "Epoch 877/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1825 - accuracy: 0.6760 - val_loss: 1.1877 - val_accuracy: 0.6669\n",
            "Epoch 878/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.1819 - accuracy: 0.6761 - val_loss: 1.1871 - val_accuracy: 0.6670\n",
            "Epoch 879/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1813 - accuracy: 0.6762 - val_loss: 1.1865 - val_accuracy: 0.6670\n",
            "Epoch 880/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1808 - accuracy: 0.6763 - val_loss: 1.1859 - val_accuracy: 0.6671\n",
            "Epoch 881/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1802 - accuracy: 0.6765 - val_loss: 1.1854 - val_accuracy: 0.6672\n",
            "Epoch 882/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1796 - accuracy: 0.6765 - val_loss: 1.1848 - val_accuracy: 0.6673\n",
            "Epoch 883/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1790 - accuracy: 0.6766 - val_loss: 1.1842 - val_accuracy: 0.6674\n",
            "Epoch 884/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1784 - accuracy: 0.6768 - val_loss: 1.1836 - val_accuracy: 0.6675\n",
            "Epoch 885/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1778 - accuracy: 0.6769 - val_loss: 1.1830 - val_accuracy: 0.6676\n",
            "Epoch 886/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.1772 - accuracy: 0.6770 - val_loss: 1.1824 - val_accuracy: 0.6677\n",
            "Epoch 887/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1766 - accuracy: 0.6770 - val_loss: 1.1819 - val_accuracy: 0.6679\n",
            "Epoch 888/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1760 - accuracy: 0.6771 - val_loss: 1.1813 - val_accuracy: 0.6679\n",
            "Epoch 889/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1755 - accuracy: 0.6771 - val_loss: 1.1807 - val_accuracy: 0.6678\n",
            "Epoch 890/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1749 - accuracy: 0.6772 - val_loss: 1.1801 - val_accuracy: 0.6678\n",
            "Epoch 891/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.1743 - accuracy: 0.6773 - val_loss: 1.1795 - val_accuracy: 0.6676\n",
            "Epoch 892/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1737 - accuracy: 0.6775 - val_loss: 1.1790 - val_accuracy: 0.6677\n",
            "Epoch 893/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1731 - accuracy: 0.6777 - val_loss: 1.1784 - val_accuracy: 0.6681\n",
            "Epoch 894/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.1725 - accuracy: 0.6778 - val_loss: 1.1778 - val_accuracy: 0.6683\n",
            "Epoch 895/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1720 - accuracy: 0.6779 - val_loss: 1.1772 - val_accuracy: 0.6684\n",
            "Epoch 896/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1714 - accuracy: 0.6779 - val_loss: 1.1767 - val_accuracy: 0.6686\n",
            "Epoch 897/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1708 - accuracy: 0.6779 - val_loss: 1.1761 - val_accuracy: 0.6686\n",
            "Epoch 898/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1702 - accuracy: 0.6780 - val_loss: 1.1755 - val_accuracy: 0.6685\n",
            "Epoch 899/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1696 - accuracy: 0.6782 - val_loss: 1.1749 - val_accuracy: 0.6687\n",
            "Epoch 900/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.1691 - accuracy: 0.6783 - val_loss: 1.1744 - val_accuracy: 0.6690\n",
            "Epoch 901/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1685 - accuracy: 0.6783 - val_loss: 1.1738 - val_accuracy: 0.6692\n",
            "Epoch 902/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.1679 - accuracy: 0.6784 - val_loss: 1.1732 - val_accuracy: 0.6694\n",
            "Epoch 903/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1673 - accuracy: 0.6785 - val_loss: 1.1727 - val_accuracy: 0.6697\n",
            "Epoch 904/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1668 - accuracy: 0.6785 - val_loss: 1.1721 - val_accuracy: 0.6698\n",
            "Epoch 905/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.1662 - accuracy: 0.6786 - val_loss: 1.1715 - val_accuracy: 0.6700\n",
            "Epoch 906/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1656 - accuracy: 0.6786 - val_loss: 1.1710 - val_accuracy: 0.6703\n",
            "Epoch 907/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1651 - accuracy: 0.6787 - val_loss: 1.1704 - val_accuracy: 0.6704\n",
            "Epoch 908/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1645 - accuracy: 0.6787 - val_loss: 1.1698 - val_accuracy: 0.6706\n",
            "Epoch 909/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1639 - accuracy: 0.6787 - val_loss: 1.1693 - val_accuracy: 0.6706\n",
            "Epoch 910/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1633 - accuracy: 0.6788 - val_loss: 1.1687 - val_accuracy: 0.6707\n",
            "Epoch 911/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.1628 - accuracy: 0.6788 - val_loss: 1.1682 - val_accuracy: 0.6707\n",
            "Epoch 912/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1622 - accuracy: 0.6790 - val_loss: 1.1676 - val_accuracy: 0.6709\n",
            "Epoch 913/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1616 - accuracy: 0.6791 - val_loss: 1.1670 - val_accuracy: 0.6710\n",
            "Epoch 914/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.1611 - accuracy: 0.6793 - val_loss: 1.1665 - val_accuracy: 0.6714\n",
            "Epoch 915/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1605 - accuracy: 0.6794 - val_loss: 1.1659 - val_accuracy: 0.6715\n",
            "Epoch 916/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1599 - accuracy: 0.6795 - val_loss: 1.1654 - val_accuracy: 0.6716\n",
            "Epoch 917/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1594 - accuracy: 0.6796 - val_loss: 1.1648 - val_accuracy: 0.6716\n",
            "Epoch 918/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1588 - accuracy: 0.6796 - val_loss: 1.1642 - val_accuracy: 0.6717\n",
            "Epoch 919/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1583 - accuracy: 0.6797 - val_loss: 1.1637 - val_accuracy: 0.6720\n",
            "Epoch 920/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.1577 - accuracy: 0.6797 - val_loss: 1.1631 - val_accuracy: 0.6721\n",
            "Epoch 921/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1571 - accuracy: 0.6798 - val_loss: 1.1626 - val_accuracy: 0.6724\n",
            "Epoch 922/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.1566 - accuracy: 0.6799 - val_loss: 1.1620 - val_accuracy: 0.6725\n",
            "Epoch 923/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1560 - accuracy: 0.6799 - val_loss: 1.1615 - val_accuracy: 0.6725\n",
            "Epoch 924/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1555 - accuracy: 0.6800 - val_loss: 1.1609 - val_accuracy: 0.6726\n",
            "Epoch 925/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1549 - accuracy: 0.6801 - val_loss: 1.1604 - val_accuracy: 0.6726\n",
            "Epoch 926/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1543 - accuracy: 0.6802 - val_loss: 1.1598 - val_accuracy: 0.6727\n",
            "Epoch 927/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1538 - accuracy: 0.6803 - val_loss: 1.1593 - val_accuracy: 0.6728\n",
            "Epoch 928/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.1532 - accuracy: 0.6804 - val_loss: 1.1587 - val_accuracy: 0.6728\n",
            "Epoch 929/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1527 - accuracy: 0.6805 - val_loss: 1.1582 - val_accuracy: 0.6729\n",
            "Epoch 930/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.1521 - accuracy: 0.6806 - val_loss: 1.1576 - val_accuracy: 0.6730\n",
            "Epoch 931/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1516 - accuracy: 0.6808 - val_loss: 1.1571 - val_accuracy: 0.6731\n",
            "Epoch 932/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.1510 - accuracy: 0.6807 - val_loss: 1.1565 - val_accuracy: 0.6734\n",
            "Epoch 933/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1505 - accuracy: 0.6808 - val_loss: 1.1560 - val_accuracy: 0.6735\n",
            "Epoch 934/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1499 - accuracy: 0.6809 - val_loss: 1.1554 - val_accuracy: 0.6736\n",
            "Epoch 935/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1494 - accuracy: 0.6810 - val_loss: 1.1549 - val_accuracy: 0.6735\n",
            "Epoch 936/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1488 - accuracy: 0.6810 - val_loss: 1.1544 - val_accuracy: 0.6735\n",
            "Epoch 937/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1483 - accuracy: 0.6811 - val_loss: 1.1538 - val_accuracy: 0.6737\n",
            "Epoch 938/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.1477 - accuracy: 0.6811 - val_loss: 1.1533 - val_accuracy: 0.6740\n",
            "Epoch 939/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1472 - accuracy: 0.6812 - val_loss: 1.1527 - val_accuracy: 0.6742\n",
            "Epoch 940/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1466 - accuracy: 0.6812 - val_loss: 1.1522 - val_accuracy: 0.6742\n",
            "Epoch 941/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1461 - accuracy: 0.6813 - val_loss: 1.1517 - val_accuracy: 0.6744\n",
            "Epoch 942/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1455 - accuracy: 0.6814 - val_loss: 1.1511 - val_accuracy: 0.6745\n",
            "Epoch 943/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1450 - accuracy: 0.6815 - val_loss: 1.1506 - val_accuracy: 0.6746\n",
            "Epoch 944/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.1445 - accuracy: 0.6816 - val_loss: 1.1500 - val_accuracy: 0.6747\n",
            "Epoch 945/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1439 - accuracy: 0.6817 - val_loss: 1.1495 - val_accuracy: 0.6748\n",
            "Epoch 946/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1434 - accuracy: 0.6818 - val_loss: 1.1490 - val_accuracy: 0.6748\n",
            "Epoch 947/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1428 - accuracy: 0.6819 - val_loss: 1.1484 - val_accuracy: 0.6750\n",
            "Epoch 948/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1423 - accuracy: 0.6820 - val_loss: 1.1479 - val_accuracy: 0.6750\n",
            "Epoch 949/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1417 - accuracy: 0.6821 - val_loss: 1.1474 - val_accuracy: 0.6750\n",
            "Epoch 950/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1412 - accuracy: 0.6821 - val_loss: 1.1468 - val_accuracy: 0.6750\n",
            "Epoch 951/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.1407 - accuracy: 0.6822 - val_loss: 1.1463 - val_accuracy: 0.6751\n",
            "Epoch 952/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1401 - accuracy: 0.6823 - val_loss: 1.1458 - val_accuracy: 0.6752\n",
            "Epoch 953/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1396 - accuracy: 0.6824 - val_loss: 1.1452 - val_accuracy: 0.6755\n",
            "Epoch 954/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1391 - accuracy: 0.6825 - val_loss: 1.1447 - val_accuracy: 0.6756\n",
            "Epoch 955/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1385 - accuracy: 0.6827 - val_loss: 1.1442 - val_accuracy: 0.6756\n",
            "Epoch 956/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1380 - accuracy: 0.6827 - val_loss: 1.1437 - val_accuracy: 0.6756\n",
            "Epoch 957/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.1375 - accuracy: 0.6829 - val_loss: 1.1431 - val_accuracy: 0.6756\n",
            "Epoch 958/5000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.1369 - accuracy: 0.6830 - val_loss: 1.1426 - val_accuracy: 0.6756\n",
            "Epoch 959/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1364 - accuracy: 0.6830 - val_loss: 1.1421 - val_accuracy: 0.6756\n",
            "Epoch 960/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1359 - accuracy: 0.6831 - val_loss: 1.1415 - val_accuracy: 0.6757\n",
            "Epoch 961/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.1353 - accuracy: 0.6832 - val_loss: 1.1410 - val_accuracy: 0.6759\n",
            "Epoch 962/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1348 - accuracy: 0.6833 - val_loss: 1.1405 - val_accuracy: 0.6762\n",
            "Epoch 963/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1343 - accuracy: 0.6833 - val_loss: 1.1400 - val_accuracy: 0.6763\n",
            "Epoch 964/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1337 - accuracy: 0.6834 - val_loss: 1.1395 - val_accuracy: 0.6763\n",
            "Epoch 965/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1332 - accuracy: 0.6835 - val_loss: 1.1389 - val_accuracy: 0.6766\n",
            "Epoch 966/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.1327 - accuracy: 0.6837 - val_loss: 1.1384 - val_accuracy: 0.6768\n",
            "Epoch 967/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1322 - accuracy: 0.6837 - val_loss: 1.1379 - val_accuracy: 0.6770\n",
            "Epoch 968/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1316 - accuracy: 0.6838 - val_loss: 1.1374 - val_accuracy: 0.6770\n",
            "Epoch 969/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1311 - accuracy: 0.6839 - val_loss: 1.1369 - val_accuracy: 0.6769\n",
            "Epoch 970/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.1306 - accuracy: 0.6839 - val_loss: 1.1363 - val_accuracy: 0.6770\n",
            "Epoch 971/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1301 - accuracy: 0.6840 - val_loss: 1.1358 - val_accuracy: 0.6768\n",
            "Epoch 972/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1295 - accuracy: 0.6840 - val_loss: 1.1353 - val_accuracy: 0.6769\n",
            "Epoch 973/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1290 - accuracy: 0.6841 - val_loss: 1.1348 - val_accuracy: 0.6770\n",
            "Epoch 974/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1285 - accuracy: 0.6842 - val_loss: 1.1343 - val_accuracy: 0.6770\n",
            "Epoch 975/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1280 - accuracy: 0.6843 - val_loss: 1.1338 - val_accuracy: 0.6771\n",
            "Epoch 976/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1275 - accuracy: 0.6844 - val_loss: 1.1332 - val_accuracy: 0.6772\n",
            "Epoch 977/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.1269 - accuracy: 0.6845 - val_loss: 1.1327 - val_accuracy: 0.6772\n",
            "Epoch 978/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1264 - accuracy: 0.6846 - val_loss: 1.1322 - val_accuracy: 0.6773\n",
            "Epoch 979/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1259 - accuracy: 0.6847 - val_loss: 1.1317 - val_accuracy: 0.6773\n",
            "Epoch 980/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1254 - accuracy: 0.6847 - val_loss: 1.1312 - val_accuracy: 0.6774\n",
            "Epoch 981/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1249 - accuracy: 0.6848 - val_loss: 1.1307 - val_accuracy: 0.6773\n",
            "Epoch 982/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1243 - accuracy: 0.6848 - val_loss: 1.1302 - val_accuracy: 0.6773\n",
            "Epoch 983/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1238 - accuracy: 0.6850 - val_loss: 1.1297 - val_accuracy: 0.6774\n",
            "Epoch 984/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1233 - accuracy: 0.6851 - val_loss: 1.1291 - val_accuracy: 0.6774\n",
            "Epoch 985/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1228 - accuracy: 0.6852 - val_loss: 1.1286 - val_accuracy: 0.6774\n",
            "Epoch 986/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1223 - accuracy: 0.6852 - val_loss: 1.1281 - val_accuracy: 0.6777\n",
            "Epoch 987/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1218 - accuracy: 0.6854 - val_loss: 1.1276 - val_accuracy: 0.6779\n",
            "Epoch 988/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1213 - accuracy: 0.6855 - val_loss: 1.1271 - val_accuracy: 0.6779\n",
            "Epoch 989/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.1207 - accuracy: 0.6855 - val_loss: 1.1266 - val_accuracy: 0.6779\n",
            "Epoch 990/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1202 - accuracy: 0.6856 - val_loss: 1.1261 - val_accuracy: 0.6779\n",
            "Epoch 991/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1197 - accuracy: 0.6857 - val_loss: 1.1256 - val_accuracy: 0.6779\n",
            "Epoch 992/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1192 - accuracy: 0.6857 - val_loss: 1.1251 - val_accuracy: 0.6779\n",
            "Epoch 993/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.1187 - accuracy: 0.6857 - val_loss: 1.1246 - val_accuracy: 0.6781\n",
            "Epoch 994/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1182 - accuracy: 0.6859 - val_loss: 1.1241 - val_accuracy: 0.6781\n",
            "Epoch 995/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1177 - accuracy: 0.6860 - val_loss: 1.1236 - val_accuracy: 0.6783\n",
            "Epoch 996/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1172 - accuracy: 0.6862 - val_loss: 1.1231 - val_accuracy: 0.6784\n",
            "Epoch 997/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1167 - accuracy: 0.6863 - val_loss: 1.1226 - val_accuracy: 0.6786\n",
            "Epoch 998/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1162 - accuracy: 0.6864 - val_loss: 1.1221 - val_accuracy: 0.6786\n",
            "Epoch 999/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1157 - accuracy: 0.6865 - val_loss: 1.1216 - val_accuracy: 0.6787\n",
            "Epoch 1000/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1152 - accuracy: 0.6866 - val_loss: 1.1211 - val_accuracy: 0.6787\n",
            "Epoch 1001/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1147 - accuracy: 0.6866 - val_loss: 1.1206 - val_accuracy: 0.6787\n",
            "Epoch 1002/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1142 - accuracy: 0.6867 - val_loss: 1.1201 - val_accuracy: 0.6786\n",
            "Epoch 1003/5000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.1137 - accuracy: 0.6868 - val_loss: 1.1196 - val_accuracy: 0.6786\n",
            "Epoch 1004/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1132 - accuracy: 0.6868 - val_loss: 1.1191 - val_accuracy: 0.6789\n",
            "Epoch 1005/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1127 - accuracy: 0.6869 - val_loss: 1.1186 - val_accuracy: 0.6790\n",
            "Epoch 1006/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1122 - accuracy: 0.6871 - val_loss: 1.1181 - val_accuracy: 0.6790\n",
            "Epoch 1007/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.1117 - accuracy: 0.6872 - val_loss: 1.1176 - val_accuracy: 0.6792\n",
            "Epoch 1008/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.1112 - accuracy: 0.6873 - val_loss: 1.1171 - val_accuracy: 0.6792\n",
            "Epoch 1009/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.1107 - accuracy: 0.6874 - val_loss: 1.1166 - val_accuracy: 0.6792\n",
            "Epoch 1010/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1102 - accuracy: 0.6875 - val_loss: 1.1162 - val_accuracy: 0.6794\n",
            "Epoch 1011/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1097 - accuracy: 0.6876 - val_loss: 1.1157 - val_accuracy: 0.6796\n",
            "Epoch 1012/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.1092 - accuracy: 0.6876 - val_loss: 1.1152 - val_accuracy: 0.6801\n",
            "Epoch 1013/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.1087 - accuracy: 0.6877 - val_loss: 1.1147 - val_accuracy: 0.6802\n",
            "Epoch 1014/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.1082 - accuracy: 0.6878 - val_loss: 1.1142 - val_accuracy: 0.6805\n",
            "Epoch 1015/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.1077 - accuracy: 0.6878 - val_loss: 1.1137 - val_accuracy: 0.6807\n",
            "Epoch 1016/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1072 - accuracy: 0.6879 - val_loss: 1.1132 - val_accuracy: 0.6809\n",
            "Epoch 1017/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1067 - accuracy: 0.6880 - val_loss: 1.1127 - val_accuracy: 0.6811\n",
            "Epoch 1018/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1062 - accuracy: 0.6880 - val_loss: 1.1122 - val_accuracy: 0.6811\n",
            "Epoch 1019/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1057 - accuracy: 0.6880 - val_loss: 1.1118 - val_accuracy: 0.6812\n",
            "Epoch 1020/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1052 - accuracy: 0.6881 - val_loss: 1.1113 - val_accuracy: 0.6813\n",
            "Epoch 1021/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.1047 - accuracy: 0.6881 - val_loss: 1.1108 - val_accuracy: 0.6814\n",
            "Epoch 1022/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.1042 - accuracy: 0.6881 - val_loss: 1.1103 - val_accuracy: 0.6813\n",
            "Epoch 1023/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.1037 - accuracy: 0.6882 - val_loss: 1.1098 - val_accuracy: 0.6814\n",
            "Epoch 1024/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1033 - accuracy: 0.6882 - val_loss: 1.1093 - val_accuracy: 0.6814\n",
            "Epoch 1025/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1028 - accuracy: 0.6883 - val_loss: 1.1089 - val_accuracy: 0.6815\n",
            "Epoch 1026/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1023 - accuracy: 0.6884 - val_loss: 1.1084 - val_accuracy: 0.6816\n",
            "Epoch 1027/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.1018 - accuracy: 0.6885 - val_loss: 1.1079 - val_accuracy: 0.6816\n",
            "Epoch 1028/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.1013 - accuracy: 0.6885 - val_loss: 1.1074 - val_accuracy: 0.6817\n",
            "Epoch 1029/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1008 - accuracy: 0.6886 - val_loss: 1.1069 - val_accuracy: 0.6818\n",
            "Epoch 1030/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.1003 - accuracy: 0.6887 - val_loss: 1.1065 - val_accuracy: 0.6821\n",
            "Epoch 1031/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0999 - accuracy: 0.6889 - val_loss: 1.1060 - val_accuracy: 0.6821\n",
            "Epoch 1032/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0994 - accuracy: 0.6890 - val_loss: 1.1055 - val_accuracy: 0.6823\n",
            "Epoch 1033/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0989 - accuracy: 0.6890 - val_loss: 1.1050 - val_accuracy: 0.6823\n",
            "Epoch 1034/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0984 - accuracy: 0.6891 - val_loss: 1.1045 - val_accuracy: 0.6824\n",
            "Epoch 1035/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0979 - accuracy: 0.6892 - val_loss: 1.1041 - val_accuracy: 0.6823\n",
            "Epoch 1036/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0974 - accuracy: 0.6893 - val_loss: 1.1036 - val_accuracy: 0.6826\n",
            "Epoch 1037/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0970 - accuracy: 0.6895 - val_loss: 1.1031 - val_accuracy: 0.6827\n",
            "Epoch 1038/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0965 - accuracy: 0.6895 - val_loss: 1.1026 - val_accuracy: 0.6828\n",
            "Epoch 1039/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0960 - accuracy: 0.6896 - val_loss: 1.1022 - val_accuracy: 0.6828\n",
            "Epoch 1040/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0955 - accuracy: 0.6898 - val_loss: 1.1017 - val_accuracy: 0.6828\n",
            "Epoch 1041/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0950 - accuracy: 0.6899 - val_loss: 1.1012 - val_accuracy: 0.6829\n",
            "Epoch 1042/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0946 - accuracy: 0.6900 - val_loss: 1.1008 - val_accuracy: 0.6831\n",
            "Epoch 1043/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0941 - accuracy: 0.6900 - val_loss: 1.1003 - val_accuracy: 0.6832\n",
            "Epoch 1044/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0936 - accuracy: 0.6901 - val_loss: 1.0998 - val_accuracy: 0.6833\n",
            "Epoch 1045/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.0931 - accuracy: 0.6901 - val_loss: 1.0993 - val_accuracy: 0.6833\n",
            "Epoch 1046/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0927 - accuracy: 0.6902 - val_loss: 1.0989 - val_accuracy: 0.6835\n",
            "Epoch 1047/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0922 - accuracy: 0.6902 - val_loss: 1.0984 - val_accuracy: 0.6836\n",
            "Epoch 1048/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.0917 - accuracy: 0.6903 - val_loss: 1.0979 - val_accuracy: 0.6837\n",
            "Epoch 1049/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0912 - accuracy: 0.6903 - val_loss: 1.0975 - val_accuracy: 0.6837\n",
            "Epoch 1050/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0908 - accuracy: 0.6904 - val_loss: 1.0970 - val_accuracy: 0.6838\n",
            "Epoch 1051/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0903 - accuracy: 0.6905 - val_loss: 1.0965 - val_accuracy: 0.6837\n",
            "Epoch 1052/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0898 - accuracy: 0.6906 - val_loss: 1.0961 - val_accuracy: 0.6839\n",
            "Epoch 1053/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0894 - accuracy: 0.6907 - val_loss: 1.0956 - val_accuracy: 0.6841\n",
            "Epoch 1054/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0889 - accuracy: 0.6908 - val_loss: 1.0951 - val_accuracy: 0.6842\n",
            "Epoch 1055/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0884 - accuracy: 0.6909 - val_loss: 1.0947 - val_accuracy: 0.6841\n",
            "Epoch 1056/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0880 - accuracy: 0.6910 - val_loss: 1.0942 - val_accuracy: 0.6841\n",
            "Epoch 1057/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0875 - accuracy: 0.6910 - val_loss: 1.0938 - val_accuracy: 0.6844\n",
            "Epoch 1058/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0870 - accuracy: 0.6910 - val_loss: 1.0933 - val_accuracy: 0.6847\n",
            "Epoch 1059/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0865 - accuracy: 0.6911 - val_loss: 1.0928 - val_accuracy: 0.6848\n",
            "Epoch 1060/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0861 - accuracy: 0.6912 - val_loss: 1.0924 - val_accuracy: 0.6850\n",
            "Epoch 1061/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0856 - accuracy: 0.6912 - val_loss: 1.0919 - val_accuracy: 0.6850\n",
            "Epoch 1062/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0852 - accuracy: 0.6913 - val_loss: 1.0915 - val_accuracy: 0.6850\n",
            "Epoch 1063/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0847 - accuracy: 0.6915 - val_loss: 1.0910 - val_accuracy: 0.6852\n",
            "Epoch 1064/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.0842 - accuracy: 0.6916 - val_loss: 1.0905 - val_accuracy: 0.6851\n",
            "Epoch 1065/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0838 - accuracy: 0.6917 - val_loss: 1.0901 - val_accuracy: 0.6852\n",
            "Epoch 1066/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0833 - accuracy: 0.6917 - val_loss: 1.0896 - val_accuracy: 0.6854\n",
            "Epoch 1067/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.0828 - accuracy: 0.6918 - val_loss: 1.0892 - val_accuracy: 0.6856\n",
            "Epoch 1068/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0824 - accuracy: 0.6919 - val_loss: 1.0887 - val_accuracy: 0.6858\n",
            "Epoch 1069/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0819 - accuracy: 0.6920 - val_loss: 1.0883 - val_accuracy: 0.6859\n",
            "Epoch 1070/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0815 - accuracy: 0.6920 - val_loss: 1.0878 - val_accuracy: 0.6859\n",
            "Epoch 1071/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0810 - accuracy: 0.6921 - val_loss: 1.0873 - val_accuracy: 0.6861\n",
            "Epoch 1072/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0805 - accuracy: 0.6921 - val_loss: 1.0869 - val_accuracy: 0.6861\n",
            "Epoch 1073/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0801 - accuracy: 0.6922 - val_loss: 1.0864 - val_accuracy: 0.6862\n",
            "Epoch 1074/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0796 - accuracy: 0.6923 - val_loss: 1.0860 - val_accuracy: 0.6863\n",
            "Epoch 1075/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0792 - accuracy: 0.6923 - val_loss: 1.0855 - val_accuracy: 0.6864\n",
            "Epoch 1076/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0787 - accuracy: 0.6924 - val_loss: 1.0851 - val_accuracy: 0.6864\n",
            "Epoch 1077/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0782 - accuracy: 0.6926 - val_loss: 1.0846 - val_accuracy: 0.6863\n",
            "Epoch 1078/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0778 - accuracy: 0.6926 - val_loss: 1.0842 - val_accuracy: 0.6864\n",
            "Epoch 1079/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0773 - accuracy: 0.6927 - val_loss: 1.0837 - val_accuracy: 0.6864\n",
            "Epoch 1080/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0769 - accuracy: 0.6929 - val_loss: 1.0833 - val_accuracy: 0.6868\n",
            "Epoch 1081/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0764 - accuracy: 0.6929 - val_loss: 1.0828 - val_accuracy: 0.6868\n",
            "Epoch 1082/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0760 - accuracy: 0.6930 - val_loss: 1.0824 - val_accuracy: 0.6868\n",
            "Epoch 1083/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0755 - accuracy: 0.6931 - val_loss: 1.0819 - val_accuracy: 0.6868\n",
            "Epoch 1084/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0751 - accuracy: 0.6931 - val_loss: 1.0815 - val_accuracy: 0.6869\n",
            "Epoch 1085/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.0746 - accuracy: 0.6932 - val_loss: 1.0810 - val_accuracy: 0.6872\n",
            "Epoch 1086/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0742 - accuracy: 0.6932 - val_loss: 1.0806 - val_accuracy: 0.6873\n",
            "Epoch 1087/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.0737 - accuracy: 0.6933 - val_loss: 1.0802 - val_accuracy: 0.6876\n",
            "Epoch 1088/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0733 - accuracy: 0.6935 - val_loss: 1.0797 - val_accuracy: 0.6876\n",
            "Epoch 1089/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0728 - accuracy: 0.6936 - val_loss: 1.0793 - val_accuracy: 0.6875\n",
            "Epoch 1090/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0724 - accuracy: 0.6936 - val_loss: 1.0788 - val_accuracy: 0.6875\n",
            "Epoch 1091/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.0719 - accuracy: 0.6937 - val_loss: 1.0784 - val_accuracy: 0.6875\n",
            "Epoch 1092/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0715 - accuracy: 0.6937 - val_loss: 1.0779 - val_accuracy: 0.6875\n",
            "Epoch 1093/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0710 - accuracy: 0.6937 - val_loss: 1.0775 - val_accuracy: 0.6874\n",
            "Epoch 1094/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0706 - accuracy: 0.6938 - val_loss: 1.0771 - val_accuracy: 0.6878\n",
            "Epoch 1095/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0701 - accuracy: 0.6938 - val_loss: 1.0766 - val_accuracy: 0.6880\n",
            "Epoch 1096/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0697 - accuracy: 0.6938 - val_loss: 1.0762 - val_accuracy: 0.6880\n",
            "Epoch 1097/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0692 - accuracy: 0.6939 - val_loss: 1.0757 - val_accuracy: 0.6879\n",
            "Epoch 1098/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0688 - accuracy: 0.6940 - val_loss: 1.0753 - val_accuracy: 0.6881\n",
            "Epoch 1099/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.0683 - accuracy: 0.6940 - val_loss: 1.0749 - val_accuracy: 0.6881\n",
            "Epoch 1100/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.0679 - accuracy: 0.6941 - val_loss: 1.0744 - val_accuracy: 0.6881\n",
            "Epoch 1101/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0675 - accuracy: 0.6941 - val_loss: 1.0740 - val_accuracy: 0.6883\n",
            "Epoch 1102/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0670 - accuracy: 0.6942 - val_loss: 1.0735 - val_accuracy: 0.6885\n",
            "Epoch 1103/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0666 - accuracy: 0.6943 - val_loss: 1.0731 - val_accuracy: 0.6888\n",
            "Epoch 1104/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0661 - accuracy: 0.6943 - val_loss: 1.0727 - val_accuracy: 0.6889\n",
            "Epoch 1105/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0657 - accuracy: 0.6944 - val_loss: 1.0722 - val_accuracy: 0.6891\n",
            "Epoch 1106/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0653 - accuracy: 0.6945 - val_loss: 1.0718 - val_accuracy: 0.6893\n",
            "Epoch 1107/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0648 - accuracy: 0.6945 - val_loss: 1.0714 - val_accuracy: 0.6893\n",
            "Epoch 1108/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0644 - accuracy: 0.6945 - val_loss: 1.0709 - val_accuracy: 0.6894\n",
            "Epoch 1109/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0639 - accuracy: 0.6946 - val_loss: 1.0705 - val_accuracy: 0.6895\n",
            "Epoch 1110/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0635 - accuracy: 0.6946 - val_loss: 1.0701 - val_accuracy: 0.6895\n",
            "Epoch 1111/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0631 - accuracy: 0.6947 - val_loss: 1.0696 - val_accuracy: 0.6894\n",
            "Epoch 1112/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0626 - accuracy: 0.6948 - val_loss: 1.0692 - val_accuracy: 0.6896\n",
            "Epoch 1113/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0622 - accuracy: 0.6948 - val_loss: 1.0688 - val_accuracy: 0.6896\n",
            "Epoch 1114/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0618 - accuracy: 0.6948 - val_loss: 1.0684 - val_accuracy: 0.6896\n",
            "Epoch 1115/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.0613 - accuracy: 0.6949 - val_loss: 1.0679 - val_accuracy: 0.6898\n",
            "Epoch 1116/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0609 - accuracy: 0.6949 - val_loss: 1.0675 - val_accuracy: 0.6898\n",
            "Epoch 1117/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0605 - accuracy: 0.6950 - val_loss: 1.0671 - val_accuracy: 0.6897\n",
            "Epoch 1118/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.0600 - accuracy: 0.6951 - val_loss: 1.0666 - val_accuracy: 0.6897\n",
            "Epoch 1119/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0596 - accuracy: 0.6951 - val_loss: 1.0662 - val_accuracy: 0.6897\n",
            "Epoch 1120/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0592 - accuracy: 0.6951 - val_loss: 1.0658 - val_accuracy: 0.6896\n",
            "Epoch 1121/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.0587 - accuracy: 0.6952 - val_loss: 1.0654 - val_accuracy: 0.6896\n",
            "Epoch 1122/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0583 - accuracy: 0.6953 - val_loss: 1.0649 - val_accuracy: 0.6898\n",
            "Epoch 1123/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0579 - accuracy: 0.6953 - val_loss: 1.0645 - val_accuracy: 0.6901\n",
            "Epoch 1124/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0574 - accuracy: 0.6954 - val_loss: 1.0641 - val_accuracy: 0.6902\n",
            "Epoch 1125/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0570 - accuracy: 0.6956 - val_loss: 1.0637 - val_accuracy: 0.6902\n",
            "Epoch 1126/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0566 - accuracy: 0.6956 - val_loss: 1.0632 - val_accuracy: 0.6905\n",
            "Epoch 1127/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0561 - accuracy: 0.6957 - val_loss: 1.0628 - val_accuracy: 0.6906\n",
            "Epoch 1128/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0557 - accuracy: 0.6958 - val_loss: 1.0624 - val_accuracy: 0.6908\n",
            "Epoch 1129/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0553 - accuracy: 0.6958 - val_loss: 1.0620 - val_accuracy: 0.6907\n",
            "Epoch 1130/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.0549 - accuracy: 0.6959 - val_loss: 1.0615 - val_accuracy: 0.6908\n",
            "Epoch 1131/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0544 - accuracy: 0.6960 - val_loss: 1.0611 - val_accuracy: 0.6908\n",
            "Epoch 1132/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0540 - accuracy: 0.6961 - val_loss: 1.0607 - val_accuracy: 0.6909\n",
            "Epoch 1133/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0536 - accuracy: 0.6962 - val_loss: 1.0603 - val_accuracy: 0.6910\n",
            "Epoch 1134/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0532 - accuracy: 0.6963 - val_loss: 1.0599 - val_accuracy: 0.6910\n",
            "Epoch 1135/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0527 - accuracy: 0.6963 - val_loss: 1.0595 - val_accuracy: 0.6909\n",
            "Epoch 1136/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0523 - accuracy: 0.6963 - val_loss: 1.0590 - val_accuracy: 0.6908\n",
            "Epoch 1137/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0519 - accuracy: 0.6964 - val_loss: 1.0586 - val_accuracy: 0.6909\n",
            "Epoch 1138/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.0515 - accuracy: 0.6966 - val_loss: 1.0582 - val_accuracy: 0.6909\n",
            "Epoch 1139/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0510 - accuracy: 0.6966 - val_loss: 1.0578 - val_accuracy: 0.6910\n",
            "Epoch 1140/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0506 - accuracy: 0.6967 - val_loss: 1.0574 - val_accuracy: 0.6911\n",
            "Epoch 1141/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0502 - accuracy: 0.6967 - val_loss: 1.0570 - val_accuracy: 0.6911\n",
            "Epoch 1142/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.0498 - accuracy: 0.6969 - val_loss: 1.0565 - val_accuracy: 0.6911\n",
            "Epoch 1143/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0494 - accuracy: 0.6970 - val_loss: 1.0561 - val_accuracy: 0.6912\n",
            "Epoch 1144/5000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.0489 - accuracy: 0.6970 - val_loss: 1.0557 - val_accuracy: 0.6913\n",
            "Epoch 1145/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0485 - accuracy: 0.6971 - val_loss: 1.0553 - val_accuracy: 0.6914\n",
            "Epoch 1146/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0481 - accuracy: 0.6972 - val_loss: 1.0549 - val_accuracy: 0.6914\n",
            "Epoch 1147/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0477 - accuracy: 0.6972 - val_loss: 1.0545 - val_accuracy: 0.6914\n",
            "Epoch 1148/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0473 - accuracy: 0.6973 - val_loss: 1.0541 - val_accuracy: 0.6915\n",
            "Epoch 1149/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0468 - accuracy: 0.6974 - val_loss: 1.0536 - val_accuracy: 0.6915\n",
            "Epoch 1150/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0464 - accuracy: 0.6975 - val_loss: 1.0532 - val_accuracy: 0.6916\n",
            "Epoch 1151/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0460 - accuracy: 0.6975 - val_loss: 1.0528 - val_accuracy: 0.6916\n",
            "Epoch 1152/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0456 - accuracy: 0.6975 - val_loss: 1.0524 - val_accuracy: 0.6915\n",
            "Epoch 1153/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0452 - accuracy: 0.6976 - val_loss: 1.0520 - val_accuracy: 0.6917\n",
            "Epoch 1154/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.0448 - accuracy: 0.6976 - val_loss: 1.0516 - val_accuracy: 0.6916\n",
            "Epoch 1155/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0444 - accuracy: 0.6977 - val_loss: 1.0512 - val_accuracy: 0.6917\n",
            "Epoch 1156/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0439 - accuracy: 0.6977 - val_loss: 1.0508 - val_accuracy: 0.6918\n",
            "Epoch 1157/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0435 - accuracy: 0.6978 - val_loss: 1.0504 - val_accuracy: 0.6918\n",
            "Epoch 1158/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0431 - accuracy: 0.6978 - val_loss: 1.0500 - val_accuracy: 0.6917\n",
            "Epoch 1159/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0427 - accuracy: 0.6978 - val_loss: 1.0496 - val_accuracy: 0.6917\n",
            "Epoch 1160/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0423 - accuracy: 0.6979 - val_loss: 1.0492 - val_accuracy: 0.6917\n",
            "Epoch 1161/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0419 - accuracy: 0.6980 - val_loss: 1.0488 - val_accuracy: 0.6918\n",
            "Epoch 1162/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0415 - accuracy: 0.6980 - val_loss: 1.0483 - val_accuracy: 0.6918\n",
            "Epoch 1163/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0411 - accuracy: 0.6980 - val_loss: 1.0479 - val_accuracy: 0.6918\n",
            "Epoch 1164/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.0407 - accuracy: 0.6980 - val_loss: 1.0475 - val_accuracy: 0.6919\n",
            "Epoch 1165/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0402 - accuracy: 0.6981 - val_loss: 1.0471 - val_accuracy: 0.6919\n",
            "Epoch 1166/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0398 - accuracy: 0.6982 - val_loss: 1.0467 - val_accuracy: 0.6920\n",
            "Epoch 1167/5000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.0394 - accuracy: 0.6983 - val_loss: 1.0463 - val_accuracy: 0.6922\n",
            "Epoch 1168/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0390 - accuracy: 0.6984 - val_loss: 1.0459 - val_accuracy: 0.6922\n",
            "Epoch 1169/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0386 - accuracy: 0.6985 - val_loss: 1.0455 - val_accuracy: 0.6922\n",
            "Epoch 1170/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.0382 - accuracy: 0.6985 - val_loss: 1.0451 - val_accuracy: 0.6922\n",
            "Epoch 1171/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0378 - accuracy: 0.6986 - val_loss: 1.0447 - val_accuracy: 0.6923\n",
            "Epoch 1172/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0374 - accuracy: 0.6986 - val_loss: 1.0443 - val_accuracy: 0.6924\n",
            "Epoch 1173/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0370 - accuracy: 0.6987 - val_loss: 1.0439 - val_accuracy: 0.6925\n",
            "Epoch 1174/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0366 - accuracy: 0.6988 - val_loss: 1.0435 - val_accuracy: 0.6925\n",
            "Epoch 1175/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0362 - accuracy: 0.6989 - val_loss: 1.0431 - val_accuracy: 0.6926\n",
            "Epoch 1176/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0358 - accuracy: 0.6990 - val_loss: 1.0427 - val_accuracy: 0.6927\n",
            "Epoch 1177/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0354 - accuracy: 0.6991 - val_loss: 1.0423 - val_accuracy: 0.6925\n",
            "Epoch 1178/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0350 - accuracy: 0.6992 - val_loss: 1.0419 - val_accuracy: 0.6924\n",
            "Epoch 1179/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0346 - accuracy: 0.6993 - val_loss: 1.0415 - val_accuracy: 0.6924\n",
            "Epoch 1180/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0342 - accuracy: 0.6993 - val_loss: 1.0412 - val_accuracy: 0.6927\n",
            "Epoch 1181/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0338 - accuracy: 0.6994 - val_loss: 1.0408 - val_accuracy: 0.6929\n",
            "Epoch 1182/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0334 - accuracy: 0.6994 - val_loss: 1.0404 - val_accuracy: 0.6929\n",
            "Epoch 1183/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0330 - accuracy: 0.6995 - val_loss: 1.0400 - val_accuracy: 0.6929\n",
            "Epoch 1184/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0326 - accuracy: 0.6996 - val_loss: 1.0396 - val_accuracy: 0.6927\n",
            "Epoch 1185/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.0322 - accuracy: 0.6996 - val_loss: 1.0392 - val_accuracy: 0.6928\n",
            "Epoch 1186/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0318 - accuracy: 0.6997 - val_loss: 1.0388 - val_accuracy: 0.6928\n",
            "Epoch 1187/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0314 - accuracy: 0.6997 - val_loss: 1.0384 - val_accuracy: 0.6929\n",
            "Epoch 1188/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.0310 - accuracy: 0.6998 - val_loss: 1.0380 - val_accuracy: 0.6929\n",
            "Epoch 1189/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0306 - accuracy: 0.6999 - val_loss: 1.0376 - val_accuracy: 0.6930\n",
            "Epoch 1190/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0302 - accuracy: 0.6999 - val_loss: 1.0372 - val_accuracy: 0.6931\n",
            "Epoch 1191/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.0298 - accuracy: 0.6999 - val_loss: 1.0368 - val_accuracy: 0.6931\n",
            "Epoch 1192/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0294 - accuracy: 0.7000 - val_loss: 1.0364 - val_accuracy: 0.6933\n",
            "Epoch 1193/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0290 - accuracy: 0.7001 - val_loss: 1.0361 - val_accuracy: 0.6935\n",
            "Epoch 1194/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0286 - accuracy: 0.7001 - val_loss: 1.0357 - val_accuracy: 0.6937\n",
            "Epoch 1195/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0282 - accuracy: 0.7002 - val_loss: 1.0353 - val_accuracy: 0.6939\n",
            "Epoch 1196/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0278 - accuracy: 0.7003 - val_loss: 1.0349 - val_accuracy: 0.6940\n",
            "Epoch 1197/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0274 - accuracy: 0.7004 - val_loss: 1.0345 - val_accuracy: 0.6939\n",
            "Epoch 1198/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0270 - accuracy: 0.7004 - val_loss: 1.0341 - val_accuracy: 0.6942\n",
            "Epoch 1199/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.0267 - accuracy: 0.7004 - val_loss: 1.0337 - val_accuracy: 0.6942\n",
            "Epoch 1200/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0263 - accuracy: 0.7005 - val_loss: 1.0333 - val_accuracy: 0.6942\n",
            "Epoch 1201/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0259 - accuracy: 0.7005 - val_loss: 1.0330 - val_accuracy: 0.6942\n",
            "Epoch 1202/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0255 - accuracy: 0.7006 - val_loss: 1.0326 - val_accuracy: 0.6943\n",
            "Epoch 1203/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0251 - accuracy: 0.7007 - val_loss: 1.0322 - val_accuracy: 0.6945\n",
            "Epoch 1204/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0247 - accuracy: 0.7007 - val_loss: 1.0318 - val_accuracy: 0.6946\n",
            "Epoch 1205/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0243 - accuracy: 0.7009 - val_loss: 1.0314 - val_accuracy: 0.6948\n",
            "Epoch 1206/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0239 - accuracy: 0.7009 - val_loss: 1.0310 - val_accuracy: 0.6948\n",
            "Epoch 1207/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0235 - accuracy: 0.7010 - val_loss: 1.0307 - val_accuracy: 0.6949\n",
            "Epoch 1208/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0231 - accuracy: 0.7010 - val_loss: 1.0303 - val_accuracy: 0.6951\n",
            "Epoch 1209/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.0228 - accuracy: 0.7010 - val_loss: 1.0299 - val_accuracy: 0.6951\n",
            "Epoch 1210/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0224 - accuracy: 0.7011 - val_loss: 1.0295 - val_accuracy: 0.6952\n",
            "Epoch 1211/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0220 - accuracy: 0.7012 - val_loss: 1.0291 - val_accuracy: 0.6952\n",
            "Epoch 1212/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 1.0216 - accuracy: 0.7013 - val_loss: 1.0288 - val_accuracy: 0.6952\n",
            "Epoch 1213/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0212 - accuracy: 0.7014 - val_loss: 1.0284 - val_accuracy: 0.6952\n",
            "Epoch 1214/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0208 - accuracy: 0.7014 - val_loss: 1.0280 - val_accuracy: 0.6954\n",
            "Epoch 1215/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.0204 - accuracy: 0.7014 - val_loss: 1.0276 - val_accuracy: 0.6954\n",
            "Epoch 1216/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0201 - accuracy: 0.7015 - val_loss: 1.0272 - val_accuracy: 0.6956\n",
            "Epoch 1217/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0197 - accuracy: 0.7016 - val_loss: 1.0269 - val_accuracy: 0.6956\n",
            "Epoch 1218/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0193 - accuracy: 0.7017 - val_loss: 1.0265 - val_accuracy: 0.6956\n",
            "Epoch 1219/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0189 - accuracy: 0.7017 - val_loss: 1.0261 - val_accuracy: 0.6956\n",
            "Epoch 1220/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.0185 - accuracy: 0.7018 - val_loss: 1.0257 - val_accuracy: 0.6957\n",
            "Epoch 1221/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0182 - accuracy: 0.7020 - val_loss: 1.0254 - val_accuracy: 0.6957\n",
            "Epoch 1222/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0178 - accuracy: 0.7020 - val_loss: 1.0250 - val_accuracy: 0.6961\n",
            "Epoch 1223/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0174 - accuracy: 0.7020 - val_loss: 1.0246 - val_accuracy: 0.6963\n",
            "Epoch 1224/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0170 - accuracy: 0.7021 - val_loss: 1.0242 - val_accuracy: 0.6965\n",
            "Epoch 1225/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0166 - accuracy: 0.7021 - val_loss: 1.0239 - val_accuracy: 0.6966\n",
            "Epoch 1226/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0163 - accuracy: 0.7022 - val_loss: 1.0235 - val_accuracy: 0.6966\n",
            "Epoch 1227/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0159 - accuracy: 0.7023 - val_loss: 1.0231 - val_accuracy: 0.6965\n",
            "Epoch 1228/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0155 - accuracy: 0.7024 - val_loss: 1.0227 - val_accuracy: 0.6965\n",
            "Epoch 1229/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0151 - accuracy: 0.7024 - val_loss: 1.0224 - val_accuracy: 0.6965\n",
            "Epoch 1230/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0147 - accuracy: 0.7024 - val_loss: 1.0220 - val_accuracy: 0.6965\n",
            "Epoch 1231/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0144 - accuracy: 0.7026 - val_loss: 1.0216 - val_accuracy: 0.6967\n",
            "Epoch 1232/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0140 - accuracy: 0.7026 - val_loss: 1.0213 - val_accuracy: 0.6967\n",
            "Epoch 1233/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0136 - accuracy: 0.7026 - val_loss: 1.0209 - val_accuracy: 0.6968\n",
            "Epoch 1234/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0132 - accuracy: 0.7026 - val_loss: 1.0205 - val_accuracy: 0.6969\n",
            "Epoch 1235/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0129 - accuracy: 0.7027 - val_loss: 1.0201 - val_accuracy: 0.6969\n",
            "Epoch 1236/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0125 - accuracy: 0.7028 - val_loss: 1.0198 - val_accuracy: 0.6969\n",
            "Epoch 1237/5000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0121 - accuracy: 0.7028 - val_loss: 1.0194 - val_accuracy: 0.6969\n",
            "Epoch 1238/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 1.0117 - accuracy: 0.7029 - val_loss: 1.0190 - val_accuracy: 0.6970\n",
            "Epoch 1239/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0114 - accuracy: 0.7029 - val_loss: 1.0187 - val_accuracy: 0.6970\n",
            "Epoch 1240/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0110 - accuracy: 0.7030 - val_loss: 1.0183 - val_accuracy: 0.6971\n",
            "Epoch 1241/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0106 - accuracy: 0.7031 - val_loss: 1.0179 - val_accuracy: 0.6973\n",
            "Epoch 1242/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.0103 - accuracy: 0.7031 - val_loss: 1.0176 - val_accuracy: 0.6972\n",
            "Epoch 1243/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0099 - accuracy: 0.7032 - val_loss: 1.0172 - val_accuracy: 0.6972\n",
            "Epoch 1244/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0095 - accuracy: 0.7032 - val_loss: 1.0168 - val_accuracy: 0.6973\n",
            "Epoch 1245/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0091 - accuracy: 0.7033 - val_loss: 1.0165 - val_accuracy: 0.6975\n",
            "Epoch 1246/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.0088 - accuracy: 0.7035 - val_loss: 1.0161 - val_accuracy: 0.6976\n",
            "Epoch 1247/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0084 - accuracy: 0.7035 - val_loss: 1.0157 - val_accuracy: 0.6977\n",
            "Epoch 1248/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.0080 - accuracy: 0.7036 - val_loss: 1.0154 - val_accuracy: 0.6978\n",
            "Epoch 1249/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0077 - accuracy: 0.7036 - val_loss: 1.0150 - val_accuracy: 0.6978\n",
            "Epoch 1250/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0073 - accuracy: 0.7036 - val_loss: 1.0147 - val_accuracy: 0.6980\n",
            "Epoch 1251/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 1.0069 - accuracy: 0.7037 - val_loss: 1.0143 - val_accuracy: 0.6980\n",
            "Epoch 1252/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 1.0066 - accuracy: 0.7038 - val_loss: 1.0139 - val_accuracy: 0.6980\n",
            "Epoch 1253/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0062 - accuracy: 0.7038 - val_loss: 1.0136 - val_accuracy: 0.6981\n",
            "Epoch 1254/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0058 - accuracy: 0.7040 - val_loss: 1.0132 - val_accuracy: 0.6981\n",
            "Epoch 1255/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0055 - accuracy: 0.7040 - val_loss: 1.0128 - val_accuracy: 0.6982\n",
            "Epoch 1256/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 1.0051 - accuracy: 0.7040 - val_loss: 1.0125 - val_accuracy: 0.6984\n",
            "Epoch 1257/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0047 - accuracy: 0.7042 - val_loss: 1.0121 - val_accuracy: 0.6986\n",
            "Epoch 1258/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0044 - accuracy: 0.7042 - val_loss: 1.0118 - val_accuracy: 0.6987\n",
            "Epoch 1259/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.0040 - accuracy: 0.7043 - val_loss: 1.0114 - val_accuracy: 0.6987\n",
            "Epoch 1260/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0036 - accuracy: 0.7044 - val_loss: 1.0111 - val_accuracy: 0.6987\n",
            "Epoch 1261/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 1.0033 - accuracy: 0.7046 - val_loss: 1.0107 - val_accuracy: 0.6988\n",
            "Epoch 1262/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0029 - accuracy: 0.7046 - val_loss: 1.0103 - val_accuracy: 0.6988\n",
            "Epoch 1263/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.0026 - accuracy: 0.7047 - val_loss: 1.0100 - val_accuracy: 0.6990\n",
            "Epoch 1264/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0022 - accuracy: 0.7047 - val_loss: 1.0096 - val_accuracy: 0.6989\n",
            "Epoch 1265/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 1.0018 - accuracy: 0.7048 - val_loss: 1.0093 - val_accuracy: 0.6990\n",
            "Epoch 1266/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.0015 - accuracy: 0.7048 - val_loss: 1.0089 - val_accuracy: 0.6987\n",
            "Epoch 1267/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.0011 - accuracy: 0.7049 - val_loss: 1.0086 - val_accuracy: 0.6987\n",
            "Epoch 1268/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 1.0007 - accuracy: 0.7050 - val_loss: 1.0082 - val_accuracy: 0.6988\n",
            "Epoch 1269/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 1.0004 - accuracy: 0.7052 - val_loss: 1.0078 - val_accuracy: 0.6989\n",
            "Epoch 1270/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0000 - accuracy: 0.7052 - val_loss: 1.0075 - val_accuracy: 0.6989\n",
            "Epoch 1271/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9997 - accuracy: 0.7053 - val_loss: 1.0071 - val_accuracy: 0.6993\n",
            "Epoch 1272/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9993 - accuracy: 0.7054 - val_loss: 1.0068 - val_accuracy: 0.6994\n",
            "Epoch 1273/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9990 - accuracy: 0.7054 - val_loss: 1.0064 - val_accuracy: 0.6994\n",
            "Epoch 1274/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9986 - accuracy: 0.7055 - val_loss: 1.0061 - val_accuracy: 0.6995\n",
            "Epoch 1275/5000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.9982 - accuracy: 0.7056 - val_loss: 1.0057 - val_accuracy: 0.6995\n",
            "Epoch 1276/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.9979 - accuracy: 0.7057 - val_loss: 1.0054 - val_accuracy: 0.6996\n",
            "Epoch 1277/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.9975 - accuracy: 0.7057 - val_loss: 1.0050 - val_accuracy: 0.6995\n",
            "Epoch 1278/5000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.9972 - accuracy: 0.7058 - val_loss: 1.0047 - val_accuracy: 0.6996\n",
            "Epoch 1279/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9968 - accuracy: 0.7059 - val_loss: 1.0043 - val_accuracy: 0.6998\n",
            "Epoch 1280/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9965 - accuracy: 0.7059 - val_loss: 1.0040 - val_accuracy: 0.6999\n",
            "Epoch 1281/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.9961 - accuracy: 0.7060 - val_loss: 1.0036 - val_accuracy: 0.7000\n",
            "Epoch 1282/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9958 - accuracy: 0.7060 - val_loss: 1.0033 - val_accuracy: 0.7000\n",
            "Epoch 1283/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9954 - accuracy: 0.7060 - val_loss: 1.0029 - val_accuracy: 0.7000\n",
            "Epoch 1284/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9950 - accuracy: 0.7061 - val_loss: 1.0026 - val_accuracy: 0.6999\n",
            "Epoch 1285/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9947 - accuracy: 0.7061 - val_loss: 1.0022 - val_accuracy: 0.6997\n",
            "Epoch 1286/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9943 - accuracy: 0.7062 - val_loss: 1.0019 - val_accuracy: 0.6998\n",
            "Epoch 1287/5000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.9940 - accuracy: 0.7062 - val_loss: 1.0015 - val_accuracy: 0.6997\n",
            "Epoch 1288/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9936 - accuracy: 0.7063 - val_loss: 1.0012 - val_accuracy: 0.6997\n",
            "Epoch 1289/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9933 - accuracy: 0.7064 - val_loss: 1.0009 - val_accuracy: 0.6997\n",
            "Epoch 1290/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.9929 - accuracy: 0.7064 - val_loss: 1.0005 - val_accuracy: 0.6997\n",
            "Epoch 1291/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9926 - accuracy: 0.7064 - val_loss: 1.0002 - val_accuracy: 0.6998\n",
            "Epoch 1292/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9922 - accuracy: 0.7065 - val_loss: 0.9998 - val_accuracy: 0.6999\n",
            "Epoch 1293/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9919 - accuracy: 0.7066 - val_loss: 0.9995 - val_accuracy: 0.6999\n",
            "Epoch 1294/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9915 - accuracy: 0.7066 - val_loss: 0.9991 - val_accuracy: 0.6999\n",
            "Epoch 1295/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9912 - accuracy: 0.7066 - val_loss: 0.9988 - val_accuracy: 0.6999\n",
            "Epoch 1296/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9908 - accuracy: 0.7067 - val_loss: 0.9984 - val_accuracy: 0.7001\n",
            "Epoch 1297/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9905 - accuracy: 0.7068 - val_loss: 0.9981 - val_accuracy: 0.7001\n",
            "Epoch 1298/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9901 - accuracy: 0.7069 - val_loss: 0.9978 - val_accuracy: 0.7002\n",
            "Epoch 1299/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9898 - accuracy: 0.7069 - val_loss: 0.9974 - val_accuracy: 0.7001\n",
            "Epoch 1300/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9895 - accuracy: 0.7069 - val_loss: 0.9971 - val_accuracy: 0.7002\n",
            "Epoch 1301/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9891 - accuracy: 0.7069 - val_loss: 0.9967 - val_accuracy: 0.7002\n",
            "Epoch 1302/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9888 - accuracy: 0.7070 - val_loss: 0.9964 - val_accuracy: 0.7004\n",
            "Epoch 1303/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9884 - accuracy: 0.7070 - val_loss: 0.9961 - val_accuracy: 0.7005\n",
            "Epoch 1304/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9881 - accuracy: 0.7070 - val_loss: 0.9957 - val_accuracy: 0.7005\n",
            "Epoch 1305/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.9877 - accuracy: 0.7071 - val_loss: 0.9954 - val_accuracy: 0.7005\n",
            "Epoch 1306/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9874 - accuracy: 0.7072 - val_loss: 0.9950 - val_accuracy: 0.7008\n",
            "Epoch 1307/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.9870 - accuracy: 0.7073 - val_loss: 0.9947 - val_accuracy: 0.7008\n",
            "Epoch 1308/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9867 - accuracy: 0.7075 - val_loss: 0.9944 - val_accuracy: 0.7008\n",
            "Epoch 1309/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9864 - accuracy: 0.7075 - val_loss: 0.9940 - val_accuracy: 0.7007\n",
            "Epoch 1310/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9860 - accuracy: 0.7077 - val_loss: 0.9937 - val_accuracy: 0.7006\n",
            "Epoch 1311/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9857 - accuracy: 0.7078 - val_loss: 0.9934 - val_accuracy: 0.7006\n",
            "Epoch 1312/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9853 - accuracy: 0.7078 - val_loss: 0.9930 - val_accuracy: 0.7007\n",
            "Epoch 1313/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9850 - accuracy: 0.7079 - val_loss: 0.9927 - val_accuracy: 0.7008\n",
            "Epoch 1314/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9846 - accuracy: 0.7079 - val_loss: 0.9923 - val_accuracy: 0.7008\n",
            "Epoch 1315/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9843 - accuracy: 0.7080 - val_loss: 0.9920 - val_accuracy: 0.7007\n",
            "Epoch 1316/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9840 - accuracy: 0.7081 - val_loss: 0.9917 - val_accuracy: 0.7009\n",
            "Epoch 1317/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9836 - accuracy: 0.7081 - val_loss: 0.9913 - val_accuracy: 0.7010\n",
            "Epoch 1318/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9833 - accuracy: 0.7082 - val_loss: 0.9910 - val_accuracy: 0.7010\n",
            "Epoch 1319/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9830 - accuracy: 0.7083 - val_loss: 0.9907 - val_accuracy: 0.7012\n",
            "Epoch 1320/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.9826 - accuracy: 0.7084 - val_loss: 0.9903 - val_accuracy: 0.7013\n",
            "Epoch 1321/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9823 - accuracy: 0.7084 - val_loss: 0.9900 - val_accuracy: 0.7014\n",
            "Epoch 1322/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9819 - accuracy: 0.7085 - val_loss: 0.9897 - val_accuracy: 0.7014\n",
            "Epoch 1323/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9816 - accuracy: 0.7087 - val_loss: 0.9893 - val_accuracy: 0.7013\n",
            "Epoch 1324/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9813 - accuracy: 0.7087 - val_loss: 0.9890 - val_accuracy: 0.7013\n",
            "Epoch 1325/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9809 - accuracy: 0.7088 - val_loss: 0.9887 - val_accuracy: 0.7013\n",
            "Epoch 1326/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.9806 - accuracy: 0.7088 - val_loss: 0.9884 - val_accuracy: 0.7014\n",
            "Epoch 1327/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9803 - accuracy: 0.7089 - val_loss: 0.9880 - val_accuracy: 0.7015\n",
            "Epoch 1328/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9799 - accuracy: 0.7090 - val_loss: 0.9877 - val_accuracy: 0.7017\n",
            "Epoch 1329/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9796 - accuracy: 0.7090 - val_loss: 0.9874 - val_accuracy: 0.7017\n",
            "Epoch 1330/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9793 - accuracy: 0.7091 - val_loss: 0.9870 - val_accuracy: 0.7018\n",
            "Epoch 1331/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9789 - accuracy: 0.7091 - val_loss: 0.9867 - val_accuracy: 0.7019\n",
            "Epoch 1332/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9786 - accuracy: 0.7092 - val_loss: 0.9864 - val_accuracy: 0.7019\n",
            "Epoch 1333/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.9783 - accuracy: 0.7093 - val_loss: 0.9861 - val_accuracy: 0.7019\n",
            "Epoch 1334/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9779 - accuracy: 0.7093 - val_loss: 0.9857 - val_accuracy: 0.7019\n",
            "Epoch 1335/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9776 - accuracy: 0.7094 - val_loss: 0.9854 - val_accuracy: 0.7018\n",
            "Epoch 1336/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9773 - accuracy: 0.7094 - val_loss: 0.9851 - val_accuracy: 0.7018\n",
            "Epoch 1337/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9769 - accuracy: 0.7094 - val_loss: 0.9847 - val_accuracy: 0.7020\n",
            "Epoch 1338/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9766 - accuracy: 0.7095 - val_loss: 0.9844 - val_accuracy: 0.7020\n",
            "Epoch 1339/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.9763 - accuracy: 0.7095 - val_loss: 0.9841 - val_accuracy: 0.7020\n",
            "Epoch 1340/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9759 - accuracy: 0.7096 - val_loss: 0.9838 - val_accuracy: 0.7023\n",
            "Epoch 1341/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9756 - accuracy: 0.7096 - val_loss: 0.9834 - val_accuracy: 0.7023\n",
            "Epoch 1342/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9753 - accuracy: 0.7096 - val_loss: 0.9831 - val_accuracy: 0.7023\n",
            "Epoch 1343/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9749 - accuracy: 0.7097 - val_loss: 0.9828 - val_accuracy: 0.7024\n",
            "Epoch 1344/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9746 - accuracy: 0.7098 - val_loss: 0.9825 - val_accuracy: 0.7024\n",
            "Epoch 1345/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9743 - accuracy: 0.7099 - val_loss: 0.9821 - val_accuracy: 0.7026\n",
            "Epoch 1346/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9740 - accuracy: 0.7099 - val_loss: 0.9818 - val_accuracy: 0.7028\n",
            "Epoch 1347/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9736 - accuracy: 0.7099 - val_loss: 0.9815 - val_accuracy: 0.7028\n",
            "Epoch 1348/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9733 - accuracy: 0.7099 - val_loss: 0.9812 - val_accuracy: 0.7029\n",
            "Epoch 1349/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9730 - accuracy: 0.7100 - val_loss: 0.9809 - val_accuracy: 0.7029\n",
            "Epoch 1350/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9727 - accuracy: 0.7101 - val_loss: 0.9805 - val_accuracy: 0.7029\n",
            "Epoch 1351/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9723 - accuracy: 0.7102 - val_loss: 0.9802 - val_accuracy: 0.7029\n",
            "Epoch 1352/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9720 - accuracy: 0.7102 - val_loss: 0.9799 - val_accuracy: 0.7029\n",
            "Epoch 1353/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9717 - accuracy: 0.7102 - val_loss: 0.9796 - val_accuracy: 0.7029\n",
            "Epoch 1354/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.9713 - accuracy: 0.7103 - val_loss: 0.9793 - val_accuracy: 0.7028\n",
            "Epoch 1355/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9710 - accuracy: 0.7103 - val_loss: 0.9789 - val_accuracy: 0.7029\n",
            "Epoch 1356/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9707 - accuracy: 0.7104 - val_loss: 0.9786 - val_accuracy: 0.7030\n",
            "Epoch 1357/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9704 - accuracy: 0.7104 - val_loss: 0.9783 - val_accuracy: 0.7031\n",
            "Epoch 1358/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9701 - accuracy: 0.7105 - val_loss: 0.9780 - val_accuracy: 0.7031\n",
            "Epoch 1359/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9697 - accuracy: 0.7106 - val_loss: 0.9777 - val_accuracy: 0.7031\n",
            "Epoch 1360/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9694 - accuracy: 0.7106 - val_loss: 0.9773 - val_accuracy: 0.7031\n",
            "Epoch 1361/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9691 - accuracy: 0.7106 - val_loss: 0.9770 - val_accuracy: 0.7032\n",
            "Epoch 1362/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9688 - accuracy: 0.7106 - val_loss: 0.9767 - val_accuracy: 0.7032\n",
            "Epoch 1363/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.9684 - accuracy: 0.7107 - val_loss: 0.9764 - val_accuracy: 0.7032\n",
            "Epoch 1364/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9681 - accuracy: 0.7107 - val_loss: 0.9761 - val_accuracy: 0.7033\n",
            "Epoch 1365/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9678 - accuracy: 0.7108 - val_loss: 0.9758 - val_accuracy: 0.7033\n",
            "Epoch 1366/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9675 - accuracy: 0.7109 - val_loss: 0.9754 - val_accuracy: 0.7034\n",
            "Epoch 1367/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.9672 - accuracy: 0.7110 - val_loss: 0.9751 - val_accuracy: 0.7034\n",
            "Epoch 1368/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9668 - accuracy: 0.7110 - val_loss: 0.9748 - val_accuracy: 0.7034\n",
            "Epoch 1369/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9665 - accuracy: 0.7110 - val_loss: 0.9745 - val_accuracy: 0.7035\n",
            "Epoch 1370/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9662 - accuracy: 0.7110 - val_loss: 0.9742 - val_accuracy: 0.7035\n",
            "Epoch 1371/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9659 - accuracy: 0.7110 - val_loss: 0.9739 - val_accuracy: 0.7036\n",
            "Epoch 1372/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9656 - accuracy: 0.7112 - val_loss: 0.9736 - val_accuracy: 0.7036\n",
            "Epoch 1373/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9652 - accuracy: 0.7112 - val_loss: 0.9733 - val_accuracy: 0.7035\n",
            "Epoch 1374/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9649 - accuracy: 0.7114 - val_loss: 0.9729 - val_accuracy: 0.7035\n",
            "Epoch 1375/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9646 - accuracy: 0.7114 - val_loss: 0.9726 - val_accuracy: 0.7036\n",
            "Epoch 1376/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.9643 - accuracy: 0.7115 - val_loss: 0.9723 - val_accuracy: 0.7036\n",
            "Epoch 1377/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9640 - accuracy: 0.7116 - val_loss: 0.9720 - val_accuracy: 0.7035\n",
            "Epoch 1378/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9637 - accuracy: 0.7116 - val_loss: 0.9717 - val_accuracy: 0.7035\n",
            "Epoch 1379/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9634 - accuracy: 0.7117 - val_loss: 0.9714 - val_accuracy: 0.7035\n",
            "Epoch 1380/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.9630 - accuracy: 0.7118 - val_loss: 0.9711 - val_accuracy: 0.7036\n",
            "Epoch 1381/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9627 - accuracy: 0.7118 - val_loss: 0.9708 - val_accuracy: 0.7037\n",
            "Epoch 1382/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9624 - accuracy: 0.7118 - val_loss: 0.9705 - val_accuracy: 0.7038\n",
            "Epoch 1383/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9621 - accuracy: 0.7118 - val_loss: 0.9701 - val_accuracy: 0.7038\n",
            "Epoch 1384/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9618 - accuracy: 0.7119 - val_loss: 0.9698 - val_accuracy: 0.7037\n",
            "Epoch 1385/5000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.9615 - accuracy: 0.7120 - val_loss: 0.9695 - val_accuracy: 0.7037\n",
            "Epoch 1386/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.9612 - accuracy: 0.7121 - val_loss: 0.9692 - val_accuracy: 0.7038\n",
            "Epoch 1387/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9608 - accuracy: 0.7122 - val_loss: 0.9689 - val_accuracy: 0.7038\n",
            "Epoch 1388/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9605 - accuracy: 0.7122 - val_loss: 0.9686 - val_accuracy: 0.7038\n",
            "Epoch 1389/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.9602 - accuracy: 0.7123 - val_loss: 0.9683 - val_accuracy: 0.7038\n",
            "Epoch 1390/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9599 - accuracy: 0.7122 - val_loss: 0.9680 - val_accuracy: 0.7037\n",
            "Epoch 1391/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9596 - accuracy: 0.7123 - val_loss: 0.9677 - val_accuracy: 0.7039\n",
            "Epoch 1392/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.9593 - accuracy: 0.7124 - val_loss: 0.9674 - val_accuracy: 0.7040\n",
            "Epoch 1393/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9590 - accuracy: 0.7125 - val_loss: 0.9671 - val_accuracy: 0.7040\n",
            "Epoch 1394/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9587 - accuracy: 0.7126 - val_loss: 0.9668 - val_accuracy: 0.7040\n",
            "Epoch 1395/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9584 - accuracy: 0.7127 - val_loss: 0.9665 - val_accuracy: 0.7042\n",
            "Epoch 1396/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9580 - accuracy: 0.7127 - val_loss: 0.9662 - val_accuracy: 0.7041\n",
            "Epoch 1397/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9577 - accuracy: 0.7128 - val_loss: 0.9659 - val_accuracy: 0.7040\n",
            "Epoch 1398/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9574 - accuracy: 0.7129 - val_loss: 0.9656 - val_accuracy: 0.7041\n",
            "Epoch 1399/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9571 - accuracy: 0.7129 - val_loss: 0.9653 - val_accuracy: 0.7041\n",
            "Epoch 1400/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9568 - accuracy: 0.7130 - val_loss: 0.9649 - val_accuracy: 0.7041\n",
            "Epoch 1401/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9565 - accuracy: 0.7130 - val_loss: 0.9646 - val_accuracy: 0.7042\n",
            "Epoch 1402/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9562 - accuracy: 0.7131 - val_loss: 0.9643 - val_accuracy: 0.7042\n",
            "Epoch 1403/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9559 - accuracy: 0.7132 - val_loss: 0.9640 - val_accuracy: 0.7042\n",
            "Epoch 1404/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9556 - accuracy: 0.7132 - val_loss: 0.9637 - val_accuracy: 0.7041\n",
            "Epoch 1405/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9553 - accuracy: 0.7133 - val_loss: 0.9634 - val_accuracy: 0.7041\n",
            "Epoch 1406/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9550 - accuracy: 0.7133 - val_loss: 0.9631 - val_accuracy: 0.7041\n",
            "Epoch 1407/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9547 - accuracy: 0.7133 - val_loss: 0.9628 - val_accuracy: 0.7044\n",
            "Epoch 1408/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9544 - accuracy: 0.7135 - val_loss: 0.9625 - val_accuracy: 0.7047\n",
            "Epoch 1409/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9541 - accuracy: 0.7135 - val_loss: 0.9622 - val_accuracy: 0.7048\n",
            "Epoch 1410/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9538 - accuracy: 0.7136 - val_loss: 0.9619 - val_accuracy: 0.7048\n",
            "Epoch 1411/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9535 - accuracy: 0.7136 - val_loss: 0.9616 - val_accuracy: 0.7049\n",
            "Epoch 1412/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.9531 - accuracy: 0.7137 - val_loss: 0.9613 - val_accuracy: 0.7051\n",
            "Epoch 1413/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9528 - accuracy: 0.7137 - val_loss: 0.9610 - val_accuracy: 0.7051\n",
            "Epoch 1414/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9525 - accuracy: 0.7138 - val_loss: 0.9607 - val_accuracy: 0.7051\n",
            "Epoch 1415/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9522 - accuracy: 0.7139 - val_loss: 0.9604 - val_accuracy: 0.7052\n",
            "Epoch 1416/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9519 - accuracy: 0.7139 - val_loss: 0.9602 - val_accuracy: 0.7053\n",
            "Epoch 1417/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9516 - accuracy: 0.7139 - val_loss: 0.9599 - val_accuracy: 0.7054\n",
            "Epoch 1418/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9513 - accuracy: 0.7140 - val_loss: 0.9596 - val_accuracy: 0.7055\n",
            "Epoch 1419/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9510 - accuracy: 0.7141 - val_loss: 0.9593 - val_accuracy: 0.7056\n",
            "Epoch 1420/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9507 - accuracy: 0.7142 - val_loss: 0.9590 - val_accuracy: 0.7056\n",
            "Epoch 1421/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9504 - accuracy: 0.7141 - val_loss: 0.9587 - val_accuracy: 0.7058\n",
            "Epoch 1422/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9501 - accuracy: 0.7142 - val_loss: 0.9584 - val_accuracy: 0.7058\n",
            "Epoch 1423/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9498 - accuracy: 0.7142 - val_loss: 0.9581 - val_accuracy: 0.7059\n",
            "Epoch 1424/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9495 - accuracy: 0.7143 - val_loss: 0.9578 - val_accuracy: 0.7061\n",
            "Epoch 1425/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9492 - accuracy: 0.7143 - val_loss: 0.9575 - val_accuracy: 0.7059\n",
            "Epoch 1426/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9489 - accuracy: 0.7143 - val_loss: 0.9572 - val_accuracy: 0.7060\n",
            "Epoch 1427/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9486 - accuracy: 0.7144 - val_loss: 0.9569 - val_accuracy: 0.7060\n",
            "Epoch 1428/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9483 - accuracy: 0.7144 - val_loss: 0.9566 - val_accuracy: 0.7061\n",
            "Epoch 1429/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9480 - accuracy: 0.7145 - val_loss: 0.9563 - val_accuracy: 0.7064\n",
            "Epoch 1430/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9477 - accuracy: 0.7145 - val_loss: 0.9560 - val_accuracy: 0.7064\n",
            "Epoch 1431/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9474 - accuracy: 0.7145 - val_loss: 0.9557 - val_accuracy: 0.7064\n",
            "Epoch 1432/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9472 - accuracy: 0.7146 - val_loss: 0.9554 - val_accuracy: 0.7065\n",
            "Epoch 1433/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9469 - accuracy: 0.7146 - val_loss: 0.9552 - val_accuracy: 0.7065\n",
            "Epoch 1434/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.9466 - accuracy: 0.7146 - val_loss: 0.9549 - val_accuracy: 0.7065\n",
            "Epoch 1435/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9463 - accuracy: 0.7147 - val_loss: 0.9546 - val_accuracy: 0.7065\n",
            "Epoch 1436/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9460 - accuracy: 0.7147 - val_loss: 0.9543 - val_accuracy: 0.7067\n",
            "Epoch 1437/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.9457 - accuracy: 0.7148 - val_loss: 0.9540 - val_accuracy: 0.7071\n",
            "Epoch 1438/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9454 - accuracy: 0.7150 - val_loss: 0.9537 - val_accuracy: 0.7071\n",
            "Epoch 1439/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9451 - accuracy: 0.7151 - val_loss: 0.9534 - val_accuracy: 0.7071\n",
            "Epoch 1440/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9448 - accuracy: 0.7151 - val_loss: 0.9531 - val_accuracy: 0.7072\n",
            "Epoch 1441/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9445 - accuracy: 0.7151 - val_loss: 0.9528 - val_accuracy: 0.7072\n",
            "Epoch 1442/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9442 - accuracy: 0.7151 - val_loss: 0.9525 - val_accuracy: 0.7072\n",
            "Epoch 1443/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9439 - accuracy: 0.7152 - val_loss: 0.9523 - val_accuracy: 0.7073\n",
            "Epoch 1444/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9436 - accuracy: 0.7152 - val_loss: 0.9520 - val_accuracy: 0.7074\n",
            "Epoch 1445/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.9433 - accuracy: 0.7154 - val_loss: 0.9517 - val_accuracy: 0.7073\n",
            "Epoch 1446/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9430 - accuracy: 0.7154 - val_loss: 0.9514 - val_accuracy: 0.7073\n",
            "Epoch 1447/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.9427 - accuracy: 0.7155 - val_loss: 0.9511 - val_accuracy: 0.7072\n",
            "Epoch 1448/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9425 - accuracy: 0.7156 - val_loss: 0.9508 - val_accuracy: 0.7073\n",
            "Epoch 1449/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9422 - accuracy: 0.7156 - val_loss: 0.9505 - val_accuracy: 0.7073\n",
            "Epoch 1450/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9419 - accuracy: 0.7157 - val_loss: 0.9502 - val_accuracy: 0.7074\n",
            "Epoch 1451/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9416 - accuracy: 0.7158 - val_loss: 0.9500 - val_accuracy: 0.7075\n",
            "Epoch 1452/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9413 - accuracy: 0.7158 - val_loss: 0.9497 - val_accuracy: 0.7076\n",
            "Epoch 1453/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9410 - accuracy: 0.7159 - val_loss: 0.9494 - val_accuracy: 0.7076\n",
            "Epoch 1454/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.9407 - accuracy: 0.7159 - val_loss: 0.9491 - val_accuracy: 0.7076\n",
            "Epoch 1455/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9404 - accuracy: 0.7160 - val_loss: 0.9488 - val_accuracy: 0.7076\n",
            "Epoch 1456/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9401 - accuracy: 0.7161 - val_loss: 0.9485 - val_accuracy: 0.7077\n",
            "Epoch 1457/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9398 - accuracy: 0.7161 - val_loss: 0.9483 - val_accuracy: 0.7078\n",
            "Epoch 1458/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9396 - accuracy: 0.7162 - val_loss: 0.9480 - val_accuracy: 0.7079\n",
            "Epoch 1459/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9393 - accuracy: 0.7162 - val_loss: 0.9477 - val_accuracy: 0.7080\n",
            "Epoch 1460/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9390 - accuracy: 0.7163 - val_loss: 0.9474 - val_accuracy: 0.7082\n",
            "Epoch 1461/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9387 - accuracy: 0.7164 - val_loss: 0.9471 - val_accuracy: 0.7082\n",
            "Epoch 1462/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9384 - accuracy: 0.7165 - val_loss: 0.9468 - val_accuracy: 0.7083\n",
            "Epoch 1463/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9381 - accuracy: 0.7165 - val_loss: 0.9466 - val_accuracy: 0.7083\n",
            "Epoch 1464/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9378 - accuracy: 0.7166 - val_loss: 0.9463 - val_accuracy: 0.7083\n",
            "Epoch 1465/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9376 - accuracy: 0.7166 - val_loss: 0.9460 - val_accuracy: 0.7083\n",
            "Epoch 1466/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9373 - accuracy: 0.7166 - val_loss: 0.9457 - val_accuracy: 0.7084\n",
            "Epoch 1467/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9370 - accuracy: 0.7167 - val_loss: 0.9454 - val_accuracy: 0.7085\n",
            "Epoch 1468/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9367 - accuracy: 0.7168 - val_loss: 0.9452 - val_accuracy: 0.7085\n",
            "Epoch 1469/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9364 - accuracy: 0.7168 - val_loss: 0.9449 - val_accuracy: 0.7086\n",
            "Epoch 1470/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9361 - accuracy: 0.7168 - val_loss: 0.9446 - val_accuracy: 0.7087\n",
            "Epoch 1471/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9358 - accuracy: 0.7168 - val_loss: 0.9443 - val_accuracy: 0.7087\n",
            "Epoch 1472/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9356 - accuracy: 0.7169 - val_loss: 0.9440 - val_accuracy: 0.7087\n",
            "Epoch 1473/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.9353 - accuracy: 0.7170 - val_loss: 0.9438 - val_accuracy: 0.7087\n",
            "Epoch 1474/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9350 - accuracy: 0.7170 - val_loss: 0.9435 - val_accuracy: 0.7090\n",
            "Epoch 1475/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9347 - accuracy: 0.7170 - val_loss: 0.9432 - val_accuracy: 0.7090\n",
            "Epoch 1476/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9344 - accuracy: 0.7171 - val_loss: 0.9429 - val_accuracy: 0.7090\n",
            "Epoch 1477/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9341 - accuracy: 0.7171 - val_loss: 0.9427 - val_accuracy: 0.7091\n",
            "Epoch 1478/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9339 - accuracy: 0.7171 - val_loss: 0.9424 - val_accuracy: 0.7091\n",
            "Epoch 1479/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9336 - accuracy: 0.7171 - val_loss: 0.9421 - val_accuracy: 0.7092\n",
            "Epoch 1480/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9333 - accuracy: 0.7172 - val_loss: 0.9418 - val_accuracy: 0.7092\n",
            "Epoch 1481/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9330 - accuracy: 0.7172 - val_loss: 0.9416 - val_accuracy: 0.7092\n",
            "Epoch 1482/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9327 - accuracy: 0.7172 - val_loss: 0.9413 - val_accuracy: 0.7094\n",
            "Epoch 1483/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9325 - accuracy: 0.7173 - val_loss: 0.9410 - val_accuracy: 0.7094\n",
            "Epoch 1484/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9322 - accuracy: 0.7174 - val_loss: 0.9407 - val_accuracy: 0.7094\n",
            "Epoch 1485/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9319 - accuracy: 0.7175 - val_loss: 0.9405 - val_accuracy: 0.7094\n",
            "Epoch 1486/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9316 - accuracy: 0.7175 - val_loss: 0.9402 - val_accuracy: 0.7095\n",
            "Epoch 1487/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9313 - accuracy: 0.7175 - val_loss: 0.9399 - val_accuracy: 0.7096\n",
            "Epoch 1488/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9311 - accuracy: 0.7176 - val_loss: 0.9396 - val_accuracy: 0.7096\n",
            "Epoch 1489/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9308 - accuracy: 0.7177 - val_loss: 0.9394 - val_accuracy: 0.7097\n",
            "Epoch 1490/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9305 - accuracy: 0.7178 - val_loss: 0.9391 - val_accuracy: 0.7100\n",
            "Epoch 1491/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.9302 - accuracy: 0.7179 - val_loss: 0.9388 - val_accuracy: 0.7100\n",
            "Epoch 1492/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9300 - accuracy: 0.7179 - val_loss: 0.9385 - val_accuracy: 0.7102\n",
            "Epoch 1493/5000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.9297 - accuracy: 0.7179 - val_loss: 0.9383 - val_accuracy: 0.7102\n",
            "Epoch 1494/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9294 - accuracy: 0.7180 - val_loss: 0.9380 - val_accuracy: 0.7103\n",
            "Epoch 1495/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9291 - accuracy: 0.7181 - val_loss: 0.9377 - val_accuracy: 0.7102\n",
            "Epoch 1496/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9289 - accuracy: 0.7181 - val_loss: 0.9375 - val_accuracy: 0.7102\n",
            "Epoch 1497/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9286 - accuracy: 0.7181 - val_loss: 0.9372 - val_accuracy: 0.7103\n",
            "Epoch 1498/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9283 - accuracy: 0.7182 - val_loss: 0.9369 - val_accuracy: 0.7103\n",
            "Epoch 1499/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9280 - accuracy: 0.7182 - val_loss: 0.9366 - val_accuracy: 0.7103\n",
            "Epoch 1500/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9278 - accuracy: 0.7183 - val_loss: 0.9364 - val_accuracy: 0.7105\n",
            "Epoch 1501/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9275 - accuracy: 0.7183 - val_loss: 0.9361 - val_accuracy: 0.7107\n",
            "Epoch 1502/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9272 - accuracy: 0.7184 - val_loss: 0.9358 - val_accuracy: 0.7107\n",
            "Epoch 1503/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9269 - accuracy: 0.7185 - val_loss: 0.9356 - val_accuracy: 0.7106\n",
            "Epoch 1504/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.9267 - accuracy: 0.7185 - val_loss: 0.9353 - val_accuracy: 0.7107\n",
            "Epoch 1505/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9264 - accuracy: 0.7186 - val_loss: 0.9350 - val_accuracy: 0.7106\n",
            "Epoch 1506/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9261 - accuracy: 0.7186 - val_loss: 0.9348 - val_accuracy: 0.7108\n",
            "Epoch 1507/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9258 - accuracy: 0.7187 - val_loss: 0.9345 - val_accuracy: 0.7108\n",
            "Epoch 1508/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9256 - accuracy: 0.7187 - val_loss: 0.9342 - val_accuracy: 0.7107\n",
            "Epoch 1509/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9253 - accuracy: 0.7188 - val_loss: 0.9340 - val_accuracy: 0.7108\n",
            "Epoch 1510/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.9250 - accuracy: 0.7188 - val_loss: 0.9337 - val_accuracy: 0.7110\n",
            "Epoch 1511/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9248 - accuracy: 0.7188 - val_loss: 0.9334 - val_accuracy: 0.7112\n",
            "Epoch 1512/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9245 - accuracy: 0.7189 - val_loss: 0.9332 - val_accuracy: 0.7112\n",
            "Epoch 1513/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9242 - accuracy: 0.7190 - val_loss: 0.9329 - val_accuracy: 0.7112\n",
            "Epoch 1514/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9239 - accuracy: 0.7191 - val_loss: 0.9326 - val_accuracy: 0.7113\n",
            "Epoch 1515/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9237 - accuracy: 0.7192 - val_loss: 0.9324 - val_accuracy: 0.7114\n",
            "Epoch 1516/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9234 - accuracy: 0.7192 - val_loss: 0.9321 - val_accuracy: 0.7114\n",
            "Epoch 1517/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9231 - accuracy: 0.7192 - val_loss: 0.9318 - val_accuracy: 0.7115\n",
            "Epoch 1518/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.9229 - accuracy: 0.7193 - val_loss: 0.9316 - val_accuracy: 0.7115\n",
            "Epoch 1519/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9226 - accuracy: 0.7193 - val_loss: 0.9313 - val_accuracy: 0.7116\n",
            "Epoch 1520/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.9223 - accuracy: 0.7193 - val_loss: 0.9310 - val_accuracy: 0.7117\n",
            "Epoch 1521/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9221 - accuracy: 0.7193 - val_loss: 0.9308 - val_accuracy: 0.7119\n",
            "Epoch 1522/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9218 - accuracy: 0.7194 - val_loss: 0.9305 - val_accuracy: 0.7119\n",
            "Epoch 1523/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9215 - accuracy: 0.7196 - val_loss: 0.9302 - val_accuracy: 0.7119\n",
            "Epoch 1524/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9213 - accuracy: 0.7196 - val_loss: 0.9300 - val_accuracy: 0.7120\n",
            "Epoch 1525/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9210 - accuracy: 0.7196 - val_loss: 0.9297 - val_accuracy: 0.7120\n",
            "Epoch 1526/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9207 - accuracy: 0.7197 - val_loss: 0.9295 - val_accuracy: 0.7121\n",
            "Epoch 1527/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9205 - accuracy: 0.7197 - val_loss: 0.9292 - val_accuracy: 0.7121\n",
            "Epoch 1528/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9202 - accuracy: 0.7197 - val_loss: 0.9289 - val_accuracy: 0.7121\n",
            "Epoch 1529/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.9199 - accuracy: 0.7197 - val_loss: 0.9287 - val_accuracy: 0.7122\n",
            "Epoch 1530/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9197 - accuracy: 0.7197 - val_loss: 0.9284 - val_accuracy: 0.7122\n",
            "Epoch 1531/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9194 - accuracy: 0.7197 - val_loss: 0.9281 - val_accuracy: 0.7121\n",
            "Epoch 1532/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9191 - accuracy: 0.7198 - val_loss: 0.9279 - val_accuracy: 0.7124\n",
            "Epoch 1533/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9189 - accuracy: 0.7199 - val_loss: 0.9276 - val_accuracy: 0.7124\n",
            "Epoch 1534/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9186 - accuracy: 0.7200 - val_loss: 0.9274 - val_accuracy: 0.7124\n",
            "Epoch 1535/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9183 - accuracy: 0.7200 - val_loss: 0.9271 - val_accuracy: 0.7124\n",
            "Epoch 1536/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9181 - accuracy: 0.7201 - val_loss: 0.9269 - val_accuracy: 0.7124\n",
            "Epoch 1537/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.9178 - accuracy: 0.7201 - val_loss: 0.9266 - val_accuracy: 0.7124\n",
            "Epoch 1538/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9175 - accuracy: 0.7202 - val_loss: 0.9263 - val_accuracy: 0.7124\n",
            "Epoch 1539/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9173 - accuracy: 0.7202 - val_loss: 0.9261 - val_accuracy: 0.7125\n",
            "Epoch 1540/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9170 - accuracy: 0.7203 - val_loss: 0.9258 - val_accuracy: 0.7125\n",
            "Epoch 1541/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9168 - accuracy: 0.7203 - val_loss: 0.9256 - val_accuracy: 0.7124\n",
            "Epoch 1542/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9165 - accuracy: 0.7203 - val_loss: 0.9253 - val_accuracy: 0.7124\n",
            "Epoch 1543/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9162 - accuracy: 0.7204 - val_loss: 0.9250 - val_accuracy: 0.7124\n",
            "Epoch 1544/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.9160 - accuracy: 0.7205 - val_loss: 0.9248 - val_accuracy: 0.7124\n",
            "Epoch 1545/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9157 - accuracy: 0.7206 - val_loss: 0.9245 - val_accuracy: 0.7124\n",
            "Epoch 1546/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9154 - accuracy: 0.7206 - val_loss: 0.9243 - val_accuracy: 0.7126\n",
            "Epoch 1547/5000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.9152 - accuracy: 0.7206 - val_loss: 0.9240 - val_accuracy: 0.7127\n",
            "Epoch 1548/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9149 - accuracy: 0.7207 - val_loss: 0.9238 - val_accuracy: 0.7129\n",
            "Epoch 1549/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9147 - accuracy: 0.7207 - val_loss: 0.9235 - val_accuracy: 0.7129\n",
            "Epoch 1550/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9144 - accuracy: 0.7207 - val_loss: 0.9233 - val_accuracy: 0.7129\n",
            "Epoch 1551/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9141 - accuracy: 0.7208 - val_loss: 0.9230 - val_accuracy: 0.7129\n",
            "Epoch 1552/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9139 - accuracy: 0.7209 - val_loss: 0.9227 - val_accuracy: 0.7129\n",
            "Epoch 1553/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9136 - accuracy: 0.7210 - val_loss: 0.9225 - val_accuracy: 0.7129\n",
            "Epoch 1554/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.9134 - accuracy: 0.7210 - val_loss: 0.9222 - val_accuracy: 0.7129\n",
            "Epoch 1555/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9131 - accuracy: 0.7210 - val_loss: 0.9220 - val_accuracy: 0.7129\n",
            "Epoch 1556/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9129 - accuracy: 0.7211 - val_loss: 0.9217 - val_accuracy: 0.7131\n",
            "Epoch 1557/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.9126 - accuracy: 0.7212 - val_loss: 0.9215 - val_accuracy: 0.7131\n",
            "Epoch 1558/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9123 - accuracy: 0.7212 - val_loss: 0.9212 - val_accuracy: 0.7131\n",
            "Epoch 1559/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9121 - accuracy: 0.7213 - val_loss: 0.9210 - val_accuracy: 0.7131\n",
            "Epoch 1560/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.9118 - accuracy: 0.7213 - val_loss: 0.9207 - val_accuracy: 0.7130\n",
            "Epoch 1561/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9116 - accuracy: 0.7214 - val_loss: 0.9205 - val_accuracy: 0.7132\n",
            "Epoch 1562/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9113 - accuracy: 0.7215 - val_loss: 0.9202 - val_accuracy: 0.7133\n",
            "Epoch 1563/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9111 - accuracy: 0.7216 - val_loss: 0.9200 - val_accuracy: 0.7135\n",
            "Epoch 1564/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9108 - accuracy: 0.7215 - val_loss: 0.9197 - val_accuracy: 0.7135\n",
            "Epoch 1565/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9105 - accuracy: 0.7216 - val_loss: 0.9195 - val_accuracy: 0.7137\n",
            "Epoch 1566/5000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.9103 - accuracy: 0.7216 - val_loss: 0.9192 - val_accuracy: 0.7137\n",
            "Epoch 1567/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9100 - accuracy: 0.7217 - val_loss: 0.9190 - val_accuracy: 0.7137\n",
            "Epoch 1568/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9098 - accuracy: 0.7217 - val_loss: 0.9187 - val_accuracy: 0.7137\n",
            "Epoch 1569/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9095 - accuracy: 0.7217 - val_loss: 0.9185 - val_accuracy: 0.7138\n",
            "Epoch 1570/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9093 - accuracy: 0.7218 - val_loss: 0.9182 - val_accuracy: 0.7138\n",
            "Epoch 1571/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9090 - accuracy: 0.7218 - val_loss: 0.9180 - val_accuracy: 0.7139\n",
            "Epoch 1572/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9088 - accuracy: 0.7219 - val_loss: 0.9177 - val_accuracy: 0.7140\n",
            "Epoch 1573/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9085 - accuracy: 0.7219 - val_loss: 0.9175 - val_accuracy: 0.7140\n",
            "Epoch 1574/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9083 - accuracy: 0.7220 - val_loss: 0.9172 - val_accuracy: 0.7141\n",
            "Epoch 1575/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9080 - accuracy: 0.7221 - val_loss: 0.9170 - val_accuracy: 0.7141\n",
            "Epoch 1576/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9077 - accuracy: 0.7221 - val_loss: 0.9167 - val_accuracy: 0.7141\n",
            "Epoch 1577/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9075 - accuracy: 0.7221 - val_loss: 0.9165 - val_accuracy: 0.7141\n",
            "Epoch 1578/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9072 - accuracy: 0.7221 - val_loss: 0.9162 - val_accuracy: 0.7141\n",
            "Epoch 1579/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9070 - accuracy: 0.7221 - val_loss: 0.9160 - val_accuracy: 0.7140\n",
            "Epoch 1580/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9067 - accuracy: 0.7222 - val_loss: 0.9157 - val_accuracy: 0.7140\n",
            "Epoch 1581/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9065 - accuracy: 0.7222 - val_loss: 0.9155 - val_accuracy: 0.7141\n",
            "Epoch 1582/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9062 - accuracy: 0.7222 - val_loss: 0.9152 - val_accuracy: 0.7143\n",
            "Epoch 1583/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9060 - accuracy: 0.7222 - val_loss: 0.9150 - val_accuracy: 0.7143\n",
            "Epoch 1584/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9057 - accuracy: 0.7223 - val_loss: 0.9147 - val_accuracy: 0.7143\n",
            "Epoch 1585/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9055 - accuracy: 0.7223 - val_loss: 0.9145 - val_accuracy: 0.7143\n",
            "Epoch 1586/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9052 - accuracy: 0.7224 - val_loss: 0.9142 - val_accuracy: 0.7143\n",
            "Epoch 1587/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9050 - accuracy: 0.7224 - val_loss: 0.9140 - val_accuracy: 0.7143\n",
            "Epoch 1588/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.9047 - accuracy: 0.7224 - val_loss: 0.9138 - val_accuracy: 0.7143\n",
            "Epoch 1589/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9045 - accuracy: 0.7225 - val_loss: 0.9135 - val_accuracy: 0.7143\n",
            "Epoch 1590/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.9042 - accuracy: 0.7226 - val_loss: 0.9133 - val_accuracy: 0.7144\n",
            "Epoch 1591/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.9040 - accuracy: 0.7226 - val_loss: 0.9130 - val_accuracy: 0.7145\n",
            "Epoch 1592/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.9037 - accuracy: 0.7226 - val_loss: 0.9128 - val_accuracy: 0.7146\n",
            "Epoch 1593/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9035 - accuracy: 0.7226 - val_loss: 0.9125 - val_accuracy: 0.7146\n",
            "Epoch 1594/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9032 - accuracy: 0.7226 - val_loss: 0.9123 - val_accuracy: 0.7147\n",
            "Epoch 1595/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9030 - accuracy: 0.7227 - val_loss: 0.9120 - val_accuracy: 0.7148\n",
            "Epoch 1596/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.9028 - accuracy: 0.7227 - val_loss: 0.9118 - val_accuracy: 0.7150\n",
            "Epoch 1597/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.9025 - accuracy: 0.7228 - val_loss: 0.9116 - val_accuracy: 0.7150\n",
            "Epoch 1598/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9023 - accuracy: 0.7229 - val_loss: 0.9113 - val_accuracy: 0.7150\n",
            "Epoch 1599/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.9020 - accuracy: 0.7229 - val_loss: 0.9111 - val_accuracy: 0.7152\n",
            "Epoch 1600/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.9018 - accuracy: 0.7229 - val_loss: 0.9108 - val_accuracy: 0.7153\n",
            "Epoch 1601/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9015 - accuracy: 0.7229 - val_loss: 0.9106 - val_accuracy: 0.7153\n",
            "Epoch 1602/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.9013 - accuracy: 0.7229 - val_loss: 0.9104 - val_accuracy: 0.7154\n",
            "Epoch 1603/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9010 - accuracy: 0.7229 - val_loss: 0.9101 - val_accuracy: 0.7154\n",
            "Epoch 1604/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9008 - accuracy: 0.7229 - val_loss: 0.9099 - val_accuracy: 0.7154\n",
            "Epoch 1605/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9005 - accuracy: 0.7229 - val_loss: 0.9096 - val_accuracy: 0.7154\n",
            "Epoch 1606/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.9003 - accuracy: 0.7229 - val_loss: 0.9094 - val_accuracy: 0.7155\n",
            "Epoch 1607/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9001 - accuracy: 0.7231 - val_loss: 0.9092 - val_accuracy: 0.7156\n",
            "Epoch 1608/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8998 - accuracy: 0.7231 - val_loss: 0.9089 - val_accuracy: 0.7156\n",
            "Epoch 1609/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8996 - accuracy: 0.7232 - val_loss: 0.9087 - val_accuracy: 0.7156\n",
            "Epoch 1610/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8993 - accuracy: 0.7232 - val_loss: 0.9084 - val_accuracy: 0.7157\n",
            "Epoch 1611/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8991 - accuracy: 0.7232 - val_loss: 0.9082 - val_accuracy: 0.7157\n",
            "Epoch 1612/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.8988 - accuracy: 0.7233 - val_loss: 0.9080 - val_accuracy: 0.7156\n",
            "Epoch 1613/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8986 - accuracy: 0.7233 - val_loss: 0.9077 - val_accuracy: 0.7156\n",
            "Epoch 1614/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8983 - accuracy: 0.7233 - val_loss: 0.9075 - val_accuracy: 0.7159\n",
            "Epoch 1615/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8981 - accuracy: 0.7233 - val_loss: 0.9072 - val_accuracy: 0.7159\n",
            "Epoch 1616/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8979 - accuracy: 0.7233 - val_loss: 0.9070 - val_accuracy: 0.7159\n",
            "Epoch 1617/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8976 - accuracy: 0.7234 - val_loss: 0.9068 - val_accuracy: 0.7160\n",
            "Epoch 1618/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8974 - accuracy: 0.7235 - val_loss: 0.9065 - val_accuracy: 0.7160\n",
            "Epoch 1619/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8971 - accuracy: 0.7236 - val_loss: 0.9063 - val_accuracy: 0.7161\n",
            "Epoch 1620/5000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.8969 - accuracy: 0.7236 - val_loss: 0.9061 - val_accuracy: 0.7161\n",
            "Epoch 1621/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8967 - accuracy: 0.7236 - val_loss: 0.9058 - val_accuracy: 0.7161\n",
            "Epoch 1622/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8964 - accuracy: 0.7237 - val_loss: 0.9056 - val_accuracy: 0.7162\n",
            "Epoch 1623/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8962 - accuracy: 0.7237 - val_loss: 0.9053 - val_accuracy: 0.7162\n",
            "Epoch 1624/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8959 - accuracy: 0.7237 - val_loss: 0.9051 - val_accuracy: 0.7163\n",
            "Epoch 1625/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8957 - accuracy: 0.7237 - val_loss: 0.9049 - val_accuracy: 0.7163\n",
            "Epoch 1626/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8955 - accuracy: 0.7237 - val_loss: 0.9046 - val_accuracy: 0.7164\n",
            "Epoch 1627/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8952 - accuracy: 0.7237 - val_loss: 0.9044 - val_accuracy: 0.7164\n",
            "Epoch 1628/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8950 - accuracy: 0.7238 - val_loss: 0.9042 - val_accuracy: 0.7165\n",
            "Epoch 1629/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8947 - accuracy: 0.7238 - val_loss: 0.9039 - val_accuracy: 0.7167\n",
            "Epoch 1630/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8945 - accuracy: 0.7239 - val_loss: 0.9037 - val_accuracy: 0.7167\n",
            "Epoch 1631/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8943 - accuracy: 0.7239 - val_loss: 0.9035 - val_accuracy: 0.7166\n",
            "Epoch 1632/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8940 - accuracy: 0.7239 - val_loss: 0.9032 - val_accuracy: 0.7167\n",
            "Epoch 1633/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8938 - accuracy: 0.7239 - val_loss: 0.9030 - val_accuracy: 0.7167\n",
            "Epoch 1634/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8936 - accuracy: 0.7240 - val_loss: 0.9028 - val_accuracy: 0.7167\n",
            "Epoch 1635/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8933 - accuracy: 0.7240 - val_loss: 0.9025 - val_accuracy: 0.7167\n",
            "Epoch 1636/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8931 - accuracy: 0.7240 - val_loss: 0.9023 - val_accuracy: 0.7167\n",
            "Epoch 1637/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8928 - accuracy: 0.7241 - val_loss: 0.9021 - val_accuracy: 0.7167\n",
            "Epoch 1638/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8926 - accuracy: 0.7242 - val_loss: 0.9018 - val_accuracy: 0.7167\n",
            "Epoch 1639/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8924 - accuracy: 0.7242 - val_loss: 0.9016 - val_accuracy: 0.7169\n",
            "Epoch 1640/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8921 - accuracy: 0.7242 - val_loss: 0.9014 - val_accuracy: 0.7169\n",
            "Epoch 1641/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8919 - accuracy: 0.7243 - val_loss: 0.9011 - val_accuracy: 0.7169\n",
            "Epoch 1642/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8917 - accuracy: 0.7243 - val_loss: 0.9009 - val_accuracy: 0.7170\n",
            "Epoch 1643/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8914 - accuracy: 0.7244 - val_loss: 0.9007 - val_accuracy: 0.7170\n",
            "Epoch 1644/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8912 - accuracy: 0.7245 - val_loss: 0.9005 - val_accuracy: 0.7170\n",
            "Epoch 1645/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8910 - accuracy: 0.7246 - val_loss: 0.9002 - val_accuracy: 0.7171\n",
            "Epoch 1646/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8907 - accuracy: 0.7246 - val_loss: 0.9000 - val_accuracy: 0.7171\n",
            "Epoch 1647/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8905 - accuracy: 0.7246 - val_loss: 0.8998 - val_accuracy: 0.7171\n",
            "Epoch 1648/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8903 - accuracy: 0.7246 - val_loss: 0.8995 - val_accuracy: 0.7171\n",
            "Epoch 1649/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8900 - accuracy: 0.7246 - val_loss: 0.8993 - val_accuracy: 0.7171\n",
            "Epoch 1650/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8898 - accuracy: 0.7247 - val_loss: 0.8991 - val_accuracy: 0.7170\n",
            "Epoch 1651/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8896 - accuracy: 0.7248 - val_loss: 0.8988 - val_accuracy: 0.7170\n",
            "Epoch 1652/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8893 - accuracy: 0.7248 - val_loss: 0.8986 - val_accuracy: 0.7170\n",
            "Epoch 1653/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8891 - accuracy: 0.7248 - val_loss: 0.8984 - val_accuracy: 0.7171\n",
            "Epoch 1654/5000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.8889 - accuracy: 0.7249 - val_loss: 0.8982 - val_accuracy: 0.7173\n",
            "Epoch 1655/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8886 - accuracy: 0.7248 - val_loss: 0.8979 - val_accuracy: 0.7172\n",
            "Epoch 1656/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8884 - accuracy: 0.7250 - val_loss: 0.8977 - val_accuracy: 0.7173\n",
            "Epoch 1657/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8882 - accuracy: 0.7250 - val_loss: 0.8975 - val_accuracy: 0.7174\n",
            "Epoch 1658/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8879 - accuracy: 0.7251 - val_loss: 0.8973 - val_accuracy: 0.7175\n",
            "Epoch 1659/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8877 - accuracy: 0.7251 - val_loss: 0.8970 - val_accuracy: 0.7176\n",
            "Epoch 1660/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8875 - accuracy: 0.7251 - val_loss: 0.8968 - val_accuracy: 0.7176\n",
            "Epoch 1661/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8872 - accuracy: 0.7251 - val_loss: 0.8966 - val_accuracy: 0.7176\n",
            "Epoch 1662/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8870 - accuracy: 0.7251 - val_loss: 0.8963 - val_accuracy: 0.7177\n",
            "Epoch 1663/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8868 - accuracy: 0.7252 - val_loss: 0.8961 - val_accuracy: 0.7177\n",
            "Epoch 1664/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.8865 - accuracy: 0.7252 - val_loss: 0.8959 - val_accuracy: 0.7178\n",
            "Epoch 1665/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8863 - accuracy: 0.7252 - val_loss: 0.8957 - val_accuracy: 0.7178\n",
            "Epoch 1666/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8861 - accuracy: 0.7253 - val_loss: 0.8954 - val_accuracy: 0.7178\n",
            "Epoch 1667/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8859 - accuracy: 0.7253 - val_loss: 0.8952 - val_accuracy: 0.7180\n",
            "Epoch 1668/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8856 - accuracy: 0.7254 - val_loss: 0.8950 - val_accuracy: 0.7179\n",
            "Epoch 1669/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8854 - accuracy: 0.7254 - val_loss: 0.8948 - val_accuracy: 0.7179\n",
            "Epoch 1670/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8852 - accuracy: 0.7254 - val_loss: 0.8945 - val_accuracy: 0.7181\n",
            "Epoch 1671/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8849 - accuracy: 0.7254 - val_loss: 0.8943 - val_accuracy: 0.7181\n",
            "Epoch 1672/5000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.8847 - accuracy: 0.7255 - val_loss: 0.8941 - val_accuracy: 0.7182\n",
            "Epoch 1673/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8845 - accuracy: 0.7256 - val_loss: 0.8939 - val_accuracy: 0.7182\n",
            "Epoch 1674/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8843 - accuracy: 0.7256 - val_loss: 0.8936 - val_accuracy: 0.7181\n",
            "Epoch 1675/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8840 - accuracy: 0.7256 - val_loss: 0.8934 - val_accuracy: 0.7182\n",
            "Epoch 1676/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8838 - accuracy: 0.7257 - val_loss: 0.8932 - val_accuracy: 0.7183\n",
            "Epoch 1677/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8836 - accuracy: 0.7257 - val_loss: 0.8930 - val_accuracy: 0.7185\n",
            "Epoch 1678/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8833 - accuracy: 0.7257 - val_loss: 0.8928 - val_accuracy: 0.7184\n",
            "Epoch 1679/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8831 - accuracy: 0.7258 - val_loss: 0.8925 - val_accuracy: 0.7184\n",
            "Epoch 1680/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8829 - accuracy: 0.7258 - val_loss: 0.8923 - val_accuracy: 0.7184\n",
            "Epoch 1681/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8827 - accuracy: 0.7258 - val_loss: 0.8921 - val_accuracy: 0.7184\n",
            "Epoch 1682/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8824 - accuracy: 0.7257 - val_loss: 0.8919 - val_accuracy: 0.7184\n",
            "Epoch 1683/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8822 - accuracy: 0.7258 - val_loss: 0.8916 - val_accuracy: 0.7183\n",
            "Epoch 1684/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8820 - accuracy: 0.7258 - val_loss: 0.8914 - val_accuracy: 0.7183\n",
            "Epoch 1685/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8818 - accuracy: 0.7258 - val_loss: 0.8912 - val_accuracy: 0.7183\n",
            "Epoch 1686/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8815 - accuracy: 0.7258 - val_loss: 0.8910 - val_accuracy: 0.7183\n",
            "Epoch 1687/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8813 - accuracy: 0.7259 - val_loss: 0.8908 - val_accuracy: 0.7182\n",
            "Epoch 1688/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8811 - accuracy: 0.7259 - val_loss: 0.8905 - val_accuracy: 0.7183\n",
            "Epoch 1689/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8809 - accuracy: 0.7260 - val_loss: 0.8903 - val_accuracy: 0.7183\n",
            "Epoch 1690/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8806 - accuracy: 0.7260 - val_loss: 0.8901 - val_accuracy: 0.7182\n",
            "Epoch 1691/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8804 - accuracy: 0.7260 - val_loss: 0.8899 - val_accuracy: 0.7182\n",
            "Epoch 1692/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8802 - accuracy: 0.7261 - val_loss: 0.8897 - val_accuracy: 0.7183\n",
            "Epoch 1693/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8800 - accuracy: 0.7261 - val_loss: 0.8894 - val_accuracy: 0.7183\n",
            "Epoch 1694/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8797 - accuracy: 0.7261 - val_loss: 0.8892 - val_accuracy: 0.7183\n",
            "Epoch 1695/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8795 - accuracy: 0.7261 - val_loss: 0.8890 - val_accuracy: 0.7183\n",
            "Epoch 1696/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8793 - accuracy: 0.7262 - val_loss: 0.8888 - val_accuracy: 0.7183\n",
            "Epoch 1697/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8791 - accuracy: 0.7262 - val_loss: 0.8886 - val_accuracy: 0.7183\n",
            "Epoch 1698/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8789 - accuracy: 0.7262 - val_loss: 0.8884 - val_accuracy: 0.7182\n",
            "Epoch 1699/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8786 - accuracy: 0.7262 - val_loss: 0.8881 - val_accuracy: 0.7181\n",
            "Epoch 1700/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8784 - accuracy: 0.7263 - val_loss: 0.8879 - val_accuracy: 0.7182\n",
            "Epoch 1701/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8782 - accuracy: 0.7263 - val_loss: 0.8877 - val_accuracy: 0.7182\n",
            "Epoch 1702/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8780 - accuracy: 0.7264 - val_loss: 0.8875 - val_accuracy: 0.7182\n",
            "Epoch 1703/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8777 - accuracy: 0.7264 - val_loss: 0.8873 - val_accuracy: 0.7182\n",
            "Epoch 1704/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8775 - accuracy: 0.7265 - val_loss: 0.8871 - val_accuracy: 0.7182\n",
            "Epoch 1705/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8773 - accuracy: 0.7265 - val_loss: 0.8868 - val_accuracy: 0.7181\n",
            "Epoch 1706/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8771 - accuracy: 0.7265 - val_loss: 0.8866 - val_accuracy: 0.7181\n",
            "Epoch 1707/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8769 - accuracy: 0.7266 - val_loss: 0.8864 - val_accuracy: 0.7181\n",
            "Epoch 1708/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8766 - accuracy: 0.7266 - val_loss: 0.8862 - val_accuracy: 0.7180\n",
            "Epoch 1709/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8764 - accuracy: 0.7266 - val_loss: 0.8860 - val_accuracy: 0.7180\n",
            "Epoch 1710/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8762 - accuracy: 0.7266 - val_loss: 0.8858 - val_accuracy: 0.7180\n",
            "Epoch 1711/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8760 - accuracy: 0.7267 - val_loss: 0.8855 - val_accuracy: 0.7179\n",
            "Epoch 1712/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8758 - accuracy: 0.7268 - val_loss: 0.8853 - val_accuracy: 0.7179\n",
            "Epoch 1713/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8755 - accuracy: 0.7268 - val_loss: 0.8851 - val_accuracy: 0.7180\n",
            "Epoch 1714/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8753 - accuracy: 0.7269 - val_loss: 0.8849 - val_accuracy: 0.7181\n",
            "Epoch 1715/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8751 - accuracy: 0.7269 - val_loss: 0.8847 - val_accuracy: 0.7181\n",
            "Epoch 1716/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8749 - accuracy: 0.7270 - val_loss: 0.8845 - val_accuracy: 0.7181\n",
            "Epoch 1717/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8747 - accuracy: 0.7271 - val_loss: 0.8843 - val_accuracy: 0.7182\n",
            "Epoch 1718/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8745 - accuracy: 0.7272 - val_loss: 0.8840 - val_accuracy: 0.7182\n",
            "Epoch 1719/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8742 - accuracy: 0.7272 - val_loss: 0.8838 - val_accuracy: 0.7183\n",
            "Epoch 1720/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.8740 - accuracy: 0.7272 - val_loss: 0.8836 - val_accuracy: 0.7183\n",
            "Epoch 1721/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8738 - accuracy: 0.7272 - val_loss: 0.8834 - val_accuracy: 0.7184\n",
            "Epoch 1722/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8736 - accuracy: 0.7272 - val_loss: 0.8832 - val_accuracy: 0.7184\n",
            "Epoch 1723/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8734 - accuracy: 0.7273 - val_loss: 0.8830 - val_accuracy: 0.7184\n",
            "Epoch 1724/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8732 - accuracy: 0.7274 - val_loss: 0.8828 - val_accuracy: 0.7185\n",
            "Epoch 1725/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8729 - accuracy: 0.7274 - val_loss: 0.8826 - val_accuracy: 0.7185\n",
            "Epoch 1726/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8727 - accuracy: 0.7275 - val_loss: 0.8823 - val_accuracy: 0.7186\n",
            "Epoch 1727/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8725 - accuracy: 0.7276 - val_loss: 0.8821 - val_accuracy: 0.7186\n",
            "Epoch 1728/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8723 - accuracy: 0.7276 - val_loss: 0.8819 - val_accuracy: 0.7186\n",
            "Epoch 1729/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8721 - accuracy: 0.7277 - val_loss: 0.8817 - val_accuracy: 0.7187\n",
            "Epoch 1730/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8719 - accuracy: 0.7277 - val_loss: 0.8815 - val_accuracy: 0.7188\n",
            "Epoch 1731/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8716 - accuracy: 0.7278 - val_loss: 0.8813 - val_accuracy: 0.7189\n",
            "Epoch 1732/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8714 - accuracy: 0.7278 - val_loss: 0.8811 - val_accuracy: 0.7189\n",
            "Epoch 1733/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8712 - accuracy: 0.7278 - val_loss: 0.8809 - val_accuracy: 0.7188\n",
            "Epoch 1734/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8710 - accuracy: 0.7279 - val_loss: 0.8807 - val_accuracy: 0.7189\n",
            "Epoch 1735/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8708 - accuracy: 0.7279 - val_loss: 0.8804 - val_accuracy: 0.7191\n",
            "Epoch 1736/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8706 - accuracy: 0.7279 - val_loss: 0.8802 - val_accuracy: 0.7191\n",
            "Epoch 1737/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8704 - accuracy: 0.7279 - val_loss: 0.8800 - val_accuracy: 0.7193\n",
            "Epoch 1738/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8701 - accuracy: 0.7280 - val_loss: 0.8798 - val_accuracy: 0.7193\n",
            "Epoch 1739/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8699 - accuracy: 0.7280 - val_loss: 0.8796 - val_accuracy: 0.7193\n",
            "Epoch 1740/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8697 - accuracy: 0.7281 - val_loss: 0.8794 - val_accuracy: 0.7193\n",
            "Epoch 1741/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.8695 - accuracy: 0.7281 - val_loss: 0.8792 - val_accuracy: 0.7193\n",
            "Epoch 1742/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8693 - accuracy: 0.7282 - val_loss: 0.8790 - val_accuracy: 0.7193\n",
            "Epoch 1743/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8691 - accuracy: 0.7282 - val_loss: 0.8788 - val_accuracy: 0.7194\n",
            "Epoch 1744/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8689 - accuracy: 0.7282 - val_loss: 0.8786 - val_accuracy: 0.7194\n",
            "Epoch 1745/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.8687 - accuracy: 0.7283 - val_loss: 0.8784 - val_accuracy: 0.7195\n",
            "Epoch 1746/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8684 - accuracy: 0.7284 - val_loss: 0.8782 - val_accuracy: 0.7196\n",
            "Epoch 1747/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8682 - accuracy: 0.7284 - val_loss: 0.8779 - val_accuracy: 0.7196\n",
            "Epoch 1748/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.8680 - accuracy: 0.7285 - val_loss: 0.8777 - val_accuracy: 0.7197\n",
            "Epoch 1749/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8678 - accuracy: 0.7286 - val_loss: 0.8775 - val_accuracy: 0.7198\n",
            "Epoch 1750/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8676 - accuracy: 0.7286 - val_loss: 0.8773 - val_accuracy: 0.7198\n",
            "Epoch 1751/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8674 - accuracy: 0.7286 - val_loss: 0.8771 - val_accuracy: 0.7199\n",
            "Epoch 1752/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8672 - accuracy: 0.7287 - val_loss: 0.8769 - val_accuracy: 0.7199\n",
            "Epoch 1753/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.8670 - accuracy: 0.7288 - val_loss: 0.8767 - val_accuracy: 0.7199\n",
            "Epoch 1754/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8668 - accuracy: 0.7288 - val_loss: 0.8765 - val_accuracy: 0.7199\n",
            "Epoch 1755/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8666 - accuracy: 0.7289 - val_loss: 0.8763 - val_accuracy: 0.7201\n",
            "Epoch 1756/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8663 - accuracy: 0.7290 - val_loss: 0.8761 - val_accuracy: 0.7202\n",
            "Epoch 1757/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8661 - accuracy: 0.7290 - val_loss: 0.8759 - val_accuracy: 0.7202\n",
            "Epoch 1758/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8659 - accuracy: 0.7290 - val_loss: 0.8757 - val_accuracy: 0.7202\n",
            "Epoch 1759/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8657 - accuracy: 0.7291 - val_loss: 0.8755 - val_accuracy: 0.7202\n",
            "Epoch 1760/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8655 - accuracy: 0.7291 - val_loss: 0.8753 - val_accuracy: 0.7202\n",
            "Epoch 1761/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8653 - accuracy: 0.7291 - val_loss: 0.8751 - val_accuracy: 0.7203\n",
            "Epoch 1762/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.8651 - accuracy: 0.7291 - val_loss: 0.8749 - val_accuracy: 0.7204\n",
            "Epoch 1763/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8649 - accuracy: 0.7291 - val_loss: 0.8747 - val_accuracy: 0.7206\n",
            "Epoch 1764/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8647 - accuracy: 0.7292 - val_loss: 0.8745 - val_accuracy: 0.7208\n",
            "Epoch 1765/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8645 - accuracy: 0.7293 - val_loss: 0.8742 - val_accuracy: 0.7209\n",
            "Epoch 1766/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8643 - accuracy: 0.7293 - val_loss: 0.8740 - val_accuracy: 0.7210\n",
            "Epoch 1767/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8641 - accuracy: 0.7293 - val_loss: 0.8738 - val_accuracy: 0.7210\n",
            "Epoch 1768/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8638 - accuracy: 0.7294 - val_loss: 0.8736 - val_accuracy: 0.7209\n",
            "Epoch 1769/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8636 - accuracy: 0.7293 - val_loss: 0.8734 - val_accuracy: 0.7211\n",
            "Epoch 1770/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8634 - accuracy: 0.7294 - val_loss: 0.8732 - val_accuracy: 0.7211\n",
            "Epoch 1771/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8632 - accuracy: 0.7294 - val_loss: 0.8730 - val_accuracy: 0.7213\n",
            "Epoch 1772/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8630 - accuracy: 0.7294 - val_loss: 0.8728 - val_accuracy: 0.7213\n",
            "Epoch 1773/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8628 - accuracy: 0.7294 - val_loss: 0.8726 - val_accuracy: 0.7213\n",
            "Epoch 1774/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8626 - accuracy: 0.7295 - val_loss: 0.8724 - val_accuracy: 0.7215\n",
            "Epoch 1775/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8624 - accuracy: 0.7295 - val_loss: 0.8722 - val_accuracy: 0.7215\n",
            "Epoch 1776/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8622 - accuracy: 0.7295 - val_loss: 0.8720 - val_accuracy: 0.7214\n",
            "Epoch 1777/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8620 - accuracy: 0.7295 - val_loss: 0.8718 - val_accuracy: 0.7214\n",
            "Epoch 1778/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8618 - accuracy: 0.7295 - val_loss: 0.8716 - val_accuracy: 0.7215\n",
            "Epoch 1779/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8616 - accuracy: 0.7296 - val_loss: 0.8714 - val_accuracy: 0.7217\n",
            "Epoch 1780/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8614 - accuracy: 0.7296 - val_loss: 0.8712 - val_accuracy: 0.7217\n",
            "Epoch 1781/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8612 - accuracy: 0.7296 - val_loss: 0.8710 - val_accuracy: 0.7217\n",
            "Epoch 1782/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8610 - accuracy: 0.7297 - val_loss: 0.8708 - val_accuracy: 0.7217\n",
            "Epoch 1783/5000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.8608 - accuracy: 0.7297 - val_loss: 0.8706 - val_accuracy: 0.7217\n",
            "Epoch 1784/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8606 - accuracy: 0.7297 - val_loss: 0.8704 - val_accuracy: 0.7218\n",
            "Epoch 1785/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8604 - accuracy: 0.7297 - val_loss: 0.8702 - val_accuracy: 0.7218\n",
            "Epoch 1786/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8601 - accuracy: 0.7298 - val_loss: 0.8700 - val_accuracy: 0.7220\n",
            "Epoch 1787/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8599 - accuracy: 0.7299 - val_loss: 0.8698 - val_accuracy: 0.7220\n",
            "Epoch 1788/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8597 - accuracy: 0.7299 - val_loss: 0.8696 - val_accuracy: 0.7220\n",
            "Epoch 1789/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8595 - accuracy: 0.7299 - val_loss: 0.8694 - val_accuracy: 0.7223\n",
            "Epoch 1790/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8593 - accuracy: 0.7300 - val_loss: 0.8692 - val_accuracy: 0.7221\n",
            "Epoch 1791/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8591 - accuracy: 0.7300 - val_loss: 0.8690 - val_accuracy: 0.7222\n",
            "Epoch 1792/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8589 - accuracy: 0.7301 - val_loss: 0.8688 - val_accuracy: 0.7223\n",
            "Epoch 1793/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8587 - accuracy: 0.7301 - val_loss: 0.8686 - val_accuracy: 0.7224\n",
            "Epoch 1794/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8585 - accuracy: 0.7302 - val_loss: 0.8684 - val_accuracy: 0.7224\n",
            "Epoch 1795/5000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.8583 - accuracy: 0.7302 - val_loss: 0.8682 - val_accuracy: 0.7224\n",
            "Epoch 1796/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8581 - accuracy: 0.7302 - val_loss: 0.8680 - val_accuracy: 0.7224\n",
            "Epoch 1797/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8579 - accuracy: 0.7302 - val_loss: 0.8678 - val_accuracy: 0.7225\n",
            "Epoch 1798/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8577 - accuracy: 0.7303 - val_loss: 0.8676 - val_accuracy: 0.7226\n",
            "Epoch 1799/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8575 - accuracy: 0.7303 - val_loss: 0.8674 - val_accuracy: 0.7226\n",
            "Epoch 1800/5000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.8573 - accuracy: 0.7304 - val_loss: 0.8672 - val_accuracy: 0.7227\n",
            "Epoch 1801/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8571 - accuracy: 0.7304 - val_loss: 0.8670 - val_accuracy: 0.7227\n",
            "Epoch 1802/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8569 - accuracy: 0.7304 - val_loss: 0.8669 - val_accuracy: 0.7228\n",
            "Epoch 1803/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8567 - accuracy: 0.7305 - val_loss: 0.8667 - val_accuracy: 0.7228\n",
            "Epoch 1804/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8565 - accuracy: 0.7305 - val_loss: 0.8665 - val_accuracy: 0.7227\n",
            "Epoch 1805/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8563 - accuracy: 0.7305 - val_loss: 0.8663 - val_accuracy: 0.7228\n",
            "Epoch 1806/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8561 - accuracy: 0.7305 - val_loss: 0.8661 - val_accuracy: 0.7228\n",
            "Epoch 1807/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8559 - accuracy: 0.7307 - val_loss: 0.8659 - val_accuracy: 0.7228\n",
            "Epoch 1808/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8557 - accuracy: 0.7307 - val_loss: 0.8657 - val_accuracy: 0.7228\n",
            "Epoch 1809/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8555 - accuracy: 0.7307 - val_loss: 0.8655 - val_accuracy: 0.7229\n",
            "Epoch 1810/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8553 - accuracy: 0.7308 - val_loss: 0.8653 - val_accuracy: 0.7231\n",
            "Epoch 1811/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8551 - accuracy: 0.7308 - val_loss: 0.8651 - val_accuracy: 0.7231\n",
            "Epoch 1812/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8549 - accuracy: 0.7308 - val_loss: 0.8649 - val_accuracy: 0.7232\n",
            "Epoch 1813/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8547 - accuracy: 0.7308 - val_loss: 0.8647 - val_accuracy: 0.7232\n",
            "Epoch 1814/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8545 - accuracy: 0.7309 - val_loss: 0.8645 - val_accuracy: 0.7233\n",
            "Epoch 1815/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8543 - accuracy: 0.7309 - val_loss: 0.8643 - val_accuracy: 0.7235\n",
            "Epoch 1816/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8541 - accuracy: 0.7309 - val_loss: 0.8641 - val_accuracy: 0.7236\n",
            "Epoch 1817/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8539 - accuracy: 0.7309 - val_loss: 0.8639 - val_accuracy: 0.7237\n",
            "Epoch 1818/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8537 - accuracy: 0.7310 - val_loss: 0.8637 - val_accuracy: 0.7237\n",
            "Epoch 1819/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8535 - accuracy: 0.7310 - val_loss: 0.8635 - val_accuracy: 0.7238\n",
            "Epoch 1820/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8533 - accuracy: 0.7311 - val_loss: 0.8634 - val_accuracy: 0.7238\n",
            "Epoch 1821/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8531 - accuracy: 0.7312 - val_loss: 0.8632 - val_accuracy: 0.7239\n",
            "Epoch 1822/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8529 - accuracy: 0.7312 - val_loss: 0.8630 - val_accuracy: 0.7239\n",
            "Epoch 1823/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8528 - accuracy: 0.7313 - val_loss: 0.8628 - val_accuracy: 0.7240\n",
            "Epoch 1824/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8526 - accuracy: 0.7313 - val_loss: 0.8626 - val_accuracy: 0.7240\n",
            "Epoch 1825/5000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.8524 - accuracy: 0.7314 - val_loss: 0.8624 - val_accuracy: 0.7240\n",
            "Epoch 1826/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8522 - accuracy: 0.7314 - val_loss: 0.8622 - val_accuracy: 0.7241\n",
            "Epoch 1827/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8520 - accuracy: 0.7315 - val_loss: 0.8620 - val_accuracy: 0.7242\n",
            "Epoch 1828/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8518 - accuracy: 0.7314 - val_loss: 0.8618 - val_accuracy: 0.7242\n",
            "Epoch 1829/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8516 - accuracy: 0.7315 - val_loss: 0.8616 - val_accuracy: 0.7241\n",
            "Epoch 1830/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8514 - accuracy: 0.7315 - val_loss: 0.8614 - val_accuracy: 0.7241\n",
            "Epoch 1831/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8512 - accuracy: 0.7315 - val_loss: 0.8612 - val_accuracy: 0.7242\n",
            "Epoch 1832/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8510 - accuracy: 0.7315 - val_loss: 0.8611 - val_accuracy: 0.7243\n",
            "Epoch 1833/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8508 - accuracy: 0.7316 - val_loss: 0.8609 - val_accuracy: 0.7243\n",
            "Epoch 1834/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8506 - accuracy: 0.7316 - val_loss: 0.8607 - val_accuracy: 0.7244\n",
            "Epoch 1835/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8504 - accuracy: 0.7317 - val_loss: 0.8605 - val_accuracy: 0.7243\n",
            "Epoch 1836/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8502 - accuracy: 0.7318 - val_loss: 0.8603 - val_accuracy: 0.7244\n",
            "Epoch 1837/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8500 - accuracy: 0.7318 - val_loss: 0.8601 - val_accuracy: 0.7244\n",
            "Epoch 1838/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8498 - accuracy: 0.7318 - val_loss: 0.8599 - val_accuracy: 0.7245\n",
            "Epoch 1839/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.8496 - accuracy: 0.7318 - val_loss: 0.8597 - val_accuracy: 0.7245\n",
            "Epoch 1840/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8494 - accuracy: 0.7319 - val_loss: 0.8595 - val_accuracy: 0.7247\n",
            "Epoch 1841/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8493 - accuracy: 0.7320 - val_loss: 0.8593 - val_accuracy: 0.7247\n",
            "Epoch 1842/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8491 - accuracy: 0.7320 - val_loss: 0.8592 - val_accuracy: 0.7248\n",
            "Epoch 1843/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8489 - accuracy: 0.7320 - val_loss: 0.8590 - val_accuracy: 0.7248\n",
            "Epoch 1844/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8487 - accuracy: 0.7321 - val_loss: 0.8588 - val_accuracy: 0.7249\n",
            "Epoch 1845/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8485 - accuracy: 0.7321 - val_loss: 0.8586 - val_accuracy: 0.7249\n",
            "Epoch 1846/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8483 - accuracy: 0.7321 - val_loss: 0.8584 - val_accuracy: 0.7249\n",
            "Epoch 1847/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8481 - accuracy: 0.7321 - val_loss: 0.8582 - val_accuracy: 0.7250\n",
            "Epoch 1848/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8479 - accuracy: 0.7322 - val_loss: 0.8580 - val_accuracy: 0.7251\n",
            "Epoch 1849/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8477 - accuracy: 0.7323 - val_loss: 0.8578 - val_accuracy: 0.7251\n",
            "Epoch 1850/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8475 - accuracy: 0.7323 - val_loss: 0.8577 - val_accuracy: 0.7251\n",
            "Epoch 1851/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8473 - accuracy: 0.7323 - val_loss: 0.8575 - val_accuracy: 0.7252\n",
            "Epoch 1852/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8471 - accuracy: 0.7324 - val_loss: 0.8573 - val_accuracy: 0.7252\n",
            "Epoch 1853/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8469 - accuracy: 0.7324 - val_loss: 0.8571 - val_accuracy: 0.7252\n",
            "Epoch 1854/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8468 - accuracy: 0.7325 - val_loss: 0.8569 - val_accuracy: 0.7252\n",
            "Epoch 1855/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8466 - accuracy: 0.7325 - val_loss: 0.8567 - val_accuracy: 0.7251\n",
            "Epoch 1856/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8464 - accuracy: 0.7325 - val_loss: 0.8565 - val_accuracy: 0.7251\n",
            "Epoch 1857/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8462 - accuracy: 0.7325 - val_loss: 0.8563 - val_accuracy: 0.7251\n",
            "Epoch 1858/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8460 - accuracy: 0.7326 - val_loss: 0.8562 - val_accuracy: 0.7252\n",
            "Epoch 1859/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8458 - accuracy: 0.7326 - val_loss: 0.8560 - val_accuracy: 0.7252\n",
            "Epoch 1860/5000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.8456 - accuracy: 0.7326 - val_loss: 0.8558 - val_accuracy: 0.7251\n",
            "Epoch 1861/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8454 - accuracy: 0.7326 - val_loss: 0.8556 - val_accuracy: 0.7251\n",
            "Epoch 1862/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8452 - accuracy: 0.7327 - val_loss: 0.8554 - val_accuracy: 0.7251\n",
            "Epoch 1863/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8450 - accuracy: 0.7327 - val_loss: 0.8552 - val_accuracy: 0.7251\n",
            "Epoch 1864/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8449 - accuracy: 0.7328 - val_loss: 0.8550 - val_accuracy: 0.7251\n",
            "Epoch 1865/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8447 - accuracy: 0.7329 - val_loss: 0.8549 - val_accuracy: 0.7250\n",
            "Epoch 1866/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8445 - accuracy: 0.7329 - val_loss: 0.8547 - val_accuracy: 0.7250\n",
            "Epoch 1867/5000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.8443 - accuracy: 0.7329 - val_loss: 0.8545 - val_accuracy: 0.7249\n",
            "Epoch 1868/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8441 - accuracy: 0.7329 - val_loss: 0.8543 - val_accuracy: 0.7249\n",
            "Epoch 1869/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8439 - accuracy: 0.7330 - val_loss: 0.8541 - val_accuracy: 0.7249\n",
            "Epoch 1870/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8437 - accuracy: 0.7330 - val_loss: 0.8539 - val_accuracy: 0.7249\n",
            "Epoch 1871/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8435 - accuracy: 0.7330 - val_loss: 0.8538 - val_accuracy: 0.7249\n",
            "Epoch 1872/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8434 - accuracy: 0.7331 - val_loss: 0.8536 - val_accuracy: 0.7249\n",
            "Epoch 1873/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8432 - accuracy: 0.7332 - val_loss: 0.8534 - val_accuracy: 0.7249\n",
            "Epoch 1874/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8430 - accuracy: 0.7332 - val_loss: 0.8532 - val_accuracy: 0.7250\n",
            "Epoch 1875/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8428 - accuracy: 0.7333 - val_loss: 0.8530 - val_accuracy: 0.7250\n",
            "Epoch 1876/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8426 - accuracy: 0.7334 - val_loss: 0.8528 - val_accuracy: 0.7251\n",
            "Epoch 1877/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8424 - accuracy: 0.7334 - val_loss: 0.8527 - val_accuracy: 0.7250\n",
            "Epoch 1878/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8422 - accuracy: 0.7334 - val_loss: 0.8525 - val_accuracy: 0.7250\n",
            "Epoch 1879/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8420 - accuracy: 0.7334 - val_loss: 0.8523 - val_accuracy: 0.7251\n",
            "Epoch 1880/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8419 - accuracy: 0.7335 - val_loss: 0.8521 - val_accuracy: 0.7251\n",
            "Epoch 1881/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8417 - accuracy: 0.7335 - val_loss: 0.8519 - val_accuracy: 0.7253\n",
            "Epoch 1882/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8415 - accuracy: 0.7335 - val_loss: 0.8517 - val_accuracy: 0.7253\n",
            "Epoch 1883/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8413 - accuracy: 0.7335 - val_loss: 0.8516 - val_accuracy: 0.7253\n",
            "Epoch 1884/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.8411 - accuracy: 0.7335 - val_loss: 0.8514 - val_accuracy: 0.7254\n",
            "Epoch 1885/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8409 - accuracy: 0.7336 - val_loss: 0.8512 - val_accuracy: 0.7255\n",
            "Epoch 1886/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8407 - accuracy: 0.7336 - val_loss: 0.8510 - val_accuracy: 0.7255\n",
            "Epoch 1887/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8406 - accuracy: 0.7336 - val_loss: 0.8508 - val_accuracy: 0.7256\n",
            "Epoch 1888/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8404 - accuracy: 0.7337 - val_loss: 0.8507 - val_accuracy: 0.7257\n",
            "Epoch 1889/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8402 - accuracy: 0.7337 - val_loss: 0.8505 - val_accuracy: 0.7257\n",
            "Epoch 1890/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8400 - accuracy: 0.7337 - val_loss: 0.8503 - val_accuracy: 0.7257\n",
            "Epoch 1891/5000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.8398 - accuracy: 0.7338 - val_loss: 0.8501 - val_accuracy: 0.7259\n",
            "Epoch 1892/5000\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.8396 - accuracy: 0.7338 - val_loss: 0.8499 - val_accuracy: 0.7259\n",
            "Epoch 1893/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8395 - accuracy: 0.7339 - val_loss: 0.8498 - val_accuracy: 0.7259\n",
            "Epoch 1894/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8393 - accuracy: 0.7339 - val_loss: 0.8496 - val_accuracy: 0.7259\n",
            "Epoch 1895/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8391 - accuracy: 0.7339 - val_loss: 0.8494 - val_accuracy: 0.7260\n",
            "Epoch 1896/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8389 - accuracy: 0.7339 - val_loss: 0.8492 - val_accuracy: 0.7260\n",
            "Epoch 1897/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8387 - accuracy: 0.7339 - val_loss: 0.8490 - val_accuracy: 0.7261\n",
            "Epoch 1898/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8385 - accuracy: 0.7340 - val_loss: 0.8489 - val_accuracy: 0.7261\n",
            "Epoch 1899/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8384 - accuracy: 0.7340 - val_loss: 0.8487 - val_accuracy: 0.7261\n",
            "Epoch 1900/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8382 - accuracy: 0.7341 - val_loss: 0.8485 - val_accuracy: 0.7261\n",
            "Epoch 1901/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8380 - accuracy: 0.7341 - val_loss: 0.8483 - val_accuracy: 0.7263\n",
            "Epoch 1902/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8378 - accuracy: 0.7341 - val_loss: 0.8481 - val_accuracy: 0.7263\n",
            "Epoch 1903/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8376 - accuracy: 0.7341 - val_loss: 0.8480 - val_accuracy: 0.7264\n",
            "Epoch 1904/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8374 - accuracy: 0.7341 - val_loss: 0.8478 - val_accuracy: 0.7265\n",
            "Epoch 1905/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8373 - accuracy: 0.7342 - val_loss: 0.8476 - val_accuracy: 0.7266\n",
            "Epoch 1906/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8371 - accuracy: 0.7342 - val_loss: 0.8474 - val_accuracy: 0.7266\n",
            "Epoch 1907/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.8369 - accuracy: 0.7342 - val_loss: 0.8473 - val_accuracy: 0.7267\n",
            "Epoch 1908/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8367 - accuracy: 0.7343 - val_loss: 0.8471 - val_accuracy: 0.7268\n",
            "Epoch 1909/5000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.8365 - accuracy: 0.7343 - val_loss: 0.8469 - val_accuracy: 0.7270\n",
            "Epoch 1910/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8364 - accuracy: 0.7344 - val_loss: 0.8467 - val_accuracy: 0.7271\n",
            "Epoch 1911/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8362 - accuracy: 0.7344 - val_loss: 0.8465 - val_accuracy: 0.7272\n",
            "Epoch 1912/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8360 - accuracy: 0.7344 - val_loss: 0.8464 - val_accuracy: 0.7273\n",
            "Epoch 1913/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8358 - accuracy: 0.7344 - val_loss: 0.8462 - val_accuracy: 0.7273\n",
            "Epoch 1914/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8356 - accuracy: 0.7344 - val_loss: 0.8460 - val_accuracy: 0.7273\n",
            "Epoch 1915/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8355 - accuracy: 0.7344 - val_loss: 0.8458 - val_accuracy: 0.7273\n",
            "Epoch 1916/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8353 - accuracy: 0.7344 - val_loss: 0.8457 - val_accuracy: 0.7273\n",
            "Epoch 1917/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8351 - accuracy: 0.7346 - val_loss: 0.8455 - val_accuracy: 0.7274\n",
            "Epoch 1918/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8349 - accuracy: 0.7346 - val_loss: 0.8453 - val_accuracy: 0.7275\n",
            "Epoch 1919/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8347 - accuracy: 0.7347 - val_loss: 0.8451 - val_accuracy: 0.7275\n",
            "Epoch 1920/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8346 - accuracy: 0.7347 - val_loss: 0.8450 - val_accuracy: 0.7275\n",
            "Epoch 1921/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8344 - accuracy: 0.7348 - val_loss: 0.8448 - val_accuracy: 0.7275\n",
            "Epoch 1922/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8342 - accuracy: 0.7348 - val_loss: 0.8446 - val_accuracy: 0.7276\n",
            "Epoch 1923/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8340 - accuracy: 0.7348 - val_loss: 0.8444 - val_accuracy: 0.7278\n",
            "Epoch 1924/5000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.8338 - accuracy: 0.7348 - val_loss: 0.8443 - val_accuracy: 0.7278\n",
            "Epoch 1925/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8337 - accuracy: 0.7349 - val_loss: 0.8441 - val_accuracy: 0.7278\n",
            "Epoch 1926/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8335 - accuracy: 0.7349 - val_loss: 0.8439 - val_accuracy: 0.7278\n",
            "Epoch 1927/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8333 - accuracy: 0.7350 - val_loss: 0.8437 - val_accuracy: 0.7279\n",
            "Epoch 1928/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8331 - accuracy: 0.7350 - val_loss: 0.8436 - val_accuracy: 0.7279\n",
            "Epoch 1929/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8329 - accuracy: 0.7351 - val_loss: 0.8434 - val_accuracy: 0.7279\n",
            "Epoch 1930/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8328 - accuracy: 0.7351 - val_loss: 0.8432 - val_accuracy: 0.7279\n",
            "Epoch 1931/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8326 - accuracy: 0.7352 - val_loss: 0.8430 - val_accuracy: 0.7279\n",
            "Epoch 1932/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8324 - accuracy: 0.7352 - val_loss: 0.8429 - val_accuracy: 0.7279\n",
            "Epoch 1933/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8322 - accuracy: 0.7352 - val_loss: 0.8427 - val_accuracy: 0.7278\n",
            "Epoch 1934/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8321 - accuracy: 0.7352 - val_loss: 0.8425 - val_accuracy: 0.7278\n",
            "Epoch 1935/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8319 - accuracy: 0.7352 - val_loss: 0.8424 - val_accuracy: 0.7278\n",
            "Epoch 1936/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8317 - accuracy: 0.7352 - val_loss: 0.8422 - val_accuracy: 0.7278\n",
            "Epoch 1937/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8315 - accuracy: 0.7353 - val_loss: 0.8420 - val_accuracy: 0.7278\n",
            "Epoch 1938/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8314 - accuracy: 0.7353 - val_loss: 0.8418 - val_accuracy: 0.7278\n",
            "Epoch 1939/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8312 - accuracy: 0.7353 - val_loss: 0.8417 - val_accuracy: 0.7279\n",
            "Epoch 1940/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8310 - accuracy: 0.7354 - val_loss: 0.8415 - val_accuracy: 0.7279\n",
            "Epoch 1941/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8308 - accuracy: 0.7355 - val_loss: 0.8413 - val_accuracy: 0.7280\n",
            "Epoch 1942/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.8306 - accuracy: 0.7355 - val_loss: 0.8411 - val_accuracy: 0.7281\n",
            "Epoch 1943/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8305 - accuracy: 0.7356 - val_loss: 0.8410 - val_accuracy: 0.7282\n",
            "Epoch 1944/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8303 - accuracy: 0.7357 - val_loss: 0.8408 - val_accuracy: 0.7283\n",
            "Epoch 1945/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8301 - accuracy: 0.7357 - val_loss: 0.8406 - val_accuracy: 0.7283\n",
            "Epoch 1946/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8299 - accuracy: 0.7357 - val_loss: 0.8405 - val_accuracy: 0.7286\n",
            "Epoch 1947/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8298 - accuracy: 0.7357 - val_loss: 0.8403 - val_accuracy: 0.7286\n",
            "Epoch 1948/5000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.8296 - accuracy: 0.7357 - val_loss: 0.8401 - val_accuracy: 0.7287\n",
            "Epoch 1949/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8294 - accuracy: 0.7357 - val_loss: 0.8399 - val_accuracy: 0.7287\n",
            "Epoch 1950/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.8292 - accuracy: 0.7358 - val_loss: 0.8398 - val_accuracy: 0.7289\n",
            "Epoch 1951/5000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.8291 - accuracy: 0.7358 - val_loss: 0.8396 - val_accuracy: 0.7290\n",
            "Epoch 1952/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8289 - accuracy: 0.7358 - val_loss: 0.8394 - val_accuracy: 0.7290\n",
            "Epoch 1953/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8287 - accuracy: 0.7359 - val_loss: 0.8393 - val_accuracy: 0.7291\n",
            "Epoch 1954/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8285 - accuracy: 0.7359 - val_loss: 0.8391 - val_accuracy: 0.7291\n",
            "Epoch 1955/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8284 - accuracy: 0.7360 - val_loss: 0.8389 - val_accuracy: 0.7291\n",
            "Epoch 1956/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8282 - accuracy: 0.7360 - val_loss: 0.8388 - val_accuracy: 0.7291\n",
            "Epoch 1957/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8280 - accuracy: 0.7361 - val_loss: 0.8386 - val_accuracy: 0.7291\n",
            "Epoch 1958/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8279 - accuracy: 0.7361 - val_loss: 0.8384 - val_accuracy: 0.7291\n",
            "Epoch 1959/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8277 - accuracy: 0.7362 - val_loss: 0.8382 - val_accuracy: 0.7292\n",
            "Epoch 1960/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8275 - accuracy: 0.7362 - val_loss: 0.8381 - val_accuracy: 0.7293\n",
            "Epoch 1961/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8273 - accuracy: 0.7362 - val_loss: 0.8379 - val_accuracy: 0.7293\n",
            "Epoch 1962/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8272 - accuracy: 0.7362 - val_loss: 0.8377 - val_accuracy: 0.7292\n",
            "Epoch 1963/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8270 - accuracy: 0.7362 - val_loss: 0.8376 - val_accuracy: 0.7291\n",
            "Epoch 1964/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8268 - accuracy: 0.7362 - val_loss: 0.8374 - val_accuracy: 0.7291\n",
            "Epoch 1965/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.8266 - accuracy: 0.7363 - val_loss: 0.8372 - val_accuracy: 0.7291\n",
            "Epoch 1966/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8265 - accuracy: 0.7363 - val_loss: 0.8371 - val_accuracy: 0.7291\n",
            "Epoch 1967/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8263 - accuracy: 0.7363 - val_loss: 0.8369 - val_accuracy: 0.7292\n",
            "Epoch 1968/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8261 - accuracy: 0.7363 - val_loss: 0.8367 - val_accuracy: 0.7293\n",
            "Epoch 1969/5000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.8260 - accuracy: 0.7364 - val_loss: 0.8366 - val_accuracy: 0.7294\n",
            "Epoch 1970/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8258 - accuracy: 0.7364 - val_loss: 0.8364 - val_accuracy: 0.7294\n",
            "Epoch 1971/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8256 - accuracy: 0.7364 - val_loss: 0.8362 - val_accuracy: 0.7295\n",
            "Epoch 1972/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.8254 - accuracy: 0.7365 - val_loss: 0.8361 - val_accuracy: 0.7296\n",
            "Epoch 1973/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8253 - accuracy: 0.7365 - val_loss: 0.8359 - val_accuracy: 0.7296\n",
            "Epoch 1974/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8251 - accuracy: 0.7365 - val_loss: 0.8357 - val_accuracy: 0.7296\n",
            "Epoch 1975/5000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.8249 - accuracy: 0.7365 - val_loss: 0.8356 - val_accuracy: 0.7296\n",
            "Epoch 1976/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8248 - accuracy: 0.7365 - val_loss: 0.8354 - val_accuracy: 0.7296\n",
            "Epoch 1977/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8246 - accuracy: 0.7366 - val_loss: 0.8352 - val_accuracy: 0.7296\n",
            "Epoch 1978/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8244 - accuracy: 0.7366 - val_loss: 0.8351 - val_accuracy: 0.7297\n",
            "Epoch 1979/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8243 - accuracy: 0.7366 - val_loss: 0.8349 - val_accuracy: 0.7297\n",
            "Epoch 1980/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8241 - accuracy: 0.7366 - val_loss: 0.8347 - val_accuracy: 0.7298\n",
            "Epoch 1981/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8239 - accuracy: 0.7366 - val_loss: 0.8346 - val_accuracy: 0.7299\n",
            "Epoch 1982/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8237 - accuracy: 0.7366 - val_loss: 0.8344 - val_accuracy: 0.7300\n",
            "Epoch 1983/5000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.8236 - accuracy: 0.7367 - val_loss: 0.8342 - val_accuracy: 0.7300\n",
            "Epoch 1984/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8234 - accuracy: 0.7368 - val_loss: 0.8341 - val_accuracy: 0.7300\n",
            "Epoch 1985/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8232 - accuracy: 0.7368 - val_loss: 0.8339 - val_accuracy: 0.7300\n",
            "Epoch 1986/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8231 - accuracy: 0.7369 - val_loss: 0.8337 - val_accuracy: 0.7301\n",
            "Epoch 1987/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8229 - accuracy: 0.7369 - val_loss: 0.8336 - val_accuracy: 0.7300\n",
            "Epoch 1988/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8227 - accuracy: 0.7369 - val_loss: 0.8334 - val_accuracy: 0.7302\n",
            "Epoch 1989/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8226 - accuracy: 0.7370 - val_loss: 0.8332 - val_accuracy: 0.7302\n",
            "Epoch 1990/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8224 - accuracy: 0.7370 - val_loss: 0.8331 - val_accuracy: 0.7303\n",
            "Epoch 1991/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.8222 - accuracy: 0.7370 - val_loss: 0.8329 - val_accuracy: 0.7303\n",
            "Epoch 1992/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8221 - accuracy: 0.7371 - val_loss: 0.8327 - val_accuracy: 0.7303\n",
            "Epoch 1993/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8219 - accuracy: 0.7371 - val_loss: 0.8326 - val_accuracy: 0.7303\n",
            "Epoch 1994/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8217 - accuracy: 0.7372 - val_loss: 0.8324 - val_accuracy: 0.7303\n",
            "Epoch 1995/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8215 - accuracy: 0.7372 - val_loss: 0.8322 - val_accuracy: 0.7304\n",
            "Epoch 1996/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8214 - accuracy: 0.7372 - val_loss: 0.8321 - val_accuracy: 0.7305\n",
            "Epoch 1997/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8212 - accuracy: 0.7372 - val_loss: 0.8319 - val_accuracy: 0.7306\n",
            "Epoch 1998/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8210 - accuracy: 0.7373 - val_loss: 0.8318 - val_accuracy: 0.7306\n",
            "Epoch 1999/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8209 - accuracy: 0.7374 - val_loss: 0.8316 - val_accuracy: 0.7306\n",
            "Epoch 2000/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8207 - accuracy: 0.7374 - val_loss: 0.8314 - val_accuracy: 0.7305\n",
            "Epoch 2001/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8205 - accuracy: 0.7375 - val_loss: 0.8313 - val_accuracy: 0.7305\n",
            "Epoch 2002/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8204 - accuracy: 0.7375 - val_loss: 0.8311 - val_accuracy: 0.7305\n",
            "Epoch 2003/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8202 - accuracy: 0.7375 - val_loss: 0.8309 - val_accuracy: 0.7305\n",
            "Epoch 2004/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8200 - accuracy: 0.7375 - val_loss: 0.8308 - val_accuracy: 0.7305\n",
            "Epoch 2005/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8199 - accuracy: 0.7375 - val_loss: 0.8306 - val_accuracy: 0.7306\n",
            "Epoch 2006/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8197 - accuracy: 0.7376 - val_loss: 0.8305 - val_accuracy: 0.7305\n",
            "Epoch 2007/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8195 - accuracy: 0.7375 - val_loss: 0.8303 - val_accuracy: 0.7305\n",
            "Epoch 2008/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8194 - accuracy: 0.7375 - val_loss: 0.8301 - val_accuracy: 0.7305\n",
            "Epoch 2009/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8192 - accuracy: 0.7375 - val_loss: 0.8300 - val_accuracy: 0.7306\n",
            "Epoch 2010/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8190 - accuracy: 0.7375 - val_loss: 0.8298 - val_accuracy: 0.7306\n",
            "Epoch 2011/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8189 - accuracy: 0.7376 - val_loss: 0.8296 - val_accuracy: 0.7306\n",
            "Epoch 2012/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8187 - accuracy: 0.7376 - val_loss: 0.8295 - val_accuracy: 0.7306\n",
            "Epoch 2013/5000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.8186 - accuracy: 0.7376 - val_loss: 0.8293 - val_accuracy: 0.7306\n",
            "Epoch 2014/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8184 - accuracy: 0.7377 - val_loss: 0.8292 - val_accuracy: 0.7306\n",
            "Epoch 2015/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8182 - accuracy: 0.7377 - val_loss: 0.8290 - val_accuracy: 0.7307\n",
            "Epoch 2016/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8181 - accuracy: 0.7378 - val_loss: 0.8288 - val_accuracy: 0.7307\n",
            "Epoch 2017/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8179 - accuracy: 0.7378 - val_loss: 0.8287 - val_accuracy: 0.7308\n",
            "Epoch 2018/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8177 - accuracy: 0.7379 - val_loss: 0.8285 - val_accuracy: 0.7309\n",
            "Epoch 2019/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8176 - accuracy: 0.7379 - val_loss: 0.8284 - val_accuracy: 0.7309\n",
            "Epoch 2020/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8174 - accuracy: 0.7379 - val_loss: 0.8282 - val_accuracy: 0.7309\n",
            "Epoch 2021/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8172 - accuracy: 0.7379 - val_loss: 0.8280 - val_accuracy: 0.7309\n",
            "Epoch 2022/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8171 - accuracy: 0.7380 - val_loss: 0.8279 - val_accuracy: 0.7309\n",
            "Epoch 2023/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8169 - accuracy: 0.7380 - val_loss: 0.8277 - val_accuracy: 0.7309\n",
            "Epoch 2024/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8167 - accuracy: 0.7381 - val_loss: 0.8276 - val_accuracy: 0.7309\n",
            "Epoch 2025/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8166 - accuracy: 0.7382 - val_loss: 0.8274 - val_accuracy: 0.7309\n",
            "Epoch 2026/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8164 - accuracy: 0.7382 - val_loss: 0.8272 - val_accuracy: 0.7309\n",
            "Epoch 2027/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8163 - accuracy: 0.7382 - val_loss: 0.8271 - val_accuracy: 0.7309\n",
            "Epoch 2028/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8161 - accuracy: 0.7382 - val_loss: 0.8269 - val_accuracy: 0.7309\n",
            "Epoch 2029/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8159 - accuracy: 0.7383 - val_loss: 0.8268 - val_accuracy: 0.7310\n",
            "Epoch 2030/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8158 - accuracy: 0.7383 - val_loss: 0.8266 - val_accuracy: 0.7310\n",
            "Epoch 2031/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8156 - accuracy: 0.7383 - val_loss: 0.8264 - val_accuracy: 0.7310\n",
            "Epoch 2032/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8154 - accuracy: 0.7383 - val_loss: 0.8263 - val_accuracy: 0.7310\n",
            "Epoch 2033/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8153 - accuracy: 0.7383 - val_loss: 0.8261 - val_accuracy: 0.7310\n",
            "Epoch 2034/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8151 - accuracy: 0.7383 - val_loss: 0.8260 - val_accuracy: 0.7311\n",
            "Epoch 2035/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8150 - accuracy: 0.7383 - val_loss: 0.8258 - val_accuracy: 0.7311\n",
            "Epoch 2036/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8148 - accuracy: 0.7383 - val_loss: 0.8256 - val_accuracy: 0.7311\n",
            "Epoch 2037/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8146 - accuracy: 0.7383 - val_loss: 0.8255 - val_accuracy: 0.7311\n",
            "Epoch 2038/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8145 - accuracy: 0.7384 - val_loss: 0.8253 - val_accuracy: 0.7311\n",
            "Epoch 2039/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8143 - accuracy: 0.7384 - val_loss: 0.8252 - val_accuracy: 0.7312\n",
            "Epoch 2040/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8141 - accuracy: 0.7384 - val_loss: 0.8250 - val_accuracy: 0.7312\n",
            "Epoch 2041/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8140 - accuracy: 0.7385 - val_loss: 0.8249 - val_accuracy: 0.7312\n",
            "Epoch 2042/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8138 - accuracy: 0.7385 - val_loss: 0.8247 - val_accuracy: 0.7312\n",
            "Epoch 2043/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8137 - accuracy: 0.7386 - val_loss: 0.8245 - val_accuracy: 0.7312\n",
            "Epoch 2044/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8135 - accuracy: 0.7386 - val_loss: 0.8244 - val_accuracy: 0.7312\n",
            "Epoch 2045/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8133 - accuracy: 0.7386 - val_loss: 0.8242 - val_accuracy: 0.7313\n",
            "Epoch 2046/5000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.8132 - accuracy: 0.7386 - val_loss: 0.8241 - val_accuracy: 0.7313\n",
            "Epoch 2047/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8130 - accuracy: 0.7387 - val_loss: 0.8239 - val_accuracy: 0.7314\n",
            "Epoch 2048/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8129 - accuracy: 0.7387 - val_loss: 0.8238 - val_accuracy: 0.7314\n",
            "Epoch 2049/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.8127 - accuracy: 0.7387 - val_loss: 0.8236 - val_accuracy: 0.7315\n",
            "Epoch 2050/5000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.8125 - accuracy: 0.7387 - val_loss: 0.8235 - val_accuracy: 0.7315\n",
            "Epoch 2051/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8124 - accuracy: 0.7388 - val_loss: 0.8233 - val_accuracy: 0.7314\n",
            "Epoch 2052/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8122 - accuracy: 0.7388 - val_loss: 0.8231 - val_accuracy: 0.7314\n",
            "Epoch 2053/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8121 - accuracy: 0.7388 - val_loss: 0.8230 - val_accuracy: 0.7314\n",
            "Epoch 2054/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8119 - accuracy: 0.7389 - val_loss: 0.8228 - val_accuracy: 0.7315\n",
            "Epoch 2055/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8117 - accuracy: 0.7389 - val_loss: 0.8227 - val_accuracy: 0.7316\n",
            "Epoch 2056/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8116 - accuracy: 0.7389 - val_loss: 0.8225 - val_accuracy: 0.7317\n",
            "Epoch 2057/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8114 - accuracy: 0.7390 - val_loss: 0.8224 - val_accuracy: 0.7318\n",
            "Epoch 2058/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8113 - accuracy: 0.7390 - val_loss: 0.8222 - val_accuracy: 0.7318\n",
            "Epoch 2059/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8111 - accuracy: 0.7391 - val_loss: 0.8221 - val_accuracy: 0.7319\n",
            "Epoch 2060/5000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.8109 - accuracy: 0.7391 - val_loss: 0.8219 - val_accuracy: 0.7319\n",
            "Epoch 2061/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8108 - accuracy: 0.7391 - val_loss: 0.8217 - val_accuracy: 0.7319\n",
            "Epoch 2062/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8106 - accuracy: 0.7392 - val_loss: 0.8216 - val_accuracy: 0.7319\n",
            "Epoch 2063/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8105 - accuracy: 0.7393 - val_loss: 0.8214 - val_accuracy: 0.7320\n",
            "Epoch 2064/5000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.8103 - accuracy: 0.7393 - val_loss: 0.8213 - val_accuracy: 0.7321\n",
            "Epoch 2065/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8102 - accuracy: 0.7394 - val_loss: 0.8211 - val_accuracy: 0.7323\n",
            "Epoch 2066/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8100 - accuracy: 0.7394 - val_loss: 0.8210 - val_accuracy: 0.7323\n",
            "Epoch 2067/5000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.8098 - accuracy: 0.7394 - val_loss: 0.8208 - val_accuracy: 0.7324\n",
            "Epoch 2068/5000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.8097 - accuracy: 0.7394 - val_loss: 0.8207 - val_accuracy: 0.7324\n",
            "Epoch 2069/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8095 - accuracy: 0.7395 - val_loss: 0.8205 - val_accuracy: 0.7325\n",
            "Epoch 2070/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8094 - accuracy: 0.7395 - val_loss: 0.8204 - val_accuracy: 0.7325\n",
            "Epoch 2071/5000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.8092 - accuracy: 0.7395 - val_loss: 0.8202 - val_accuracy: 0.7326\n",
            "Epoch 2072/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8091 - accuracy: 0.7395 - val_loss: 0.8200 - val_accuracy: 0.7327\n",
            "Epoch 2073/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8089 - accuracy: 0.7395 - val_loss: 0.8199 - val_accuracy: 0.7327\n",
            "Epoch 2074/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8087 - accuracy: 0.7396 - val_loss: 0.8197 - val_accuracy: 0.7327\n",
            "Epoch 2075/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8086 - accuracy: 0.7396 - val_loss: 0.8196 - val_accuracy: 0.7328\n",
            "Epoch 2076/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8084 - accuracy: 0.7396 - val_loss: 0.8194 - val_accuracy: 0.7328\n",
            "Epoch 2077/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8083 - accuracy: 0.7396 - val_loss: 0.8193 - val_accuracy: 0.7328\n",
            "Epoch 2078/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8081 - accuracy: 0.7396 - val_loss: 0.8191 - val_accuracy: 0.7329\n",
            "Epoch 2079/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8080 - accuracy: 0.7396 - val_loss: 0.8190 - val_accuracy: 0.7329\n",
            "Epoch 2080/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8078 - accuracy: 0.7397 - val_loss: 0.8188 - val_accuracy: 0.7328\n",
            "Epoch 2081/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8076 - accuracy: 0.7397 - val_loss: 0.8187 - val_accuracy: 0.7330\n",
            "Epoch 2082/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8075 - accuracy: 0.7397 - val_loss: 0.8185 - val_accuracy: 0.7331\n",
            "Epoch 2083/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8073 - accuracy: 0.7397 - val_loss: 0.8184 - val_accuracy: 0.7331\n",
            "Epoch 2084/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8072 - accuracy: 0.7398 - val_loss: 0.8182 - val_accuracy: 0.7332\n",
            "Epoch 2085/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8070 - accuracy: 0.7398 - val_loss: 0.8181 - val_accuracy: 0.7332\n",
            "Epoch 2086/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8069 - accuracy: 0.7398 - val_loss: 0.8179 - val_accuracy: 0.7332\n",
            "Epoch 2087/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8067 - accuracy: 0.7398 - val_loss: 0.8178 - val_accuracy: 0.7332\n",
            "Epoch 2088/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.8066 - accuracy: 0.7399 - val_loss: 0.8176 - val_accuracy: 0.7332\n",
            "Epoch 2089/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8064 - accuracy: 0.7399 - val_loss: 0.8175 - val_accuracy: 0.7332\n",
            "Epoch 2090/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8063 - accuracy: 0.7399 - val_loss: 0.8173 - val_accuracy: 0.7333\n",
            "Epoch 2091/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8061 - accuracy: 0.7399 - val_loss: 0.8172 - val_accuracy: 0.7333\n",
            "Epoch 2092/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.8059 - accuracy: 0.7399 - val_loss: 0.8170 - val_accuracy: 0.7335\n",
            "Epoch 2093/5000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.8058 - accuracy: 0.7400 - val_loss: 0.8169 - val_accuracy: 0.7335\n",
            "Epoch 2094/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8056 - accuracy: 0.7401 - val_loss: 0.8167 - val_accuracy: 0.7334\n",
            "Epoch 2095/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8055 - accuracy: 0.7401 - val_loss: 0.8166 - val_accuracy: 0.7334\n",
            "Epoch 2096/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8053 - accuracy: 0.7401 - val_loss: 0.8164 - val_accuracy: 0.7334\n",
            "Epoch 2097/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8052 - accuracy: 0.7402 - val_loss: 0.8163 - val_accuracy: 0.7334\n",
            "Epoch 2098/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8050 - accuracy: 0.7402 - val_loss: 0.8161 - val_accuracy: 0.7334\n",
            "Epoch 2099/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8049 - accuracy: 0.7403 - val_loss: 0.8160 - val_accuracy: 0.7335\n",
            "Epoch 2100/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8047 - accuracy: 0.7403 - val_loss: 0.8158 - val_accuracy: 0.7336\n",
            "Epoch 2101/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8046 - accuracy: 0.7403 - val_loss: 0.8157 - val_accuracy: 0.7335\n",
            "Epoch 2102/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8044 - accuracy: 0.7403 - val_loss: 0.8155 - val_accuracy: 0.7336\n",
            "Epoch 2103/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8043 - accuracy: 0.7404 - val_loss: 0.8154 - val_accuracy: 0.7336\n",
            "Epoch 2104/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8041 - accuracy: 0.7405 - val_loss: 0.8152 - val_accuracy: 0.7336\n",
            "Epoch 2105/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8039 - accuracy: 0.7405 - val_loss: 0.8151 - val_accuracy: 0.7337\n",
            "Epoch 2106/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8038 - accuracy: 0.7406 - val_loss: 0.8149 - val_accuracy: 0.7337\n",
            "Epoch 2107/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.8036 - accuracy: 0.7406 - val_loss: 0.8148 - val_accuracy: 0.7337\n",
            "Epoch 2108/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8035 - accuracy: 0.7406 - val_loss: 0.8146 - val_accuracy: 0.7336\n",
            "Epoch 2109/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8033 - accuracy: 0.7406 - val_loss: 0.8145 - val_accuracy: 0.7336\n",
            "Epoch 2110/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8032 - accuracy: 0.7407 - val_loss: 0.8143 - val_accuracy: 0.7336\n",
            "Epoch 2111/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8030 - accuracy: 0.7407 - val_loss: 0.8142 - val_accuracy: 0.7336\n",
            "Epoch 2112/5000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.8029 - accuracy: 0.7408 - val_loss: 0.8140 - val_accuracy: 0.7336\n",
            "Epoch 2113/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8027 - accuracy: 0.7408 - val_loss: 0.8139 - val_accuracy: 0.7336\n",
            "Epoch 2114/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.8026 - accuracy: 0.7408 - val_loss: 0.8137 - val_accuracy: 0.7337\n",
            "Epoch 2115/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8024 - accuracy: 0.7409 - val_loss: 0.8136 - val_accuracy: 0.7337\n",
            "Epoch 2116/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8023 - accuracy: 0.7409 - val_loss: 0.8134 - val_accuracy: 0.7337\n",
            "Epoch 2117/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8021 - accuracy: 0.7409 - val_loss: 0.8133 - val_accuracy: 0.7338\n",
            "Epoch 2118/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8020 - accuracy: 0.7410 - val_loss: 0.8131 - val_accuracy: 0.7338\n",
            "Epoch 2119/5000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.8018 - accuracy: 0.7410 - val_loss: 0.8130 - val_accuracy: 0.7338\n",
            "Epoch 2120/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8017 - accuracy: 0.7410 - val_loss: 0.8128 - val_accuracy: 0.7338\n",
            "Epoch 2121/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.8015 - accuracy: 0.7410 - val_loss: 0.8127 - val_accuracy: 0.7339\n",
            "Epoch 2122/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8014 - accuracy: 0.7410 - val_loss: 0.8125 - val_accuracy: 0.7340\n",
            "Epoch 2123/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8012 - accuracy: 0.7411 - val_loss: 0.8124 - val_accuracy: 0.7342\n",
            "Epoch 2124/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8011 - accuracy: 0.7411 - val_loss: 0.8123 - val_accuracy: 0.7342\n",
            "Epoch 2125/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8009 - accuracy: 0.7411 - val_loss: 0.8121 - val_accuracy: 0.7343\n",
            "Epoch 2126/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8008 - accuracy: 0.7411 - val_loss: 0.8120 - val_accuracy: 0.7343\n",
            "Epoch 2127/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.8006 - accuracy: 0.7412 - val_loss: 0.8118 - val_accuracy: 0.7343\n",
            "Epoch 2128/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8005 - accuracy: 0.7412 - val_loss: 0.8117 - val_accuracy: 0.7343\n",
            "Epoch 2129/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.8003 - accuracy: 0.7412 - val_loss: 0.8115 - val_accuracy: 0.7344\n",
            "Epoch 2130/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8002 - accuracy: 0.7412 - val_loss: 0.8114 - val_accuracy: 0.7344\n",
            "Epoch 2131/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.8000 - accuracy: 0.7412 - val_loss: 0.8112 - val_accuracy: 0.7345\n",
            "Epoch 2132/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.7999 - accuracy: 0.7412 - val_loss: 0.8111 - val_accuracy: 0.7345\n",
            "Epoch 2133/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.7997 - accuracy: 0.7412 - val_loss: 0.8109 - val_accuracy: 0.7345\n",
            "Epoch 2134/5000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.7996 - accuracy: 0.7413 - val_loss: 0.8108 - val_accuracy: 0.7345\n",
            "Epoch 2135/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.7994 - accuracy: 0.7413 - val_loss: 0.8106 - val_accuracy: 0.7346\n",
            "Epoch 2136/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7993 - accuracy: 0.7414 - val_loss: 0.8105 - val_accuracy: 0.7348\n",
            "Epoch 2137/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.7991 - accuracy: 0.7415 - val_loss: 0.8104 - val_accuracy: 0.7348\n",
            "Epoch 2138/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7990 - accuracy: 0.7415 - val_loss: 0.8102 - val_accuracy: 0.7349\n",
            "Epoch 2139/5000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.7988 - accuracy: 0.7416 - val_loss: 0.8101 - val_accuracy: 0.7349\n",
            "Epoch 2140/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.7987 - accuracy: 0.7416 - val_loss: 0.8099 - val_accuracy: 0.7350\n",
            "Epoch 2141/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7985 - accuracy: 0.7416 - val_loss: 0.8098 - val_accuracy: 0.7352\n",
            "Epoch 2142/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7984 - accuracy: 0.7418 - val_loss: 0.8096 - val_accuracy: 0.7353\n",
            "Epoch 2143/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.7982 - accuracy: 0.7418 - val_loss: 0.8095 - val_accuracy: 0.7354\n",
            "Epoch 2144/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.7981 - accuracy: 0.7418 - val_loss: 0.8093 - val_accuracy: 0.7355\n",
            "Epoch 2145/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7979 - accuracy: 0.7418 - val_loss: 0.8092 - val_accuracy: 0.7355\n",
            "Epoch 2146/5000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.7978 - accuracy: 0.7419 - val_loss: 0.8091 - val_accuracy: 0.7355\n",
            "Epoch 2147/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.7976 - accuracy: 0.7419 - val_loss: 0.8089 - val_accuracy: 0.7355\n",
            "Epoch 2148/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.7975 - accuracy: 0.7419 - val_loss: 0.8088 - val_accuracy: 0.7355\n",
            "Epoch 2149/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.7973 - accuracy: 0.7419 - val_loss: 0.8086 - val_accuracy: 0.7355\n",
            "Epoch 2150/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.7972 - accuracy: 0.7419 - val_loss: 0.8085 - val_accuracy: 0.7355\n",
            "Epoch 2151/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7971 - accuracy: 0.7420 - val_loss: 0.8083 - val_accuracy: 0.7355\n",
            "Epoch 2152/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7969 - accuracy: 0.7420 - val_loss: 0.8082 - val_accuracy: 0.7355\n",
            "Epoch 2153/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.7968 - accuracy: 0.7420 - val_loss: 0.8081 - val_accuracy: 0.7356\n",
            "Epoch 2154/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7966 - accuracy: 0.7421 - val_loss: 0.8079 - val_accuracy: 0.7355\n",
            "Epoch 2155/5000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.7965 - accuracy: 0.7422 - val_loss: 0.8078 - val_accuracy: 0.7355\n",
            "Epoch 2156/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.7963 - accuracy: 0.7422 - val_loss: 0.8076 - val_accuracy: 0.7356\n",
            "Epoch 2157/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.7962 - accuracy: 0.7423 - val_loss: 0.8075 - val_accuracy: 0.7356\n",
            "Epoch 2158/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7960 - accuracy: 0.7423 - val_loss: 0.8073 - val_accuracy: 0.7356\n",
            "Epoch 2159/5000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.7959 - accuracy: 0.7424 - val_loss: 0.8072 - val_accuracy: 0.7356\n",
            "Epoch 2160/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.7957 - accuracy: 0.7424 - val_loss: 0.8071 - val_accuracy: 0.7358\n",
            "Epoch 2161/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.7956 - accuracy: 0.7424 - val_loss: 0.8069 - val_accuracy: 0.7358\n",
            "Epoch 2162/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.7954 - accuracy: 0.7424 - val_loss: 0.8068 - val_accuracy: 0.7359\n",
            "Epoch 2163/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.7953 - accuracy: 0.7424 - val_loss: 0.8066 - val_accuracy: 0.7359\n",
            "Epoch 2164/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.7952 - accuracy: 0.7424 - val_loss: 0.8065 - val_accuracy: 0.7359\n",
            "Epoch 2165/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.7950 - accuracy: 0.7425 - val_loss: 0.8063 - val_accuracy: 0.7359\n",
            "Epoch 2166/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.7949 - accuracy: 0.7425 - val_loss: 0.8062 - val_accuracy: 0.7360\n",
            "Epoch 2167/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7947 - accuracy: 0.7425 - val_loss: 0.8061 - val_accuracy: 0.7360\n",
            "Epoch 2168/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.7946 - accuracy: 0.7426 - val_loss: 0.8059 - val_accuracy: 0.7360\n",
            "Epoch 2169/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.7944 - accuracy: 0.7426 - val_loss: 0.8058 - val_accuracy: 0.7361\n",
            "Epoch 2170/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7943 - accuracy: 0.7426 - val_loss: 0.8056 - val_accuracy: 0.7361\n",
            "Epoch 2171/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7941 - accuracy: 0.7427 - val_loss: 0.8055 - val_accuracy: 0.7361\n",
            "Epoch 2172/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7940 - accuracy: 0.7428 - val_loss: 0.8054 - val_accuracy: 0.7362\n",
            "Epoch 2173/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7938 - accuracy: 0.7428 - val_loss: 0.8052 - val_accuracy: 0.7362\n",
            "Epoch 2174/5000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.7937 - accuracy: 0.7428 - val_loss: 0.8051 - val_accuracy: 0.7362\n",
            "Epoch 2175/5000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.7936 - accuracy: 0.7429 - val_loss: 0.8049 - val_accuracy: 0.7362\n",
            "Epoch 2176/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.7934 - accuracy: 0.7429 - val_loss: 0.8048 - val_accuracy: 0.7362\n",
            "Epoch 2177/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7933 - accuracy: 0.7430 - val_loss: 0.8046 - val_accuracy: 0.7362\n",
            "Epoch 2178/5000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.7931 - accuracy: 0.7430 - val_loss: 0.8045 - val_accuracy: 0.7362\n",
            "Epoch 2179/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7930 - accuracy: 0.7431 - val_loss: 0.8044 - val_accuracy: 0.7364\n",
            "Epoch 2180/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7928 - accuracy: 0.7431 - val_loss: 0.8042 - val_accuracy: 0.7365\n",
            "Epoch 2181/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.7927 - accuracy: 0.7432 - val_loss: 0.8041 - val_accuracy: 0.7364\n",
            "Epoch 2182/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7926 - accuracy: 0.7432 - val_loss: 0.8039 - val_accuracy: 0.7364\n",
            "Epoch 2183/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7924 - accuracy: 0.7432 - val_loss: 0.8038 - val_accuracy: 0.7363\n",
            "Epoch 2184/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7923 - accuracy: 0.7433 - val_loss: 0.8037 - val_accuracy: 0.7363\n",
            "Epoch 2185/5000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.7921 - accuracy: 0.7433 - val_loss: 0.8035 - val_accuracy: 0.7363\n",
            "Epoch 2186/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.7920 - accuracy: 0.7434 - val_loss: 0.8034 - val_accuracy: 0.7363\n",
            "Epoch 2187/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.7918 - accuracy: 0.7434 - val_loss: 0.8032 - val_accuracy: 0.7366\n",
            "Epoch 2188/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7917 - accuracy: 0.7434 - val_loss: 0.8031 - val_accuracy: 0.7366\n",
            "Epoch 2189/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.7915 - accuracy: 0.7434 - val_loss: 0.8030 - val_accuracy: 0.7367\n",
            "Epoch 2190/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7914 - accuracy: 0.7434 - val_loss: 0.8028 - val_accuracy: 0.7368\n",
            "Epoch 2191/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7913 - accuracy: 0.7434 - val_loss: 0.8027 - val_accuracy: 0.7368\n",
            "Epoch 2192/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7911 - accuracy: 0.7435 - val_loss: 0.8026 - val_accuracy: 0.7368\n",
            "Epoch 2193/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7910 - accuracy: 0.7435 - val_loss: 0.8024 - val_accuracy: 0.7368\n",
            "Epoch 2194/5000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.7908 - accuracy: 0.7436 - val_loss: 0.8023 - val_accuracy: 0.7368\n",
            "Epoch 2195/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.7907 - accuracy: 0.7436 - val_loss: 0.8021 - val_accuracy: 0.7368\n",
            "Epoch 2196/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.7906 - accuracy: 0.7437 - val_loss: 0.8020 - val_accuracy: 0.7368\n",
            "Epoch 2197/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7904 - accuracy: 0.7437 - val_loss: 0.8019 - val_accuracy: 0.7368\n",
            "Epoch 2198/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7903 - accuracy: 0.7437 - val_loss: 0.8017 - val_accuracy: 0.7367\n",
            "Epoch 2199/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7901 - accuracy: 0.7438 - val_loss: 0.8016 - val_accuracy: 0.7366\n",
            "Epoch 2200/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7900 - accuracy: 0.7438 - val_loss: 0.8014 - val_accuracy: 0.7367\n",
            "Epoch 2201/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.7898 - accuracy: 0.7438 - val_loss: 0.8013 - val_accuracy: 0.7368\n",
            "Epoch 2202/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.7897 - accuracy: 0.7439 - val_loss: 0.8012 - val_accuracy: 0.7369\n",
            "Epoch 2203/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7896 - accuracy: 0.7439 - val_loss: 0.8010 - val_accuracy: 0.7370\n",
            "Epoch 2204/5000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.7894 - accuracy: 0.7440 - val_loss: 0.8009 - val_accuracy: 0.7370\n",
            "Epoch 2205/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.7893 - accuracy: 0.7440 - val_loss: 0.8008 - val_accuracy: 0.7370\n",
            "Epoch 2206/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7891 - accuracy: 0.7440 - val_loss: 0.8006 - val_accuracy: 0.7370\n",
            "Epoch 2207/5000\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.7890 - accuracy: 0.7440 - val_loss: 0.8005 - val_accuracy: 0.7371\n",
            "Epoch 2208/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7889 - accuracy: 0.7441 - val_loss: 0.8003 - val_accuracy: 0.7371\n",
            "Epoch 2209/5000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.7887 - accuracy: 0.7441 - val_loss: 0.8002 - val_accuracy: 0.7371\n",
            "Epoch 2210/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.7886 - accuracy: 0.7441 - val_loss: 0.8001 - val_accuracy: 0.7371\n",
            "Epoch 2211/5000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.7884 - accuracy: 0.7441 - val_loss: 0.7999 - val_accuracy: 0.7372\n",
            "Epoch 2212/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7883 - accuracy: 0.7442 - val_loss: 0.7998 - val_accuracy: 0.7372\n",
            "Epoch 2213/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7882 - accuracy: 0.7443 - val_loss: 0.7997 - val_accuracy: 0.7372\n",
            "Epoch 2214/5000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.7880 - accuracy: 0.7443 - val_loss: 0.7995 - val_accuracy: 0.7371\n",
            "Epoch 2215/5000\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.7879 - accuracy: 0.7444 - val_loss: 0.7994 - val_accuracy: 0.7371\n",
            "Epoch 2216/5000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.7877 - accuracy: 0.7444 - val_loss: 0.7993 - val_accuracy: 0.7371\n",
            "Epoch 2217/5000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7876 - accuracy: 0.7444 - val_loss: 0.7991 - val_accuracy: 0.7371\n",
            "Epoch 2218/5000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7875 - accuracy: 0.7444 - val_loss: 0.7990 - val_accuracy: 0.7371\n",
            "Epoch 2219/5000\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.7873 - accuracy: 0.7444 - val_loss: 0.7988 - val_accuracy: 0.7371\n",
            "Epoch 2220/5000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7872 - accuracy: 0.7444"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiWLPdwhwQHR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "b4a1c559-dafe-4f45-9273-70da54a83d5d"
      },
      "source": [
        "# Visualization code here... \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "my_model_variable = model1# Place the model output variable here\n",
        "\n",
        "plt.plot(my_model_variable.history['loss'])\n",
        "plt.plot(my_model_variable.history['val_loss'])\n",
        "plt.title('Network Training')\n",
        "plt.ylabel('Cost Function')\n",
        "plt.xlabel('Number of Iterations')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-200-f1b647ff5cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model_variable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model_variable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Network Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cost Function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}